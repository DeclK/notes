---
title: D2L 13 计算机视觉
tag:
  - Dive into deep learning
categories:
  - 课程
  - Dive into deep learning
mathjax: true
abbrlink: 15b3ab53
date: 2022-04-08 00:00:00
---

# D2L 13 计算机视觉

因为对于计算机视觉已经比较熟悉了（主要是通过看论文了解的），所以这一章节也是简单整理

## 图像增广

随机改变训练样本可以减少模型对某些属性的依赖，从而提高模型的泛化能力

为了在预测过程中得到确切的结果，我们通常对训练样本只进行图像增广，而在预测过程中不使用带随机操作的图像增广

### 常用的图像增广方法

通常可以使用 `torchvision.transforms` 中的方法进行操作。这里暂时不整理

1. 翻转和剪裁
2. 改变颜色
3. 混合增广方法

## 微调（迁移学习）

由于训练样本数量有限，训练模型的准确性可能无法满足实际要求，例如：会产生过拟合的现象

一种解决方案是应用迁移学习（transfer learning）将从*源数据集*学到的知识迁移到*目标数据集*。 例如，尽管 ImageNet 数据集中的大多数图像与椅子无关，但在此数据集上训练的模型可能会提取更通用的图像特征，这有助于识别边缘、纹理、形状和对象组合。 这些类似的特征也可能有效地识别椅子

### 步骤

1. 在源数据集（例如 ImageNet 数据集）上预训练神经网络模型
2. 创建一个新的神经网络模型，即目标模型。复制源模型上的所有模型设计及其参数（输出层除外）
3. 向目标模型添加输出层，其输出数是目标数据集中的类别数。然后随机初始化该层的模型参数
4. 在目标数据集（如椅子数据集）上训练目标模型。输出层将从头开始进行训练，而所有其他层的参数将根据源模型的参数进行微调

下面贴一下载入模型和更改参数的简单代码

```python
finetune_net = torchvision.models.resnet18(pretrained=True)
finetune_net.fc = nn.Linear(finetune_net.fc.in_features, 2)
nn.init.xavier_uniform_(finetune_net.fc.weight)
```

**通常，微调参数使用较小的学习率，而从头开始训练输出层可以使用更大的学习率**

## 目标检测和边界框

目标检测不仅可以识别图像中所有感兴趣的物体，还能识别它们的位置，该位置通常由矩形边界框表示

我们可以在两种常用的边界框表示（中间，宽度，高度）和（左上，右下）坐标之间进行转换

## 锚框

目标检测算法通常会在输入图像中采样大量的区域，然后判断这些区域中是否包含我们感兴趣的目标，并调整区域边界从而更准确地预测目标的真实边界框（ground-truth bounding box）

我们介绍其中的一种方法：以每个像素为中心，生成多个缩放比和宽高比（aspect ratio）不同的边界框。 这些边界框被称为锚框（anchor box）

### 标注锚框

在训练集中，我们将每个锚框视为一个训练样本。 为了训练目标检测模型，我们需要每个锚框的类别（class）和偏移量（offset）作为标签。前者是与锚框相关的对象的类别，后者是真实边界框相对于锚框的偏移量

为了标注锚框，自然就需要知道锚框与哪个 ground truth 相关。这就需要将 ground truth 分配给锚框。教材给出了一个方法，但似乎与我了解的方法不太符合

我所了解的方法：最终的目的是要得到**正负锚框样本**，也就是知道这是一个好的锚框，还是一个坏的锚框。评价标注就是锚框与 ground truth 的 iou
如果 iou > thresh 则是正样本，若 iou < thresh 则是负样本（正负样本的 thresh 也可以不同）。有了正负样本就可以进行二元交叉熵计算分类损失了（其实只有正样本也能行，但是加入负样本能够有更强的监督信号）

对于回归损失通常只对正样本进行计算，个人认为是为了保持模型的稳定，在训练初期负样本的回归损失会很大。样本标签是锚框和 ground truth 的残差

### NMS

这是一个非常重要的方法！强烈建议自己写一写，面试常考！其目的是为了在众多预测选框中去除重复选框，筛选获得质量高的选框。贴一个我自己写的版本

```python
def nms(data, thresh):
    """Pure Python NMS baseline."""
    data = data[np.argsort(data[:, -1])[::-1],:]
    x1 = data[:, 0]
    y1 = data[:, 1]
    x2 = data[:, 2]
    y2 = data[:, 3]
    areas = (x2 - x1) * (y2 - y1)
    index = np.arange(len(data))    # 重点，使用 index 保存仍需要计算的 boxes
    keep = []
    while index.size > 0:
        i = index[0]
        keep.append(data[i])
        xx1 = np.maximum(x1[i], x1[index])	# 仅计算 index 中的 boxes
        yy1 = np.maximum(y1[i], y1[index])
        xx2 = np.minimum(x2[i], x2[index])
        yy2 = np.minimum(y2[i], y2[index])
        w = np.maximum(0.0, xx2 - xx1)
        h = np.maximum(0.0, yy2 - yy1)
        inter = w * h
        ovr = inter / (areas[i] + areas[index] - inter)
        index = index[ovr < thresh]
    return np.stack(keep, axis=0)
```

## 多尺度目标检测

- 在多个尺度下，我们可以**生成不同尺寸的锚框**来检测不同尺寸的目标
- 通过定义特征图的形状，**我们可以决定任何图像上均匀采样的锚框的中心**

## SSD

我们可以设计基础网络，使它输出的高和宽较大。 这样一来，基于该特征图生成的锚框数量较多，可以用来检测尺寸较小的目标。 接下来的每个多尺度特征块将上一层提供的特征图的高和宽缩小（如减半），并使特征图中每个单元在输入图像上的感受野变得更广阔

<img src="D2L 13 计算机视觉/image-20220324202819029.png" alt="image-20220324202819029" style="zoom:50%;" />

几个点：

1. 多尺度特征块的分辨率通常是逐渐降低的
2. 需要将所有预测的结果 concat 起来
3. 分类损失使用交叉熵损失函数；边界框预测使用 L1 损失函数

## R-CNN

具体来说，（最原始的）R-CNN 包括以下四个步骤：

1. 对输入图像使用选择性搜索来选取多个高质量的提议区域（proposal）（锚框也是一种选取方法）
2. 将每个提议区域变形为网络需要的输入尺寸，并通过 CNN 提议区域特征
3. 使用每个 proposal 的特征预测每个 proposal 的类别和边界框

**缺点：它的速度很慢。** 想象一下，我们可能从一张图像中选出上千个提议区域，这需要上千次的卷积神经网络的前向传播来执行目标检测

### Fast R-CNN

R-CNN的主要性能瓶颈在于，对每个提议区域，卷积神经网络的前向传播是独立的，而没有共享计算。 由于这些区域通常有重叠，独立的特征抽取会导致重复的计算

 Fast R-CNN 对 R-CNN 的主要改进之一，是**仅在整张图象上执行卷积神经网络的前向传播**，并提出了 Region of Interest Pooling (roi pooling) 可从形状各异的兴趣区域中均抽取出形状相同的特征

<img src="D2L 13 计算机视觉/image-20220325192949807.png" alt="image-20220325192949807" style="zoom:50%;" />

### Faster R-CNN

Fast R-CNN模型通常需要在选择性搜索中生成大量的提议区域，个人理解是：Fast R-CNN 是直接把锚框作为 roi？锚框的数量级是较大的，对每一个锚框都要进行 roi pooling 并进行预测的计算量将很大。Faster R-CNN 提出将选择性搜索替换为区域提议网络（region proposal network），从而减少提议区域的生成数量。**更具体的内容强烈建议阅读 Faster R-CNN 的论文或者相关博客**

### Mask R-CNN

Mask R-CNN 是基于 Faster R-CNN 修改而来的。 具体来说，Mask R-CNN 将 roi pooling 替换为了 roi align，使用双线性插值（bilinear interpolation）来保留特征图上的空间信息，从而更适于像素级预测，这就属于语义分割的范围了，接下来就介绍一些语义分割常用的结构

## 转置卷积与全卷积

卷积层通常会减少下采样输入图像的空间维度（高和宽），然而如果输入和输出图像的空间维度相同，在以像素级分类的语义分割中将会很方便。 而转置卷积（Transposed Convolution，有的地方也叫反卷积 Deconvolution）可以增加上采样中间层特征图的空间维度，用于逆转下采样导致的空间尺寸减小

<img src="D2L 13 计算机视觉/image-20220325194527594.png" alt="image-20220325194527594" style="zoom:50%;" />

通过转置卷积，就能够构造基本的全卷积网络（Fully Convolutional Network, FCN）

<img src="D2L 13 计算机视觉/image-20220325195223193.png" alt="image-20220325195223193" style="zoom:50%;" />

全卷积网络先使用卷积神经网络抽取图像特征，然后通过1×1卷积层将通道数变换为类别个数，最后通过转置卷积层将特征图的高和宽变换为输入图像的尺寸

在图像处理中，我们有时需要将图像放大，即上采样（upsampling）。 双线性插值（bilinear interpolation） 是常用的上采样方法之一，它也经常用于初始化转置卷积层。个人理解，通过设置转置卷积的参数，我们可以实现双线性插值的功能？

## 风格迁移

这里我们需要两张输入图像：一张是内容图像，另一张是风格图像。 我们将使用神经网络修改内容图像，使其在风格上接近风格图像

我们选择一个预训练的卷积神经网络来抽取图像的特征，其中的模型参数在训练中**无须更新，合成图像是风格迁移过程中唯一需要更新的变量**（可以认为我们不断去更新我们的输入以匹配）

<img src="D2L 13 计算机视觉/image-20220325202045253.png" alt="image-20220325202045253" style="zoom:50%;" />

我们可以选择其中某些层的输出作为内容特征或风格特征。 这里选取的预训练的神经网络含有3个卷积层，其中第二层输出内容特征，第一层和第三层输出风格特征

上图提到的三个损失函数就不细究了，内容损失就是简单的特征之间的 MSE 损失，样式损失为计算 gram 矩阵（通道之间的协方差矩阵）的 MSE 损失，全变分损失（TV Loss）用于去噪
