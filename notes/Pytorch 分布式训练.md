---
title: Pytorch åˆ†å¸ƒå¼è®­ç»ƒ
tags:
  - Pytorch
  - åˆ†å¸ƒå¼
categories:
  - ç¼–ç¨‹
  - Python
  - Pytorch
abbrlink: b45af34d
date: 2022-07-10 22:20:38
---

# Pytorch åˆ†å¸ƒå¼è®­ç»ƒ

å¯¹äºåˆ†å¸ƒå¼è®­ç»ƒçš„åŸç†å®åœ¨æ˜¯å¾ˆä¸æ¸…æ™°ï¼å¯èƒ½å°±ç®—æ•´ç†äº†ä¹Ÿä¸å¤ªèƒ½å®Œå…¨æ˜ç™½ï¼Œä½†æ˜¯åšäº†æ€»æ¯”æ²¡åšå¥½ğŸ˜­

å‚è€ƒï¼š[zhihu](https://zhuanlan.zhihu.com/p/113694038) [zhihu](https://zhuanlan.zhihu.com/p/358974461) [zhihu](https://zhuanlan.zhihu.com/p/393648544) [bilibili](https://www.bilibili.com/video/BV1xZ4y1S7dG/?spm_id_from=333.788)

å…³äºæŒ‡å®š GPU, CUDA_VISIBLE_DEVICES [CSDN](https://blog.csdn.net/alip39/article/details/87913543) 

## DistributedDataParallel

### å†…éƒ¨æœºåˆ¶çš„é€šä¿—ç†è§£

`DistributedDataParallel`é€šè¿‡å¤šè¿›ç¨‹åœ¨å¤šä¸ªGPUsé—´å¤åˆ¶æ¨¡å‹ï¼Œæ¯ä¸ªGPUéƒ½ç”±ä¸€ä¸ªè¿›ç¨‹æ§åˆ¶ã€‚GPUå¯ä»¥éƒ½åœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ä¸Šï¼Œä¹Ÿå¯ä»¥åˆ†å¸ƒåœ¨å¤šä¸ªèŠ‚ç‚¹ä¸Šã€‚æ¯ä¸ªè¿›ç¨‹éƒ½æ‰§è¡Œç›¸åŒçš„ä»»åŠ¡ï¼Œå¹¶ä¸”æ¯ä¸ªè¿›ç¨‹éƒ½ä¸æ‰€æœ‰å…¶ä»–è¿›ç¨‹é€šä¿¡ã€‚è¿›ç¨‹æˆ–è€…è¯´GPUä¹‹é—´åªä¼ é€’æ¢¯åº¦ï¼Œè¿™æ ·ç½‘ç»œé€šä¿¡å°±ä¸å†æ˜¯ç“¶é¢ˆ

æ¯ä¸€ä¸ªGPUéƒ½æœ‰è‡ªå·±çš„å‰å‘è¿‡ç¨‹ï¼Œç„¶åæ¢¯åº¦åœ¨å„ä¸ªGPUsé—´è¿›è¡Œ All-Reduceï¼ˆæ‰€è°“ All-Reduce å¯ä»¥ç®€å•çœ‹ä½œæ˜¯ä¸€ç§å¹³å‡ï¼‰ã€‚æ¯ä¸€å±‚çš„æ¢¯åº¦ä¸ä¾èµ–äºå‰ä¸€å±‚ï¼Œæ‰€ä»¥æ¢¯åº¦çš„ All-Reduce å’Œåå‘è¿‡ç¨‹åŒæ—¶è®¡ç®—ï¼Œä»¥è¿›ä¸€æ­¥ç¼“è§£ç½‘ç»œç“¶é¢ˆã€‚åœ¨åå‘è¿‡ç¨‹çš„æœ€åï¼Œæ¯ä¸ªèŠ‚ç‚¹éƒ½å¾—åˆ°äº†å¹³å‡æ¢¯åº¦ï¼Œè¿™æ ·æ¨¡å‹å‚æ•°ä¿æŒåŒæ­¥

### ä¸€äº›åŸºæœ¬æ¦‚å¿µ

**æ¯ä¸ªè¿›ç¨‹éƒ½éœ€è¦çŸ¥é“è¿›ç¨‹æ€»æ•°åŠå…¶åœ¨è¿›ç¨‹ä¸­çš„é¡ºåºï¼Œä»¥åŠä½¿ç”¨å“ªä¸ªGPU**ï¼Œè¿™æ ·è¿›ç¨‹ä¹‹é—´æ‰èƒ½æ­£ç¡®é€šä¿¡ï¼Œé€šå¸¸å°†è¿›ç¨‹æ€»æ•°ç§°ä¸º `world_size`ï¼Œå…¶é¡ºåºç§°ä¸º rank or local rankï¼ˆä¸¤è€…æˆ‘å…¶å®æ²¡æœ‰å¾ˆå¥½åŒºåˆ†ï¼‰

ä¸€èˆ¬æƒ…å†µä¸‹ç§°è¿›ç¨‹0ï¼ˆlocal rank == 0ï¼‰æ˜¯ master è¿›ç¨‹ï¼Œæ¯”å¦‚æˆ‘ä»¬ä¼šåœ¨è¿›ç¨‹0ä¸­æ‰“å°ä¿¡æ¯æˆ–è€…ä¿å­˜æ¨¡å‹

Pytorchæä¾›äº†`nn.utils.data.DistributedSampler`æ¥ä¸ºå„ä¸ªè¿›ç¨‹åˆ‡åˆ†æ•°æ®ï¼Œä»¥ä¿è¯è®­ç»ƒæ•°æ®ä¸é‡å 

## torchrun & torch.distributed.launch

DDP çš„å¯åŠ¨æ–¹å¼å½¢å¼ä¸Šæœ‰å¤šç§ï¼Œå†…å®¹ä¸Šæ˜¯ç»Ÿä¸€çš„ï¼šéƒ½æ˜¯å¯åŠ¨å¤šè¿›ç¨‹æ¥å®Œæˆè¿ç®—ï¼Œè¿™é‡Œå°±æ•´ç†ä¸€ä¸‹ OpenPCDet ä½¿ç”¨çš„ä¸€ç§æ–¹æ³•ã€‚ç°åœ¨ pytorch å‡†å¤‡ä½¿ç”¨ `torchrun` å®Œå…¨æ›¿ä»£ `torch.distributed.launch`

### launch

[torch.distributed.aunch](https://pytorch.org/docs/stable/distributed.html#launch-utility)

launch å®é™…ä¸Šä¸»è¦å®Œæˆçš„å·¥ä½œï¼š

1. å‚æ•°å®šä¹‰ä¸ä¼ é€’ã€‚è§£æç¯å¢ƒå˜é‡ï¼Œå¹¶å°†å˜é‡ä¼ é€’åˆ°å­è¿›ç¨‹ä¸­

2. èµ·å¤šè¿›ç¨‹ã€‚è°ƒç”¨subprocess.Popenå¯åŠ¨å¤šè¿›ç¨‹

ç”¨ launch æ–¹å¼éœ€è¦æ³¨æ„çš„ä½ç½®ï¼š

1. éœ€è¦æ·»åŠ ä¸€ä¸ªè§£æ local_rank çš„å‚æ•°

   ```python
   parser.add_argument("--local_rank", type=int)
   ```

   è¿è¡Œè„šæœ¬æ—¶ launch å°†ä¼šè‡ªåŠ¨ä¼ å…¥è¿™ä¸ªå‚æ•°çš„å€¼

2. DDPçš„è®¾å¤‡éƒ½éœ€è¦æŒ‡å®š local_rank

   ```python
   net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[args.local_rank])
   ```

ä¸€èˆ¬ä½¿ç”¨å•èŠ‚ç‚¹å¤šè¿›ç¨‹è„šæœ¬

```shell
python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE
           YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other
           arguments of your training script)
```

### torchrun

[torchrun](https://pytorch.org/docs/stable/elastic/run.html)	[train scripts](https://pytorch.org/docs/stable/elastic/train_script.html)	[quickstart](https://pytorch.org/docs/stable/elastic/quickstart.html)

æ¢åˆ° torchrun ä¸Š

```shell
torchrun
   --standalone
   --nnodes=1
   --nproc_per_node=TRAINERS_PER_NODE
   YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)
```

ä¸å†éœ€è¦ä½¿ç”¨ `--local-rank` å‚æ•°è§£æï¼Œè€Œæ˜¯ç›´æ¥åœ¨ç¯å¢ƒå˜é‡ä¸­è·å– `os.environ['LOCAL_RANK']`

ä»¥ä¸Šä¸¤ç§æ–¹æ³•éƒ½å¯ä»¥ä½¿ç”¨ `dist.get_rank() & dist.get_world_size()` è·å¾— local rank å’Œ world size

## å®Œæ•´è¿‡ç¨‹

æ€»ç»“ä¸€ä¸‹ OpenPCDet åˆ†å¸ƒå¼è®­ç»ƒçš„é€»è¾‘ï¼š

1. ä½¿ç”¨ `torch.distributed.launch` è°ƒç”¨å¤šè¿›ç¨‹

   ```shell
   python -m torch.python -m torch.distributed.launch --nproc_per_node={NUM_GPUS} ./train.py
   ```

2. åˆå§‹åŒ–è¿›ç¨‹ç»„ï¼Œå¹¶è®¾ç½® cuda device

   ```python
   mp.set_start_method('spawn')	# mmdet use 'fork' to start, faster but might be unstable
   								# not needed when use torch.distributed.launch or torchrun
   dist.init_process_group(backend='nccl')	# æŒ‡å®š backendï¼Œç”¨äºè¿›ç¨‹é—´çš„é€šä¿¡
   torch.cuda.set_device(local_rank)		# è®¾ç½®äº†å½“å‰ cuda device è¿‡åå¯ä»¥ç›´æ¥ç”¨ model.cuda()
   										# è¯¥æ­¥éª¤ä¸æ˜¯å¿…é¡»çš„
   ```

3. ä½¿ç”¨ DDP åŒ…è£… modelï¼Œå¹¶æŒ‡å®š device

   ```python
   model = DDP(model, device_ids=[args.local_rank])
   ```

4. ä½¿ç”¨ `DistributedSampler` ä½œä¸º sampler

   ```python
   sampler = torch.utils.data.distributed.DistributedSampler(dataset)
   ```

5. è®¾ç½® sampler çš„ epoch æ•°ï¼Œä»¥ç¡®å®šéšæœºç§å­ï¼Œä¿è¯åˆ‡å‰²æ•°æ®ä¸é‡å 

   ```python
   sampler.set_epoch(epoch)
   ```

### TODO

example with CUDA_VISIBLE_DEVICES, cudnn deterministicï¼Œcudnn benchmark  [zhihu](https://zhuanlan.zhihu.com/p/359058486), random seedï¼Œdataloader

fixed must prepare steps

## Official

ç°åœ¨å›è¿‡å¤´æ¥çœ‹ï¼Œåˆ†å¸ƒå¼è®­åªè¦ç†Ÿç»ƒè¿ç”¨ API å°±å¥½ã€‚è¿™æ ·è¿˜ä¸å¦‚ç›´æ¥å»å®˜ç½‘çœ‹çœ‹æ•™ç¨‹ï¼Œæ•´ç†èµ·æ¥æ›´æ¸…æ™°

[Dist Overview](https://pytorch.org/tutorials/beginner/dist_overview.html)	[Writing Dist Apps](https://pytorch.org/tutorials/intermediate/dist_tuto.html)	[**Getting Started with DDP**](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)

åˆ†å¸ƒå¼æ¨¡å—æ˜¯ç”¨ `torch.distributed`ï¼Œå¦ä¸€ä¸ªä½¿ç”¨æœ€é¢‘ç¹ç±»æ˜¯ `DistributedDataParallel` ï¼ˆç®€ç§° DDPï¼‰ã€‚ç°åœ¨åŸºæœ¬ä¸ç”¨ `DataParallel` å› ä¸ºå®ƒä½¿ç”¨çš„æ˜¯å¤šçº¿ç¨‹ï¼Œä¼šå—åˆ° GIL çš„é™åˆ¶ï¼Œæ‰€ä»¥å¾ˆæ…¢

è¦ä½¿ç”¨ DDP éœ€è¦å…ˆä½¿ç”¨ `init_group_process` è¿›è¡Œé…ç½®

### Collective Communication

Pytorch æ”¯æŒ Point-to-Point Communicationï¼Œä¹Ÿå°±æ˜¯ä»»æ„ä¸¤ä¸ª GPU ä¹‹é—´çš„äº¤æµï¼Œè¿™æ˜¯æ›´çµæ´»çš„äº¤æµæ–¹å¼ï¼Œè¿™é‡Œä¸åšæ€»ç»“ã€‚è€Œå•å¹³æ—¶ç”¨çš„æœ€å¤šçš„æ˜¯ Collective Communicationï¼Œä¹Ÿå°±æ˜¯åœ¨æ‰€æœ‰ GPU ä¹‹é—´çš„äº¤æµã€‚ä¸¾ä¸ªä¾‹å­ï¼Œå¦‚æœæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå¼ é‡åœ¨æ‰€æœ‰è¿›ç¨‹é‡Œçš„å’Œçš„æ—¶å€™å¯ä»¥ä½¿ç”¨ `dist.all_reduce(tensor, op=dist.ReduceOp.SUM)`ï¼Œé™¤äº†å’Œä»¥å¤–ï¼ŒPytorch æ€»å…±å®ç°äº†4ä¸­æ“ä½œï¼š

- `dist.ReduceOp.SUM`,
- `dist.ReduceOp.PRODUCT`,
- `dist.ReduceOp.MAX`,
- `dist.ReduceOp.MIN`.

é™¤äº† `dist.all_reduce` ä»¥å¤–ï¼ŒPytorch æ€»å…±æœ‰6ç§ collective method

- `dist.broadcast(tensor, src, group)`: Copies `tensor` from `src` to all other processes.
- `dist.reduce(tensor, dst, op, group)`: Applies `op` to every `tensor` and stores the result in `dst`.
- `dist.all_reduce(tensor, op, group)`: Same as reduce, but the result is stored in all processes.
- `dist.scatter(tensor, scatter_list, src, group)`: Copies the $i^{\text{th}}$ tensor `scatter_list[i]` to the $i^{\text{th}}$ process.
- `dist.gather(tensor, gather_list, dst, group)`: Copies `tensor` from all processes in `dst`.
- `dist.all_gather(tensor_list, tensor, group)`: Copies `tensor` from all processes to `tensor_list`, on all processes.
- `dist.barrier(group)`: Blocks all processes in group until each one has entered this function.

`src & dst` éƒ½æ˜¯å¯¹åº”çš„æŸä¸ª rankï¼Œè€Œ `group` ä¸€èˆ¬ä¸ç”¨æŒ‡å®šï¼Œé»˜è®¤æ˜¯æ‰€æœ‰çš„è¿›ç¨‹ã€‚`rank & world size` éƒ½å¯ä»¥é€šè¿‡ `dist.get_rank() & dist.get_world_size()` è·å¾—ï¼Œå‰ææ˜¯è¿›è¡Œäº† `dist.init_group_process`

åœ¨åˆ†å¸ƒå¼ä¸­éœ€è¦æ³¨æ„è¿›ç¨‹ä¹‹é—´çš„åŒæ­¥ï¼Œå…·ä½“æ¥è¯´ï¼Œåˆ†å¸ƒå¼è¦æ±‚å„ä¸ªè¿›ç¨‹è¿è¡Œçš„é€Ÿåº¦ä¸èƒ½ç›¸å·®å¤ªå¤§ï¼Œå¤§å®¶éƒ½åœ¨æŸä¸€ä¸ªæ—¶åˆ»æ‰§è¡Œç›¸åŒçš„ä»£ç ã€‚ä½†æ˜¯é€šå¸¸ä¸»è¿›ç¨‹è¦åšä¸€äº›å…¶ä»–çš„äº‹æƒ…ï¼Œå¦‚æœå„ä¸ªè¿›ç¨‹ä¹‹é—´çš„ä»£ç è¿›åº¦ç›¸å·®å¤ªå¤§ï¼Œå°±å¯èƒ½å‡ºé—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨ `dist.barrier` è¿›è¡Œä¸€ä¸ªåŒæ­¥

### Basic Usage

ä¸€èˆ¬åœ¨ä¸»ç¨‹åºçš„åˆå§‹ï¼Œç›´æ¥ä½¿ç”¨æœ€ç®€å•çš„åˆå§‹åŒ–

```python
def setup(rank, world_size, backend='nccl'):
    # initialize the process group
    dist.init_process_group(backend)

def main():
	setup(rank, world_size)
	model = Model().to(rank)	# model = Model().cuda(rank)
    ddp_model = DDP(model, device=[rank])
```

`rank & world_size` å¹¶ä¸æ˜¯å¿…é¡»çš„ä½¿ç”¨çš„ï¼Œå½“ç„¶ä¹Ÿå¯ä»¥ä¼ å…¥

````python
dist.init_process_group("gloo", rank=rank, world_size=world_size)	# use another backend 'gloo'
````

### Save Model

è™½ç„¶è¯´çš„æ˜¯ä¿å­˜æ¨¡å‹ï¼Œä½†å…¶å®ä»»ä½•ä¸åŒæ­¥çš„æ“ä½œéƒ½éœ€è¦è¿™ä¹ˆåš

```python
    if rank == 0:
        # All processes should see same parameters as they all start from same
        # random parameters and gradients are synchronized in backward passes.
        # Therefore, saving it in one process is sufficient.
        torch.save(ddp_model.state_dict(), CHECKPOINT_PATH)

    # Use a barrier() to make sure that process 1 runs after process 0 saves model.
    dist.barrier()
```