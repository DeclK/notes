---
title: D2L 10 æ³¨æ„åŠ›æœºåˆ¶
tag:
  - Dive into deep learning
categories:
  - è¯¾ç¨‹
  - Dive into deep learning
mathjax: true
abbrlink: b7d04b34
date: 2021-12-15 00:00:00
---

# D2L 10 æ³¨æ„åŠ›æœºåˆ¶

555ç»ˆäºå¯ä»¥å¼€å§‹çœ‹æ³¨æ„åŠ›æœºåˆ¶äº†ğŸ˜†Transformer æˆ‘æ¥å•¦ï¼

## æ³¨æ„åŠ›æç¤º

### ç”Ÿç‰©å­¦ä¸­çš„æ³¨æ„åŠ›æç¤º

ä¸»è¦æœ‰ä¸¤ä¸ªæ¦‚å¿µï¼š

1. **éè‡ªä¸»æ€§æç¤º/ééšæ„çº¿ç´¢**ï¼ˆnon-volitional cueï¼‰ï¼ŒåŸºäºç¯å¢ƒä¸­ç‰©ä½“çš„çªå‡ºæ€§å’Œæ˜“è§æ€§
2. **è‡ªä¸»æ€§æç¤º/éšæ„çº¿ç´¢**ï¼ˆvolitional cueï¼‰ï¼Œå—ä¸»è§‚æ„æ„¿æ¨åŠ¨

### æŸ¥è¯¢ã€é”®å’Œå€¼

æ•™æå¯¹äºè¿™ä¸‰ä¸ªæ¦‚å¿µçš„è§£é‡Šå¹¶ä¸æ¸…æ™°ï¼Œè¿˜æ˜¯çœ‹æ²ç¥çš„è®²è§£å§ [bilibili](https://www.bilibili.com/video/BV1264y1i7R1?p=1&t=447)

å·ç§¯ã€å…¨è¿æ¥ã€æ± åŒ–å±‚éƒ½åªè€ƒè™‘ä¸éšæ„çº¿ç´¢ï¼Œæ³¨æ„åŠ›æœºåˆ¶åˆ™æ˜¾å¼åœ°è€ƒè™‘éšæ„çº¿ç´¢ï¼š

1. éšæ„çº¿ç´¢è¢«ç§°ä¹‹ä¸ºæŸ¥è¯¢ï¼ˆqueryï¼‰
2. æ¯ä¸ª**è¾“å…¥**æ˜¯ä¸€ä¸ªé”®å€¼å¯¹ (key, value)ï¼Œå…¶ä¸­ key å¯è§†ä¸ºééšæ„çº¿ç´¢ï¼Œï¼ˆä¸‹é¢è¿™å¥æ˜¯æˆ‘è‡ªå·±ä¹±æƒ³çš„ï¼‰value å¯ä»¥è§†ä¸ºè¯¥çº¿ç´¢çš„ç›¸å…³å±æ€§
3. é€šè¿‡æ³¨æ„åŠ›æ± åŒ–å±‚æ¥æœ‰åå‘æ€§çš„é€‰æ‹©æŸäº›è¾“å…¥

### éå‚æ•°æ³¨æ„åŠ›æ±‡èšï¼šNadaraya-Watson æ ¸å›å½’

å®é™…ä¸Šä¸€ä¸ªæ•°å­¦è¡¨è¾¾çš„ä¾‹å­èƒ½å¤Ÿæ›´æ¸…æ¥šå±•ç¤ºè¿™ä¸‰ä¸ªæ¦‚å¿µã€‚ç»™å®šæ•°æ® $(x_i, y_i), i=1,...,n$

ç»™å®šæŸ¥è¯¢ $x$ï¼Œå¹³å‡æ± åŒ–å°†è·å¾—è¾“å‡º
$$
f(x) = \frac{1}{n}\sum_{i}{y_i}
$$
è¿™å°±æ˜¯æ²¡æœ‰æ³¨æ„åŠ›æœºåˆ¶çš„æƒ…å†µï¼Œä¸æŸ¥è¯¢å€¼æ— å…³ï¼Œå…¨å‡­ééšæ„çº¿ç´¢è·å¾—è¾“å‡ºã€‚è€Œæ›´å¥½çš„æ–¹æ¡ˆæ˜¯60å¹´ä»£æå‡ºæ¥çš„ Nadaraya-Waston æ ¸å›å½’
$$
f(x)=\sum_{i=1}^{n} \frac{K\left(x-x_{i}\right)}{\sum_{j=1}^{n} K\left(x-x_{j}\right)} y_{i}
$$
å…¶ä¸­ K å¯ä»¥çœ‹ä½œä¸€ä¸ªæ ¸å‡½æ•°ï¼Œä¾‹å¦‚ä¸€ä¸ªé«˜æ–¯æ ¸ï¼Œç”¨äºè¡¡é‡ä¸¤ç‚¹ä¹‹é—´çš„è·ç¦»
$$
K(u)=\frac{1}{\sqrt{2 \pi}} \exp \left(-\frac{u^{2}}{2}\right)
$$
è¿™å°±å°†æ³¨æ„åŠ›æœºåˆ¶æ˜¾å¼åœ°ç”¨äºè¾“å‡ºï¼Œä¹Ÿå°±æ˜¯ç»™å„ä¸ª value åŠ å…¥ç›¸å…³æƒé‡ã€‚ç°åœ¨å†æ¥çœ‹è¿™ä¸ªå›¾ç¤ºå¯èƒ½ä¼šæ›´å¥½

<img src="D2L 10 æ³¨æ„åŠ›æœºåˆ¶/image-20211210215054690.png" style="zoom:80%;" />

### å¸¦å‚æ•°æ³¨æ„åŠ›æ±‡èš

éå‚æ•°çš„Nadaraya-Watsonæ ¸å›å½’å…·æœ‰ä¸€è‡´æ€§ï¼ˆconsistencyï¼‰çš„ä¼˜ç‚¹ï¼šå¦‚æœæœ‰è¶³å¤Ÿçš„æ•°æ®ï¼Œæ­¤æ¨¡å‹ä¼šæ”¶æ•›åˆ°æœ€ä¼˜ç»“æœã€‚å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä»¬è¿˜æ˜¯å¯ä»¥è½»æ¾åœ°å°†å¯å­¦ä¹ çš„å‚æ•°é›†æˆåˆ°æ³¨æ„åŠ›æ±‡èšä¸­
$$
\begin{aligned}
f(x) &=\sum_{i=1}^{n} \alpha\left(x, x_{i}\right) y_{i} \\
&=\sum_{i=1}^{n} \frac{\exp \left(-\frac{1}{2}\left(\left(x-x_{i}\right) w\right)^{2}\right)}{\sum_{j=1}^{n} \exp \left(-\frac{1}{2}\left(\left(x-x_{j}\right) w\right)^{2}\right)} y_{i} \\
&=\sum_{i=1}^{n} \operatorname{softmax}\left(-\frac{1}{2}\left(\left(x-x_{i}\right) w\right)^{2}\right) y_{i} .
\end{aligned}
$$
æ³¨æ„è¿™é‡Œ $w$ åªæ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œå¦‚æœ $w$ è¶Šå¤§è¯´æ˜è¶Šæ³¨æ„è¿‘è·ç¦»çš„é”®å€¼ã€‚è¿™é‡Œæä¸€ä¸‹ï¼Œä¸€ä¸ªè®­ç»ƒæ ·æœ¬çš„è¾“å…¥éƒ½ä¼šå’Œ**é™¤è‡ªå·±ä»¥å¤–**çš„æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„â€œé”®ï¼å€¼â€å¯¹è¿›è¡Œè®¡ç®—ï¼Œå¦‚æœåŠ å…¥è‡ªå·±çš„ key é‚£ä¹ˆè®­ç»ƒç»“æœå¯æƒ³è€ŒçŸ¥ï¼Œå°±æ˜¯ç»™è‡ªå·±çš„ key åŠ å…¥å¾ˆå¤§çš„æƒé‡

### æ³¨æ„åŠ›å¯è§†åŒ–

å¹³å‡æ± åŒ–å¯ä»¥è¢«è§†ä¸ºè¾“å…¥çš„åŠ æƒå¹³å‡å€¼ï¼Œåªæ˜¯æƒé‡ç›¸ç­‰ã€‚è€Œæ³¨æ„åŠ›æ± åŒ–åˆ™æ˜¯çœŸæ­£çš„åŠ æƒå¹³å‡ï¼Œå…¶ä¸­æƒé‡æ˜¯åœ¨ç»™å®šçš„æŸ¥è¯¢ query å’Œä¸åŒçš„é”® key ä¹‹é—´è®¡ç®—å¾—å‡ºçš„ã€‚æ•™æè¿™é‡Œå†™äº†ä¸€äº›ä»£ç ï¼Œåœ¨ä¹‹åç”¨äºæ³¨æ„åŠ›å¯è§†åŒ–ï¼Œä»¥æç»˜å›¾åƒ $weight = f(query, key)$ ï¼Œè¿™é‡Œç›´æ¥çœ‹çœ‹ä¸Šä¸€å°èŠ‚ä¸­çš„éå‚ N-W æ ¸å›å½’ï¼ˆå·¦ä¾§ï¼‰ & å¸¦å‚ N-W æ ¸å›å½’ï¼ˆå³ä¾§ï¼‰çš„å›¾åƒ

<img src="D2L 10 æ³¨æ„åŠ›æœºåˆ¶/image-20211212175647738.png" style="zoom:80%;" />

å¯ä»¥æ˜æ˜¾çœ‹åˆ°æ³¨æ„åŠ›æƒé‡åœ¨ $query = key$ çš„æ—¶å€™åŠ é‡äº†

## æ³¨æ„åŠ›è¯„åˆ†å‡½æ•°

æˆ‘ä»¬å¯ä»¥å°†ä¸Šä¸€èŠ‚ä¸­çš„é«˜æ–¯æ ¸æŒ‡æ•°éƒ¨åˆ†è§†ä¸ºæ³¨æ„åŠ›è¯„åˆ†å‡½æ•°ï¼ˆattention scoring functionï¼‰ï¼Œç®€ç§°è¯„åˆ†å‡½æ•°ï¼ˆscoring functionï¼‰ï¼Œç„¶åæŠŠè¿™ä¸ªå‡½æ•°çš„è¾“å‡ºç»“æœè¾“å…¥åˆ° softmax å‡½æ•°ä¸­è¿›è¡Œè¿ç®—ï¼ˆè¿™æ ·å°±èƒ½ä½¿è¯„åˆ†ä»¥æ¦‚ç‡åˆ†å¸ƒå½¢å¼å±•ç°ï¼Œæ¢å¥è¯è¯´å°±æ˜¯ä½¿æƒé‡çš„å’Œä¸ºä¸€ï¼‰

è®© score function è¡¨ç¤ºæ›´åŠ æ•°å­¦åŒ–
$$
\alpha\left(\mathbf{q}, \mathbf{k}_{i}\right)=\operatorname{softmax}\left(a\left(\mathbf{q}, \mathbf{k}_{i}\right)\right)=\frac{\exp \left(a\left(\mathbf{q}, \mathbf{k}_{i}\right)\right)}{\sum_{j=1}^{m} \exp \left(a\left(\mathbf{q}, \mathbf{k}_{j}\right)\right)} \in \mathbb{R}
$$
æ­¤æ—¶æ³¨æ„åŠ›æ±‡èšå‡½æ•° $f$ å°±è¢«è¡¨ç¤ºä¸º
$$
f\left(\mathbf{q},\left(\mathbf{k}_{1}, \mathbf{v}_{1}\right), \ldots,\left(\mathbf{k}_{m}, \mathbf{v}_{m}\right)\right)=\sum_{i=1}^{m} \alpha\left(\mathbf{q}, \mathbf{k}_{i}\right) \mathbf{v}_{i} \in \mathbb{R}^{v}
$$
åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»ä¸¤ä¸ªæµè¡Œçš„è¯„åˆ†å‡½æ•°ï¼Œç¨åå°†ç”¨ä»–ä»¬æ¥å®ç°æ›´å¤æ‚çš„æ³¨æ„åŠ›æœºåˆ¶

### æ©è”½ softmax æ“ä½œ

åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¹¶éæ‰€æœ‰çš„å€¼éƒ½åº”è¯¥è¢«çº³å…¥åˆ°æ³¨æ„åŠ›æ±‡èšä¸­ã€‚ä¾‹å¦‚ï¼Œä¸ºäº†é«˜æ•ˆå¤„ç†å°æ‰¹é‡æ•°æ®é›†ï¼ŒæŸäº›æ–‡æœ¬åºåˆ—è¢«å¡«å……äº†æ²¡æœ‰æ„ä¹‰çš„ç‰¹æ®Šè¯å…ƒã€‚ä¸ºäº†ä»…å°†æœ‰æ„ä¹‰çš„è¯å…ƒä½œä¸ºå€¼æ¥è·å–æ³¨æ„åŠ›æ±‡èšï¼Œæˆ‘ä»¬å¯ä»¥æŒ‡å®šä¸€ä¸ªæœ‰æ•ˆåºåˆ—é•¿åº¦ï¼Œä»¥ä¾¿åœ¨è®¡ç®—softmaxæ—¶è¿‡æ»¤æ‰è¶…å‡ºæŒ‡å®šèŒƒå›´çš„ä½ç½®ã€‚çœ‹çœ‹å¤§æ¦‚æ•ˆæœæ˜¯ä»€ä¹ˆæ ·å­

```python
# (batch, num_query, num_key)
masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))

tensor([[[0.4527, 0.5473, 0.0000, 0.0000],
         [0.3458, 0.6542, 0.0000, 0.0000]],

        [[0.4151, 0.3528, 0.2321, 0.0000],
         [0.2604, 0.2631, 0.4765, 0.0000]]])
```

### åŠ æ€§æ³¨æ„åŠ›

å½“æŸ¥è¯¢å’Œé”®æ˜¯ä¸åŒé•¿åº¦çš„çŸ¢é‡æ—¶ï¼Œ æˆ‘ä»¬å¯ä»¥ä½¿ç”¨åŠ æ€§æ³¨æ„åŠ›ï¼ˆadditive attentionï¼‰ä½œä¸ºè¯„åˆ†å‡½æ•°
$$
a(\mathbf{q}, \mathbf{k})=\mathbf{w}_{v}^{\top} \tanh \left(\mathbf{W}_{q} \mathbf{q}+\mathbf{W}_{k} \mathbf{k}\right) \in \mathbb{R}
$$
ä¸Šä»£ç ï¼Œæ³¨æ„ä»£ç ä¸­å¯¹ query, key, value çš„æ•°é‡ï¼ˆsequence length / time stepï¼‰å…·æœ‰ä¸€èˆ¬æ€§ï¼Œå®ƒä»¬çš„å½¢çŠ¶è¡¨ç¤ºä¸º (batch_size, seq_len, feature)ï¼Œæ•™æä¸­ä½¿ç”¨çš„ query, key, value å…·ä½“å½¢çŠ¶ä¸ºï¼š(2, 1, 20)ï¼Œ(2, 10, 2) å’Œ (2, 10, 4)ï¼Œå…·ä½“èµ°ä¸€éä¼šæ¯”è¾ƒæ¸…æ™°

```python
#@save
class AdditiveAttention(nn.Module):
    """åŠ æ€§æ³¨æ„åŠ›"""
    def __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs):
        super(AdditiveAttention, self).__init__(**kwargs)
        # æ— åç½®
        # nn.Linear çš„ input ä¸º (*,H_in) * ä»£è¡¨ä»»æ„æ•°é‡çš„ç»´åº¦ï¼ŒåŒ…æ‹¬ none 
        self.W_k = nn.Linear(key_size, num_hiddens, bias=False)
        self.W_q = nn.Linear(query_size, num_hiddens, bias=False)
        self.w_v = nn.Linear(num_hiddens, 1, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, queries, keys, values, valid_lens):
        queries, keys = self.W_q(queries), self.W_k(keys)
        # åœ¨ç»´åº¦æ‰©å±•åï¼Œ
        # queriesçš„å½¢çŠ¶ï¼š(batch_sizeï¼ŒæŸ¥è¯¢çš„ä¸ªæ•°ï¼Œ1ï¼Œnum_hidden)
        # keyçš„å½¢çŠ¶ï¼š(batch_sizeï¼Œ1ï¼Œâ€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°ï¼Œnum_hiddens)
        # ä½¿ç”¨å¹¿æ’­æ–¹å¼è¿›è¡Œæ±‚å’Œ
        features = queries.unsqueeze(2) + keys.unsqueeze(1)
        features = torch.tanh(features)
        # self.w_vä»…æœ‰ä¸€ä¸ªè¾“å‡ºï¼Œå› æ­¤ä»å½¢çŠ¶ä¸­ç§»é™¤æœ€åé‚£ä¸ªç»´åº¦ã€‚
        # scoresçš„å½¢çŠ¶ï¼š(batch_sizeï¼ŒæŸ¥è¯¢çš„ä¸ªæ•°ï¼Œâ€œé”®-å€¼â€å¯¹çš„ä¸ªæ•°)
        scores = self.w_v(features).squeeze(-1)
        self.attention_weights = masked_softmax(scores, valid_lens)
        # valuesçš„å½¢çŠ¶ï¼š(batch_sizeï¼Œâ€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°ï¼Œå€¼çš„ç»´åº¦)
        # ä½¿ç”¨ batch matrix multiplicationï¼ˆ@ é‡è½½ç¬¦æ—¢å¯ä»¥è®¡ç®— mm åˆå¯ä»¥è®¡ç®— bmmï¼‰
        # ä½¿ç”¨ dropout è¿›è¡Œæ­£åˆ™åŒ–
        return torch.bmm(self.dropout(self.attention_weights), values)
```

è¿è¡Œä¸‹é¢çš„ä»£ç 

```python
queries, keys = torch.normal(0, 1, (2, 1, 20)), torch.ones((2, 10, 2))
# valuesçš„å°æ‰¹é‡ï¼Œä¸¤ä¸ªå€¼çŸ©é˜µæ˜¯ç›¸åŒçš„
values = torch.arange(40, dtype=torch.float32).reshape(1, 10, 4).repeat(
    2, 1, 1)
valid_lens = torch.tensor([2, 6])

attention = AdditiveAttention(key_size=2, query_size=20, num_hiddens=8,
                              dropout=0.1)
attention.eval()
attention(queries, keys, values, valid_lens)
```

ç»“æœå¦‚ä¸‹

```python
tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],

        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=<BmmBackward0>)
```

###  ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›

ä½¿ç”¨ç‚¹ç§¯å¯ä»¥å¾—åˆ°è®¡ç®—æ•ˆç‡æ›´é«˜çš„è¯„åˆ†å‡½æ•°ï¼Œä½†æ˜¯ç‚¹ç§¯æ“ä½œè¦æ±‚æŸ¥è¯¢å’Œé”®å…·æœ‰ç›¸åŒçš„é•¿åº¦ d

å‡è®¾æŸ¥è¯¢å’Œé”®çš„æ‰€æœ‰å…ƒç´ éƒ½æ˜¯ç‹¬ç«‹çš„éšæœºå˜é‡ï¼Œ å¹¶ä¸”éƒ½æ»¡è¶³é›¶å‡å€¼å’Œå•ä½æ–¹å·®ï¼Œ é‚£ä¹ˆä¸¤ä¸ªå‘é‡çš„ç‚¹ç§¯çš„å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º dã€‚ä¸ºç¡®ä¿æ— è®ºå‘é‡é•¿åº¦å¦‚ä½•ï¼Œç‚¹ç§¯çš„æ–¹å·®åœ¨ä¸è€ƒè™‘å‘é‡é•¿åº¦çš„æƒ…å†µä¸‹ä»ç„¶æ˜¯1ï¼Œæˆ‘ä»¬å°†ç‚¹ç§¯é™¤ä»¥ $\sqrt d$
$$
a(\mathbf{q}, \mathbf{k})=\mathbf{q}^{\top} \mathbf{k} / \sqrt{d}
$$
æŸ¥è¯¢ $Q\in R^{n\times d}$ï¼Œé”® $K \in R^{m \times d}$ ï¼Œå€¼ $V \in R^{m \times v}$ çš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆscaled dot-product attentionï¼‰æ˜¯ï¼š
$$
\operatorname{softmax}\left(\frac{\mathbf{Q K}^{\top}}{\sqrt{d}}\right) \mathbf{V} \in \mathbb{R}^{n \times v}
$$
å®ƒçš„ä»£ç å°±ç›¸å¯¹ç®€å•ä¸€äº›

```python
#@save
class DotProductAttention(nn.Module):
    """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"""
    def __init__(self, dropout, **kwargs):
        super(DotProductAttention, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)

    # queriesçš„å½¢çŠ¶ï¼š(batch_sizeï¼ŒæŸ¥è¯¢çš„ä¸ªæ•°ï¼Œd)
    # keysçš„å½¢çŠ¶ï¼š(batch_sizeï¼Œâ€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°ï¼Œd)
    # valuesçš„å½¢çŠ¶ï¼š(batch_sizeï¼Œâ€œé”®ï¼å€¼â€å¯¹çš„ä¸ªæ•°ï¼Œå€¼çš„ç»´åº¦)
    # valid_lensçš„å½¢çŠ¶:(batch_sizeï¼Œ)æˆ–è€…(batch_sizeï¼ŒæŸ¥è¯¢çš„ä¸ªæ•°)
    def forward(self, queries, keys, values, valid_lens=None):
        d = queries.shape[-1]
        # è®¾ç½®transpose_b=Trueä¸ºäº†äº¤æ¢keysçš„æœ€åä¸¤ä¸ªç»´åº¦
        scores = torch.bmm(queries, keys.transpose(1,2)) / math.sqrt(d)
        self.attention_weights = masked_softmax(scores, valid_lens)
        return torch.bmm(self.dropout(self.attention_weights), values)
```

## Bahdanau æ³¨æ„åŠ›

ä¹‹å‰æ•™æè®¨è®ºäº†æœºå™¨ç¿»è¯‘é—®é¢˜ï¼šé€šè¿‡ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œç”¨äºåºåˆ—åˆ°åºåˆ—å­¦ä¹ ã€‚å…·ä½“æ¥è¯´ï¼Œç¼–ç å™¨å°†é•¿åº¦å¯å˜çš„åºåˆ—è½¬æ¢ä¸ºå›ºå®šå½¢çŠ¶çš„ä¸Šä¸‹æ–‡å˜é‡ï¼Œç„¶åè§£ç å™¨æ ¹æ®ç”Ÿæˆçš„è¯å…ƒå’Œä¸Šä¸‹æ–‡å˜é‡æŒ‰è¯å…ƒç”Ÿæˆè¾“å‡ºï¼ˆç›®æ ‡ï¼‰åºåˆ—è¯å…ƒã€‚ç„¶è€Œï¼Œå³ä½¿å¹¶éæ‰€æœ‰è¾“å…¥ï¼ˆæºï¼‰è¯å…ƒéƒ½å¯¹è§£ç æŸä¸ªè¯å…ƒéƒ½æœ‰ç”¨ï¼Œä½†æˆ‘ä»¬åœ¨æ¯ä¸ªè§£ç æ­¥éª¤ä¸­ä»ä½¿ç”¨ç¼–ç ç›¸åŒçš„ä¸Šä¸‹æ–‡å˜é‡

æœ‰ä»€ä¹ˆæ–¹æ³•èƒ½åœ¨ä¸åŒè§£ç æ­¥éª¤ä¸­ï¼Œ**ä½¿ç”¨ä¸åŒçš„ä¸Šä¸‹æ–‡å˜é‡**å‘¢ï¼Ÿè¿™ä¸ªæ—¶å€™ Bahdanau æ³¨æ„åŠ›æœºåˆ¶å°±ç™»åœºäº†ï¼Œä¸‹é¢å…·ä½“æ¥çœ‹çœ‹å§

### æ¨¡å‹

ä¸Šä¸‹æ–‡å˜é‡ä»»æ„è§£ç æ—¶é—´æ­¥ $t'$ ä¼šè¢«æ›¿æ¢ä¸º $c_{t'}$
$$
\mathbf{c}_{t^{\prime}}=\sum_{t=1}^{T} \alpha\left(\mathbf{s}_{t^{\prime}-1}, \mathbf{h}_{t}\right) \mathbf{h}_{t}
$$
å…¶ä¸­ï¼Œ$h_t$ ä¸º**ç¼–ç å™¨** t æ—¶é—´æ­¥çš„éšçŠ¶æ€ï¼Œå®ƒå³ä½¿ key åˆæ˜¯ valueï¼›$s_{t'-1}$ ä¸º**è§£ç å™¨** $t'-1$ æ—¶åˆ»æ­¥çš„éšçŠ¶æ€ï¼›æ³¨æ„åŠ›æƒé‡ $\alpha$ æ˜¯ä¹‹å‰å®šä¹‰çš„åŠ æ€§æ³¨æ„åŠ›æ‰“åˆ†å‡½æ•°ã€‚çœ‹ä¸€ä¸‹æ¨¡å‹ä»£ç 

```python
class Seq2SeqAttentionDecoder(AttentionDecoder):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)
        self.attention = d2l.AdditiveAttention(
            num_hiddens, num_hiddens, num_hiddens, dropout)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.GRU(
            embed_size + num_hiddens, num_hiddens, num_layers,
            dropout=dropout)
        self.dense = nn.Linear(num_hiddens, vocab_size)

    def init_state(self, enc_outputs, enc_valid_lens, *args):
        # outputsçš„å½¢çŠ¶ä¸º(batch_sizeï¼Œnum_stepsï¼Œnum_hiddens).
        # hidden_stateçš„å½¢çŠ¶ä¸º(num_layersï¼Œbatch_sizeï¼Œnum_hiddens)
        outputs, hidden_state = enc_outputs
        return (outputs.permute(1, 0, 2), hidden_state, enc_valid_lens)

    def forward(self, X, state):
        # X.shape = (num_batch, num_steps) å­˜å‚¨è¯å…ƒçš„ç´¢å¼•
        # enc_outputsçš„å½¢çŠ¶ä¸º(batch_size,num_steps,num_hiddens).
        # hidden_stateçš„å½¢çŠ¶ä¸º(num_layers,batch_size,
        # num_hiddens)
        enc_outputs, hidden_state, enc_valid_lens = state
        # è¾“å‡ºXçš„å½¢çŠ¶ä¸º(num_steps,batch_size,embed_size)
        X = self.embedding(X).permute(1, 0, 2)
        outputs, self._attention_weights = [], []
        for x in X:
            # queryçš„å½¢çŠ¶ä¸º(batch_size,1,num_hiddens)
            query = torch.unsqueeze(hidden_state[-1], dim=1)
            # contextçš„å½¢çŠ¶ä¸º(batch_size,1,num_hiddens)
            context = self.attention(
                query, enc_outputs, enc_outputs, enc_valid_lens)
            # åœ¨ç‰¹å¾ç»´åº¦ä¸Šè¿ç»“
            x = torch.cat((context, torch.unsqueeze(x, dim=1)), dim=-1)
            # å°†xå˜å½¢ä¸º(1,batch_size,embed_size+num_hiddens)
            out, hidden_state = self.rnn(x.permute(1, 0, 2), hidden_state)
            outputs.append(out)
            self._attention_weights.append(self.attention.attention_weights)
        # å…¨è¿æ¥å±‚å˜æ¢åï¼Œoutputsçš„å½¢çŠ¶ä¸º
        # (num_steps,batch_size,vocab_size)
        outputs = self.dense(torch.cat(outputs, dim=0))
        return outputs.permute(1, 0, 2), [enc_outputs, hidden_state,
                                          enc_valid_lens]

    @property
    def attention_weights(self):
        return self._attention_weights
```

å…³äº `nn.Embedding`ï¼Œå…¶å®åœ¨ä¹‹å‰ seq2seq ä¸­çš„ä»£ç ä¹Ÿæœ‰ä½¿ç”¨ï¼Œæ•´ç†å‡ ä¸ªç‚¹ï¼š

1. Embedding å±‚å°†æ¯ä¸ªè¯å…ƒè½¬åŒ–ä¸º embed_size ç»´åº¦çš„å‘é‡ï¼ˆä¹Ÿç§°ä¸ºè¯å‘é‡ï¼‰
2. Embedding layer å­˜å‚¨äº†ä¸€ä¸ªå‚æ•°çŸ©é˜µ (vocab_size, embed_size) æ˜¯å¯ä»¥éšç€è®­ç»ƒæ›´æ–°çš„
3. ç»è¿‡è®­ç»ƒä¹‹åç›¸ä¼¼è¯å…ƒçš„è¯å‘é‡å¯èƒ½ä¼šå˜å¾—æ›´æ¥è¿‘

æ¥çœ‹çœ‹å…¶ä¸­çš„æƒé‡æ˜¯ä»€ä¹ˆæ ·å­çš„å§ï¼Œè¾“å…¥å’Œè¾“å‡ºåˆ†åˆ«ä¸ºï¼š `i'm home . => je suis chez moi .`

![image-20211213155221786](D2L 10 æ³¨æ„åŠ›æœºåˆ¶/image-20211213155221786.png)

åº”è¯¥æ˜¯åŠ å…¥äº† `<eos>` ç‰¹æ®Šè¯å…ƒï¼Œæ‰€ä»¥ (key positions, Query positions) = (3+1, 5+1)

## å¤šå¤´æ³¨æ„åŠ›

å¤šå¤´æ³¨æ„åŠ›è¿™ä¸€éƒ¨åˆ†çš„è§†é¢‘è®²è§£æ˜¯åœ¨ Transformer ä¸­è¿›è¡Œçš„ [bilibili](https://www.bilibili.com/video/BV1Kq4y1H7FL?p=1&t=1256)

åœ¨å®è·µä¸­ï¼Œå½“ç»™å®šç›¸åŒçš„æŸ¥è¯¢ã€é”®å’Œå€¼çš„é›†åˆæ—¶ï¼Œ æˆ‘ä»¬å¸Œæœ›æ¨¡å‹å¯ä»¥**åŸºäºç›¸åŒçš„æ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ åˆ°ä¸åŒçš„è¡Œä¸º**ï¼Œç„¶åå°†ä¸åŒçš„è¡Œä¸ºä½œä¸ºçŸ¥è¯†ç»„åˆèµ·æ¥ï¼Œæ•è·åºåˆ—å†…å„ç§èŒƒå›´çš„ä¾èµ–å…³ç³»ï¼ˆä¾‹å¦‚ï¼ŒçŸ­è·ç¦»ä¾èµ–å’Œé•¿è·ç¦»ä¾èµ–å…³ç³»ï¼‰

å¯¹äºå…¶ä¸­ä¸€ä¸ªå¤´ $i$ çš„æ“ä½œç®€è¿°å¦‚ä¸‹ï¼šæ˜¯å¯¹ query, key, value å…ˆä½¿ç”¨å…¨è¿æ¥å±‚è¿›è¡Œç»´åº¦è½¬æ¢ï¼Œè½¬æ¢åˆ°ä¸€ä¸ªç›¸åŒçš„ç»´åº¦ $p_v$ï¼Œç„¶åå†ä½¿ç”¨ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ã€‚æ¯ä¸€ä¸ªå¤´éƒ½å°†è¿›è¡Œè¿™æ ·çš„æ“ä½œï¼Œå‡è®¾æœ‰ $m$ ä¸ªå¤´ï¼Œé‚£ä¹ˆå°±ä¼šå¾—åˆ° $m$ ä¸ªæ³¨æ„åŠ›æ±‡èšè¾“å‡º $h_i, i=1,...,m$ï¼Œæœ€åå°†æ‰€æœ‰çš„ $h_i$ è¿æ¥èµ·æ¥ï¼Œä½¿ç”¨ä¸€ä¸ªå…¨è¿æ¥å±‚è¿›è¡Œç‰¹å¾ç»„åˆå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºï¼Œæ•°å­¦å½¢å¼å¦‚ä¸‹ï¼š
$$
\mathbf{h}_{i}=f\left(\mathbf{W}_{i}^{(q)} \mathbf{q}, \mathbf{W}_{i}^{(k)} \mathbf{k}, \mathbf{W}_{i}^{(v)} \mathbf{v}\right) \in \mathbb{R}^{p_{v}}
\\
\mathbf{W}_{o}\left[\begin{array}{c}
\mathbf{h}_{1} \\
\vdots \\
\mathbf{h}_{h}
\end{array}\right] \in \mathbb{R}^{p_{o}}
$$
è¿™é‡Œ $W_0$ æ˜¯ä¸€ä¸ª $p_0 \times p_0$ çš„çŸ©é˜µï¼Œ$p_0 = \text{number of heads} \times p_v$ï¼Œå›¾ç¤ºå¦‚ä¸‹

<img src="D2L 10 æ³¨æ„åŠ›æœºåˆ¶/image-20211213235228429.png" style="zoom:80%;" />

åœ¨ä½¿ç”¨ä»£ç å®ç°çš„æ—¶å€™å‘ç°ï¼Œå¯ä»¥æŠŠ `num_heads` ä¸ª $W_{i}q$ ä½¿ç”¨ä¸€ä¸ªå¤§çš„ $W_q$ ä»£æ›¿ï¼Œç„¶åå°†å˜æ¢åçš„ç‰¹å¾è¿›è¡Œåˆ†å‰²å³å¯ã€‚å…¶ä»–ä¸¤ä¸ªçŸ©é˜µåŒç†ï¼Œè¿™æ ·ä¾¿äºå¹¶è¡Œå¤„ç†ï¼Œå¹¶ä¸”ä»£ç æ›´ç®€æ´

```python
#@save
class MultiHeadAttention(nn.Module):
    """å¤šå¤´æ³¨æ„åŠ›"""
    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 num_heads, dropout, bias=False, **kwargs):
        super(MultiHeadAttention, self).__init__(**kwargs)
        self.num_heads = num_heads
        self.attention = d2l.DotProductAttention(dropout)
        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)
        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)
        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)
        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)

    def forward(self, queries, keys, values, valid_lens):
        queries = transpose_qkv(self.W_q(queries), self.num_heads)
        keys = transpose_qkv(self.W_k(keys), self.num_heads)
        values = transpose_qkv(self.W_v(values), self.num_heads)

        if valid_lens is not None:
            # åœ¨è½´0ï¼Œå°†ç¬¬ä¸€é¡¹ï¼ˆæ ‡é‡æˆ–è€…çŸ¢é‡ï¼‰å¤åˆ¶num_headsæ¬¡
            valid_lens = torch.repeat_interleave(
                valid_lens, repeats=self.num_heads, dim=0)

        # outputçš„å½¢çŠ¶:(batch_size*num_headsï¼Œnum_query,
        # num_hiddens/num_heads)
        output = self.attention(queries, keys, values, valid_lens)

        output_concat = transpose_output(output, self.num_heads)
        return self.W_o(output_concat)

#@save
def transpose_qkv(X, num_heads):
    """ä¸ºäº†å¤šæ³¨æ„åŠ›å¤´çš„å¹¶è¡Œè®¡ç®—è€Œå˜æ¢å½¢çŠ¶"""
    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)

    X = X.permute(0, 2, 1, 3)

    # æœ€ç»ˆè¾“å‡ºçš„å½¢çŠ¶:(batch_size*num_heads, num_query or num_keys,
    # num_hiddens/num_heads)
    return X.reshape(-1, X.shape[2], X.shape[3])


#@save
def transpose_output(X, num_heads):
    """é€†è½¬transpose_qkvå‡½æ•°çš„æ“ä½œ"""
    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])
    X = X.permute(0, 2, 1, 3)
    return X.reshape(X.shape[0], X.shape[1], -1)
```

## è‡ªæ³¨æ„åŠ›å’Œä½ç½®ç¼–ç 

ç»ˆäºè¿›å…¥è‡ªæ³¨æ„åŠ›éƒ¨åˆ†äº†ï¼Œç¦» transformer åªæœ‰ä¸€æ­¥ä¹‹é¥ï¼

åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ç»å¸¸ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æˆ–å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å¯¹åºåˆ—è¿›è¡Œç¼–ç ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œæœ‰äº†æ³¨æ„åŠ›æœºåˆ¶ä¹‹åï¼Œæˆ‘ä»¬å°†è¯å…ƒåºåˆ—è¾“å…¥æ³¨æ„åŠ›æ± åŒ–å±‚ä¸­ï¼ŒåŒä¸€ç»„è¯å…ƒåŒæ—¶å……å½“æŸ¥è¯¢ã€é”®å’Œå€¼ã€‚æ¯ä¸ªæŸ¥è¯¢éƒ½ä¼šå…³æ³¨æ‰€æœ‰çš„é”®ï¼å€¼å¯¹å¹¶ç”Ÿæˆä¸€ä¸ªæ³¨æ„åŠ›è¾“å‡ºã€‚ç”±äºæŸ¥è¯¢ã€é”®å’Œå€¼æ¥è‡ªåŒä¸€ç»„è¾“å…¥ï¼Œå› æ­¤è¢«ç§°ä¸º è‡ªæ³¨æ„åŠ›ï¼ˆself-attentionï¼‰åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è‡ªæ³¨æ„åŠ›è¿›è¡Œåºåˆ—ç¼–ç ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨åºåˆ—çš„é¡ºåºä½œä¸ºè¡¥å……ä¿¡æ¯

### è‡ªæ³¨æ„åŠ›

ç»™å®šä¸€ä¸ªç”±è¯å…ƒç»„æˆçš„è¾“å…¥åºåˆ—ï¼Œå…¶ä¸­ä»»æ„ $x_i \in R^ d$ã€‚è¯¥åºåˆ—çš„è‡ªæ³¨æ„åŠ›è¾“å‡ºä¸ºä¸€ä¸ªé•¿åº¦ç›¸åŒçš„åºåˆ—ï¼š
$$
\mathbf{y}_{i}=f\left(\mathbf{x}_{i},\left(\mathbf{x}_{1}, \mathbf{x}_{1}\right), \ldots,\left(\mathbf{x}_{n}, \mathbf{x}_{n}\right)\right) \in \mathbb{R}^{d}
$$

### æ¯”è¾ƒå·ç§¯ç¥ç»ç½‘ç»œã€å¾ªç¯ç¥ç»ç½‘ç»œå’Œè‡ªæ³¨æ„åŠ›

ä¸‹å›¾ä¸ºä¸‰è€…è®¡ç®—çš„å›¾ç¤º

<img src="D2L 10 æ³¨æ„åŠ›æœºåˆ¶/image-20211213171227551.png" style="zoom:80%;" />

ç°åœ¨è®©æˆ‘ä»¬æ¯”è¾ƒè¿™ä¸‰ä¸ªæ¶æ„ï¼Œç›®æ ‡éƒ½æ˜¯å°†ç”± n ä¸ªè¯å…ƒç»„æˆçš„åºåˆ—æ˜ å°„åˆ°å¦ä¸€ä¸ªé•¿åº¦ç›¸ç­‰çš„åºåˆ—ï¼Œå…¶ä¸­çš„æ¯ä¸ªè¾“å…¥è¯å…ƒæˆ–è¾“å‡ºè¯å…ƒéƒ½ç”± d ç»´å‘é‡è¡¨ç¤ºã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†æ¯”è¾ƒçš„æ˜¯å·ç§¯ç¥ç»ç½‘ç»œã€å¾ªç¯ç¥ç»ç½‘ç»œå’Œè‡ªæ³¨æ„åŠ›è¿™å‡ ä¸ªæ¶æ„çš„è®¡ç®—å¤æ‚æ€§ã€é¡ºåºæ“ä½œå’Œæœ€å¤§è·¯å¾„é•¿åº¦ã€‚è¯·æ³¨æ„ï¼Œ**é¡ºåºæ“ä½œä¼šå¦¨ç¢å¹¶è¡Œè®¡ç®—ï¼Œè€Œä»»æ„çš„åºåˆ—ä½ç½®ç»„åˆä¹‹é—´çš„è·¯å¾„è¶ŠçŸ­ï¼Œåˆ™èƒ½æ›´è½»æ¾åœ°å­¦ä¹ åºåˆ—ä¸­çš„è¿œè·ç¦»ä¾èµ–å…³ç³»**

ä¸‹é¢è¿™ä¸ªè¡¨ä¾ç„¶æ¥è‡ªäºæ²ç¥è§†é¢‘ [bilibili](https://www.bilibili.com/video/BV19o4y1m7mo)ï¼Œéå¸¸æ¸…æ™°åœ°å¯¹æ¯”äº†ä¸‰è€…çš„å…³ç³»ï¼Œå…¶ä¸­ k æ˜¯ä¸€ç»´å·ç§¯æ ¸çš„ kernel size

<img src="D2L 10 æ³¨æ„åŠ›æœºåˆ¶/image-20211213172102475.png" style="zoom: 50%;" />

ç¨å¾®è§£é‡Šä¸€ä¸‹ï¼š

1. æœ€é•¿è·¯å¾„ä¸­çš„è·¯å¾„ä¸ºï¼šä¸¤ä¸ªè¯å…ƒè¿›è¡Œä¿¡æ¯ä¼ é€’çš„è®¡ç®—æ¬¡æ•°
2. å¾ªç¯ç¥ç»ç½‘ç»œçš„éšçŠ¶æ€æ—¶ï¼Œ dÃ—d æƒé‡çŸ©é˜µå’Œ d ç»´éšçŠ¶æ€çš„ä¹˜æ³•è®¡ç®—å¤æ‚åº¦ä¸º $O(d^2)$
3. åœ¨è‡ªæ³¨æ„åŠ›ä¸­ï¼ŒæŸ¥è¯¢ã€é”®å’Œå€¼éƒ½æ˜¯ nÃ—d çŸ©é˜µã€‚ å¹¶ä¸”ä½¿ç”¨ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼Œæ•…è‡ªæ³¨æ„åŠ›è®¡ç®—å¤æ‚åº¦ä¸º $O(n^2d)$ï¼Œå¹¶ä¸”ç”±äºä½¿ç”¨çš„æ˜¯çŸ©é˜µä¹˜æ³•ï¼Œè€ŒçŸ©é˜µä¹˜æ³•çš„å¹¶è¡Œåº¦ä¸º $O(n)$

æ€»è€Œè¨€ä¹‹ï¼Œå·ç§¯ç¥ç»ç½‘ç»œå’Œè‡ªæ³¨æ„åŠ›éƒ½æ‹¥æœ‰å¹¶è¡Œè®¡ç®—çš„ä¼˜åŠ¿ï¼Œè€Œä¸”è‡ªæ³¨æ„åŠ›çš„æœ€å¤§è·¯å¾„é•¿åº¦æœ€çŸ­ã€‚ä½†æ˜¯å› ä¸ºå…¶è®¡ç®—å¤æ‚åº¦æ˜¯å…³äºåºåˆ—é•¿åº¦çš„äºŒæ¬¡æ–¹ï¼Œæ‰€ä»¥åœ¨å¾ˆé•¿çš„åºåˆ—ä¸­è®¡ç®—ä¼šéå¸¸æ…¢

### ä½ç½®ç¼–ç 

ä¸åŒäº CNN å’Œ RNNï¼Œä»¥ä¸Šå¾—åˆ°çš„è‡ªæ³¨æ„åŠ›è®¡ç®—ç»“æœæ˜¯ä¸åŒ…å«ä½ç½®ï¼ˆé¡ºåºï¼‰ä¿¡æ¯çš„ã€‚ä¹Ÿå°±æ˜¯è¯´æ¢ä¸ªè¾“å…¥é¡ºåºï¼Œå¾—åˆ°çš„ç»“æœè¿˜æ˜¯é‚£äº›ç»“æœï¼Œè¿™æ˜¾ç„¶ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„ï¼Œæ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æè¿°çš„æ˜¯åŸºäºæ­£å¼¦å‡½æ•°å’Œä½™å¼¦å‡½æ•°çš„å›ºå®šä½ç½®ç¼–ç ï¼ˆä¸å¾—ä¸åæ§½ä¸€ä¸‹è¿™ä¸ªç¼–ç çœŸçš„ç•¥å¾®æŠ½è±¡

ä»¥ä¸‹æ˜¯æˆ‘çš„ä¸ªäººç†è§£ï¼šå¯¹äºä¸€ä¸ªåºåˆ— `range(n)`ï¼Œæˆ‘éœ€è¦ä½¿ç”¨ d ä¸ªç»´åº¦å¯¹å…¶ä½ç½®è¿›è¡Œç¼–ç ï¼Œé‡‡å–å¦‚ä¸‹ç¼–ç å½¢å¼ï¼Œ $p_{i,j}$ å³æ˜¯ç¬¬ i ä¸ªä½ç½®çš„ç¬¬ j ä¸ªç»´åº¦çš„ç¼–ç æ•°
$$
\begin{aligned}
p_{i, 2 j} &=\sin \left(\frac{i}{10000^{2 j / d}}\right) \\
p_{i, 2 j+1} &=\cos \left(\frac{i}{10000^{2 j / d}}\right)
\end{aligned}
$$
ç¬¬ä¸€æ¬¡çœ‹è¿™ä¸ªç¼–ç çœŸçš„å¤ªè’™åœˆäº†ï¼Œä¸è¿‡æ•™æä¸¾äº†ä¸€ä¸ªä¾‹å­ï¼šç»å¯¹ä½ç½®ä¿¡æ¯ã€‚ä¹Ÿå°±æ˜¯æˆ‘ä»¬ä½¿ç”¨ d ä½äºŒè¿›åˆ¶å¯¹åºåˆ— `range(n)` çš„ä½ç½®è¿›è¡Œç¼–ç ï¼Œè¿™æ ·æ¥çœ‹æ˜¯ä¸æ˜¯å°±ç®€å•ä¸å°‘äº†

```python
# d = 5
0çš„äºŒè¿›åˆ¶æ˜¯ï¼š00000
1çš„äºŒè¿›åˆ¶æ˜¯ï¼š00001
2çš„äºŒè¿›åˆ¶æ˜¯ï¼š00010
3çš„äºŒè¿›åˆ¶æ˜¯ï¼š00011
4çš„äºŒè¿›åˆ¶æ˜¯ï¼š00100
5çš„äºŒè¿›åˆ¶æ˜¯ï¼š00101
6çš„äºŒè¿›åˆ¶æ˜¯ï¼š00110
7çš„äºŒè¿›åˆ¶æ˜¯ï¼š00111
```

è¶Šé«˜ä½çš„æ•°å­—å˜åŒ–å¾—è¶Šæ…¢ï¼Œä¸€å…±èƒ½å¤Ÿç¼–ç  $2^n$ ä¸ªæ•°ã€‚é‚£å¦‚æœæˆ‘ä»¬ç”¨ d ç»´ (0, 1) ä¹‹é—´çš„æ•°å»å¯¹ä½ç½®è¿›è¡Œç¼–ç æ˜¯ä¸æ˜¯ä¹Ÿå¯ä»¥å‘¢ï¼Ÿæ•™æä¸­çš„ä½ç½®ç¼–ç å°±å±äºå…¶ä¸­ä¸€ç§ã€‚è¿˜å¯ä»¥ä»å¹³é¢ç©ºé—´çš„è§’åº¦æ¥è¿›è¡Œç†è§£ï¼Œå‡è®¾æœ‰ d ä¸ªç»´åº¦ï¼Œæ­¤æ—¶æˆ‘ä»¬ç”»å‡º d/2 ä¸ªå¹³é¢

![image-20211213201150635](D2L 10 æ³¨æ„åŠ›æœºåˆ¶/image-20211213201150635.png)

æ—‹è½¬è§’åº¦å°±æ˜¯å¯¹åº”ç€ (cos, sin)ï¼Œéšç€ä½æ•°è¶Šé«˜æ¯æ¬¡ i è¿›ä¸€æ—¶ï¼Œæ—‹è½¬çš„å¹…åº¦è¶Šå°ã€‚ä¸‹é¢æ˜¯ position å’Œ endoding dimension çš„çƒ­åŠ›å›¾

<img src="D2L 10 æ³¨æ„åŠ›æœºåˆ¶/image-20211213201335149.png" style="zoom:80%;" />

æˆ‘å·²ç»å°½åŠ›å»ç†è§£äº†...ä½†æ˜¯è¿˜æ˜¯æœ‰ç‚¹äº‘é‡Œé›¾é‡Œï¼Œä¸è¿‡è¿˜æ˜¯å…ˆç»§ç»­å‰è¡Œå§

## Transformer

ä¸ CNN å’Œ RNN æ¯”è¾ƒï¼Œè‡ªæ³¨æ„åŠ›åŒæ—¶å…·æœ‰å¹¶è¡Œè®¡ç®—å’Œæœ€çŸ­çš„æœ€å¤§è·¯å¾„é•¿åº¦è¿™ä¸¤ä¸ªä¼˜åŠ¿ï¼Œå› æ­¤ï¼Œä½¿ç”¨è‡ªæ³¨æ„åŠ›æ¥è®¾è®¡æ·±åº¦æ¶æ„æ˜¯å¾ˆæœ‰å¸å¼•åŠ›çš„ã€‚å°½ç®¡ transformer æœ€åˆæ˜¯åº”ç”¨äºåœ¨æ–‡æœ¬æ•°æ®ä¸Šçš„åºåˆ—åˆ°åºåˆ—å­¦ä¹ ï¼Œä½†ç°åœ¨å·²ç»æ¨å¹¿åˆ°å„ç§ç°ä»£çš„æ·±åº¦å­¦ä¹ ä¸­ï¼Œä¾‹å¦‚è¯­è¨€ã€è§†è§‰ã€è¯­éŸ³å’Œå¼ºåŒ–å­¦ä¹ é¢†åŸŸ

æˆ‘è®¤ä¸ºè¿™é‡Œè´´è‹±æ–‡çš„å›¾ç¤ºæ¯”è¾ƒå¥½ï¼Œç¬¬ä¸€æ¬¡çœ‹è¿™ä¸ªå›¾è‚¯å®šæ˜¯ä¸€å¤´é›¾æ°´ï¼Œå¯ä»¥å…ˆçœ‹ä»£ç ï¼Œäº†è§£æ¯ä¸ªæ¨¡å—çš„ç»“æ„ï¼Œç„¶åå†æ‹¼èµ·æ¥

<img src="D2L 10 æ³¨æ„åŠ›æœºåˆ¶/image-20211213212144723.png" style="zoom:80%;" />

### åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ ï¼ˆPositionwise FFNï¼‰

åå­—å¾ˆé…·ï¼Œå®é™…ä¸Šæ˜¯ä¸¤ä¸ªå…¨è¿æ¥å±‚

```python
#@save
class PositionWiseFFN(nn.Module):
    """åŸºäºä½ç½®çš„å‰é¦ˆç½‘ç»œ"""
    def __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs,
                 **kwargs):
        super(PositionWiseFFN, self).__init__(**kwargs)
        self.dense1 = nn.Linear(ffn_num_input, ffn_num_hiddens)
        self.relu = nn.ReLU()
        self.dense2 = nn.Linear(ffn_num_hiddens, ffn_num_outputs)

    def forward(self, X):
        return self.dense2(self.relu(self.dense1(X)))
```

### æ®‹å·®è¿æ¥å’Œå±‚è§„èŒƒåŒ–ï¼ˆadd & normï¼‰

æ®‹å·®è¿æ¥æ˜¯å¾ˆå¸¸è§çš„ç½‘ç»œç»“æ„ï¼Œè¿™é‡Œä¸»è¦è®²è®²è§„èŒƒåŒ–ä½¿ç”¨çš„æ˜¯ LayerNormã€‚å‡è®¾æ•°æ® $X$ çš„å½¢çŠ¶ä¸º (batch, num_steps, channels)ï¼ŒBatchNorm é’ˆå¯¹çš„æ˜¯ batch ç»´åº¦ï¼Œæœ€åå¾—åˆ°çš„å‡å€¼å’Œæ–¹å·®å½¢çŠ¶æ˜¯ (num_steps, channels)

ä½†é—®é¢˜æ¥äº†ï¼Œåœ¨è®¡ç®—æœºè§†è§‰ä¸­ num_steps ä¸€èˆ¬ä»£è¡¨çš„æ˜¯å›¾ç‰‡çš„å½¢çŠ¶ (H, W) æ‰€ä»¥æ˜¯ä¸€ä¸ªå›ºå®šçš„å€¼ï¼Œè€Œåœ¨åºåˆ—æ¨¡å‹ä¸­ï¼Œç”±äºæ¯ä¸ªæ ·æœ¬çš„æ—¶é—´æ­¥å¯èƒ½ä¸ä¸€æ ·

æ‰€ä»¥ LayerNorm é’ˆå¯¹çš„æ˜¯ num_steps ç»´åº¦ï¼ˆæ›´å®½æ³›çš„è®²å¯ä»¥æ˜¯é™¤äº† batch ä»¥å¤–çš„å…¶ä»–ç»´åº¦ï¼‰ï¼Œæœ€ç»ˆå¾—åˆ°çš„å‡å€¼å’Œæ–¹å·®å½¢çŠ¶æ˜¯ (batch, channels) or (batch,)ï¼Œè¿™å°±ä¿è¯äº†ç»Ÿä¸€æ€§ï¼Œå› ä¸º batch & channel ä¸€èˆ¬æ˜¯ä¸€ä¸ªå›ºå®šå€¼ã€‚

é™¤äº†ä½¿ç”¨ LayerNorm è¿˜ä½¿ç”¨äº† dropout æ­£åˆ™åŒ–ï¼Œä¸‹é¢æ¥çœ‹çœ‹æ ¸å¿ƒä»£ç ï¼ˆ[LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) è¾“å…¥æ˜¯ normalized_shapeï¼Œæˆ‘ç†è§£ä¸ºè®¡ç®—å•ä¸ªç»Ÿè®¡å€¼ï¼Œæ•°æ®æ‰€éœ€çš„å½¢çŠ¶ï¼‰

```python
#@save
class AddNorm(nn.Module):
    """æ®‹å·®è¿æ¥åè¿›è¡Œå±‚è§„èŒƒåŒ–"""
    def __init__(self, normalized_shape, dropout, **kwargs):
        super(AddNorm, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)
        self.ln = nn.LayerNorm(normalized_shape)

    def forward(self, X, Y):
        return self.ln(self.dropout(Y) + X)
```

### ç¼–ç å™¨

#### EncoderBlock

æœ‰äº†ä»¥ä¸Šä¸¤ä¸ªæ¨¡å—ï¼šFFN & AddNormï¼Œå†åŠ ä¸Šä¹‹å‰ä»‹ç»çš„å¤šå¤´æ³¨æ„åŠ›æ¨¡å—ï¼Œå°±èƒ½å¤Ÿæ„å»ºä¸€ä¸ªå®Œæ•´çš„ transformer ç¼–ç å™¨æ¨¡å—

```python
#@save
class EncoderBlock(nn.Module):
    """transformerç¼–ç å™¨å—"""
    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
                 dropout, use_bias=False, **kwargs):
        super(EncoderBlock, self).__init__(**kwargs)
        self.attention = d2l.MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout,
            use_bias)
        self.addnorm1 = AddNorm(norm_shape, dropout)
        self.ffn = PositionWiseFFN(
            ffn_num_input, ffn_num_hiddens, num_hiddens)
        self.addnorm2 = AddNorm(norm_shape, dropout)

    def forward(self, X, valid_lens):
        Y = self.addnorm1(X, self.attention(X, X, X, valid_lens))
        return self.addnorm2(Y, self.ffn(Y))
```

transformer ä¸€ä¸ªå¾ˆå¥½çš„æ€§è´¨æ˜¯ï¼š**ç¼–ç å™¨ä¸­çš„ä»»ä½•å±‚éƒ½ä¸ä¼šæ”¹å˜å…¶è¾“å…¥çš„å½¢çŠ¶ï¼**

```python
X = torch.ones((2, 100, 24))
valid_lens = torch.tensor([3, 2])
encoder_blk = EncoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5)
encoder_blk.eval()
encoder_blk(X, valid_lens).shape
# torch.Size([2, 100, 24])
```

#### TransformerEncoder

æœ‰äº†å•ä¸ªæ¨¡å—ä¹‹åï¼Œå°±å¯ä»¥å°†å®ƒä»¬å †å èµ·æ¥è·å¾—æ›´å¼ºå¤§ç¼–ç å™¨ï¼Œå½“ç„¶è¿™é‡Œè¿˜æœ‰ä¸¤ä¸ªéœ€è¦æ³¨æ„çš„ç‚¹ï¼š

1. Positional encodingï¼Œç»™æ¯ä¸€ä¸ªåºåˆ—ä½¿ç”¨ä¹‹å‰æ‰€å°†çš„ä¸‰è§’å‡½æ•°ä½ç½®ç¼–ç 
2. ç”±äº embedding çš„æ•°å€¼æ˜¯ç»è¿‡å½’ä¸€åŒ–çš„ï¼Œä¹Ÿå°±æ˜¯è¯´é™¤ä»¥äº† $\sqrt{d}$ï¼Œè€Œ Positional encoding çš„å€¼æ˜¯ (-1, 1) ä¹‹é—´çš„ä¸‰è§’å‡½æ•°ï¼Œä¸ºäº†è®©ä¸¤ä¸ªæ•°ç›¸åŠ ï¼ˆåŠ å…¥ä½ç½®ä¿¡æ¯ï¼‰ï¼Œå¹¶ä¸”è®©äºŒè€…çš„æ•°å€¼å¤§å°ç›¸å·®æ›´å°ï¼Œåˆ™éœ€è¦å°† embedding å†ä¹˜ä»¥é•¿åº¦ $\sqrt{d}$ ä»¥è¿˜åŸ

ä»£ç å¦‚ä¸‹

```python
#@save
class TransformerEncoder(d2l.Encoder):
    """transformerç¼–ç å™¨"""
    def __init__(self, vocab_size, key_size, query_size, value_size,
                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,
                 num_heads, num_layers, dropout, use_bias=False, **kwargs):
        super(TransformerEncoder, self).__init__(**kwargs)
        self.num_hiddens = num_hiddens
        self.embedding = nn.Embedding(vocab_size, num_hiddens)
        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)
        self.blks = nn.Sequential()
        for i in range(num_layers):
            self.blks.add_module("block"+str(i),
                EncoderBlock(key_size, query_size, value_size, num_hiddens,
                             norm_shape, ffn_num_input, ffn_num_hiddens,
                             num_heads, dropout, use_bias))

    def forward(self, X, valid_lens, *args):
        # å› ä¸ºä½ç½®ç¼–ç å€¼åœ¨-1å’Œ1ä¹‹é—´ï¼Œ
        # å› æ­¤åµŒå…¥å€¼ä¹˜ä»¥åµŒå…¥ç»´åº¦çš„å¹³æ–¹æ ¹è¿›è¡Œç¼©æ”¾ï¼Œ
        # ç„¶åå†ä¸ä½ç½®ç¼–ç ç›¸åŠ ã€‚
        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))
        self.attention_weights = [None] * len(self.blks)
        for i, blk in enumerate(self.blks):
            X = blk(X, valid_lens)
            self.attention_weights[
                i] = blk.attention.attention.attention_weights
        return X
```

ä¸‹é¢æˆ‘ä»¬æŒ‡å®šäº†è¶…å‚æ•°æ¥åˆ›å»ºä¸€ä¸ªä¸¤å±‚çš„ transformer ç¼–ç å™¨ã€‚Transformer ç¼–ç å™¨è¾“å‡ºçš„å½¢çŠ¶æ˜¯ï¼ˆæ‰¹é‡å¤§å°ï¼Œæ—¶é—´æ­¥æ•°ç›®ï¼Œ`num_hiddens`ï¼‰

```python
encoder = TransformerEncoder(
    200, 24, 24, 24, 24, [100, 24], 24, 48, 8, 2, 0.5)
encoder.eval()
encoder(torch.ones((2, 100), dtype=torch.long), valid_lens).shape
# torch.Size([2, 100, 24])
```

### è§£ç å™¨

#### DecoderBlock

åŒæ ·çš„ï¼Œå…ˆå®ç°å•ä¸ªè§£ç å™¨æ¨¡å—ã€‚å…¶å®åŸºæœ¬çš„æ¨¡å—ä¹‹å‰å·²ç»å…¨éƒ¨å®ç°äº†ï¼Œç»†èŠ‚ä¸Šçš„ä¸åŒæ˜¯ï¼š

1. ä¸ encoder ç›¸æ¯”ï¼Œdecoder å…ˆä½¿ç”¨è‡ªæ³¨æ„åŠ›æ±‡èšå¯¹ targets è¾“å…¥è¿›è¡Œç¼–ç ã€‚ç„¶åå°†è¯¥ç¼–ç ä½œä¸º queryã€å°† encoder çš„è¾“å‡ºä½œä¸º key & valueï¼Œè¾“å…¥åˆ°å¤šå¤´æ³¨æ„åŠ›æ±‡èšä¸­
2. åŒ seq2seq ä¸€æ ·ï¼Œåœ¨è¿›è¡Œè§£ç æ—¶ä¸åº”è¯¥çœ‹åˆ°è¯¥æ—¶é—´æ­¥åŠå…¶ä¹‹åçš„ä¿¡æ¯ï¼Œæ‰€ä»¥éœ€è¦æ©ç  `dec_valid_lens`ï¼Œä»¥ä¿æŒå…¶è‡ªå›å½’å±æ€§

å…·ä½“ä»£ç å¦‚ä¸‹

```python
class DecoderBlock(nn.Module):
    """è§£ç å™¨ä¸­ç¬¬ i ä¸ªå—"""
    def __init__(self, key_size, query_size, value_size, num_hiddens,
                 norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,
                 dropout, i, **kwargs):
        super(DecoderBlock, self).__init__(**kwargs)
        self.i = i
        self.attention1 = d2l.MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout)
        self.addnorm1 = AddNorm(norm_shape, dropout)
        self.attention2 = d2l.MultiHeadAttention(
            key_size, query_size, value_size, num_hiddens, num_heads, dropout)
        self.addnorm2 = AddNorm(norm_shape, dropout)
        self.ffn = PositionWiseFFN(ffn_num_input, ffn_num_hiddens,
                                   num_hiddens)
        self.addnorm3 = AddNorm(norm_shape, dropout)

    def forward(self, X, state):
        enc_outputs, enc_valid_lens = state[0], state[1]
        # è®­ç»ƒé˜¶æ®µï¼Œè¾“å‡ºåºåˆ—çš„æ‰€æœ‰è¯å…ƒéƒ½åœ¨åŒä¸€æ—¶é—´å¤„ç†ï¼Œ
        # å› æ­¤ `state[2][self.i]` åˆå§‹åŒ–ä¸º `None`ã€‚
        # é¢„æµ‹é˜¶æ®µï¼Œè¾“å‡ºåºåˆ—æ˜¯é€šè¿‡è¯å…ƒä¸€ä¸ªæ¥ç€ä¸€ä¸ªè§£ç çš„ï¼Œ
        # å› æ­¤ `state[2][self.i]` åŒ…å«ç€ç›´åˆ°å½“å‰æ—¶é—´æ­¥ç¬¬ `i` ä¸ªå—è§£ç çš„è¾“å‡ºè¡¨ç¤º
        if state[2][self.i] is None:
            key_values = X
        else:
            key_values = torch.cat((state[2][self.i], X), axis=1)
        state[2][self.i] = key_values
        if self.training:
            batch_size, num_steps, _ = X.shape
            # `dec_valid_lens` çš„å¼€å¤´: (`batch_size`, `num_steps`),
            # å…¶ä¸­æ¯ä¸€è¡Œæ˜¯ [1, 2, ..., `num_steps`]
            dec_valid_lens = torch.arange(
                1, num_steps + 1, device=X.device).repeat(batch_size, 1)
        else:
            dec_valid_lens = None

        # è‡ªæ³¨æ„åŠ›
        X2 = self.attention1(X, key_values, key_values, dec_valid_lens)
        Y = self.addnorm1(X, X2)
        # ç¼–ç å™¨ï¼è§£ç å™¨æ³¨æ„åŠ›ã€‚
        # `enc_outputs` çš„å¼€å¤´: (`batch_size`, `num_steps`, `num_hiddens`)
        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens)
        Z = self.addnorm2(Y, Y2)
        return self.addnorm3(Z, self.ffn(Z)), state
```

ä¸ºäº†ä¾¿äºåœ¨â€œç¼–ç å™¨ï¼è§£ç å™¨â€æ³¨æ„åŠ›ä¸­è¿›è¡Œç¼©æ”¾ç‚¹ç§¯è®¡ç®—å’Œæ®‹å·®è¿æ¥ä¸­è¿›è¡ŒåŠ æ³•è®¡ç®—ï¼Œç¼–ç å™¨å’Œè§£ç å™¨çš„ç‰¹å¾ç»´åº¦éƒ½æ˜¯ `num_hiddens`ï¼Œæ‰€ä»¥è¯´ decoder ä¹Ÿæ˜¯ä¸æ”¹å˜æ•°æ®å½¢çŠ¶çš„

```python
decoder_blk = DecoderBlock(24, 24, 24, 24, [100, 24], 24, 48, 8, 0.5, 0)
decoder_blk.eval()
X = torch.ones((2, 100, 24))
state = [encoder_blk(X, valid_lens), valid_lens, [None]]
decoder_blk(X, state)[0].shape
# torch.Size([2, 100, 24])
```

#### TransformerDecoder

ä¸‹é¢å°†å¤šä¸ª decoder ç»„åˆèµ·æ¥ï¼Œå¹¶ä¿å­˜æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§†åŒ–

```python
class TransformerDecoder(d2l.AttentionDecoder):
    def __init__(self, vocab_size, key_size, query_size, value_size,
                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,
                 num_heads, num_layers, dropout, **kwargs):
        super(TransformerDecoder, self).__init__(**kwargs)
        self.num_hiddens = num_hiddens
        self.num_layers = num_layers
        self.embedding = nn.Embedding(vocab_size, num_hiddens)
        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)
        self.blks = nn.Sequential()
        for i in range(num_layers):
            self.blks.add_module("block"+str(i),
                DecoderBlock(key_size, query_size, value_size, num_hiddens,
                             norm_shape, ffn_num_input, ffn_num_hiddens,
                             num_heads, dropout, i))
        self.dense = nn.Linear(num_hiddens, vocab_size)

    def init_state(self, enc_outputs, enc_valid_lens, *args):
        return [enc_outputs, enc_valid_lens, [None] * self.num_layers]

    def forward(self, X, state):
        X = self.pos_encoding(self.embedding(X) * math.sqrt(self.num_hiddens))
        self._attention_weights = [[None] * len(self.blks) for _ in range (2)]
        for i, blk in enumerate(self.blks):
            X, state = blk(X, state)
            # è§£ç å™¨è‡ªæ³¨æ„åŠ›æƒé‡
            self._attention_weights[0][
                i] = blk.attention1.attention.attention_weights
            # â€œç¼–ç å™¨ï¼è§£ç å™¨â€è‡ªæ³¨æ„åŠ›æƒé‡
            self._attention_weights[1][
                i] = blk.attention2.attention.attention_weights
        return self.dense(X), state

    @property
    def attention_weights(self):
        return self._attention_weights
```

ä¸‹é¢æ¥çœ‹çœ‹ä¸‰ä¸ª Multi-attention çš„å¯è§†åŒ–ç»“æœï¼Œä¸»è¦æ˜¯ä½“ä¼š valid_len çš„æ•ˆæœ

1. Encoder self-attention weights

   <img src="D2L 10 æ³¨æ„åŠ›æœºåˆ¶/image-20211215153222782.png" style="zoom: 67%;" />

   å¯ä»¥çœ‹åˆ°åœ¨æŸä¸ª key positions è¿‡åæ˜¯æ²¡æœ‰æ³¨æ„åŠ›æƒé‡çš„ï¼Œæ˜¯å› ä¸ºä¹‹åçš„ key éƒ½æ˜¯ <pad> è¯å…ƒï¼Œä¸éœ€è¦è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—

2. Decoder self-attention weights

   <img src="D2L 10 æ³¨æ„åŠ›æœºåˆ¶/image-20211215153713172.png" alt="image-20211215153713172" style="zoom: 67%;" />

   ç”±äº decoder çš„æ¯ä¸ª valid_len æ˜¯éšç€æ—¶é—´æ­¥é€æ¸å¢åŠ çš„ï¼Œæ‰€ä»¥å¯ä»¥çœ‹åˆ° self-attention weights ä¼¼ä¹æ•´ä½“æ˜¯å‘ˆä¸‹ä¸‰è§’å½¢çŠ¶

3. Encoder-decoder attention weights

   <img src="D2L 10 æ³¨æ„åŠ›æœºåˆ¶/image-20211215154614074.png" alt="image-20211215154614074" style="zoom:67%;" />

   åˆå‡ºç°äº† encoder self-attention ä¸­çš„æƒ…å†µï¼Œè¶…è¿‡æŸä¸ª key position å°±æ²¡æœ‰æƒé‡äº†ï¼Œå› ä¸ºåªæœ‰è¿™ä¹ˆå¤šä¸ª key (source time step)

### æ„Ÿè¨€

å¯ç®—æ˜¯å®Œæˆäº†æ€»ç»“ğŸ˜­ğŸ˜­è™½ç„¶çœ‹å¾—è¿˜æ˜¯æ¯”è¾ƒç²—ç³™ï¼Œä½†æ˜¯æ€»å½’æ˜¯æœ‰äº›æ¦‚å¿µäº†ã€‚åœ¨æ²ç¥è¯»è®ºæ–‡çš„è§†é¢‘ [bilibili](https://www.bilibili.com/video/BV1pu411o7BE) ä¸­è®²åˆ°ï¼šè™½ç„¶ transformer è®ºæ–‡å«åš `Attention Is All You Need`ï¼Œä½†äº‹å®ä¸Šå„ä¸ªç»“æ„éƒ½æ˜¯å¾ˆé‡è¦çš„ï¼Œä¾‹å¦‚ï¼šæ®‹å·®è¿æ¥å’Œå±‚è§„è§„èŒƒåŒ–åœ¨è®­ç»ƒæ·±åº¦ç½‘ç»œæ—¶æ˜¯éå¸¸é‡è¦çš„ã€‚Transformer (attention) æ•´ä½“æ¥çœ‹ä»æ˜¯ä¸€ä¸ªå‘å±•åˆæœŸçš„æ¶æ„ï¼Œåœ¨æœªæ¥æˆ–è®¸æœ‰æ›´å¤šçš„æ¶æ„å‡ºç°ï¼Œä¸€èµ·æœŸå¾…å§

