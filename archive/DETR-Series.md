---
title: DETR Series
date: 2022-12-27
categories:
  - papers
mathjax: true
---

# DETR

[zhihu](https://zhuanlan.zhihu.com/p/348060767)	[bilibili](https://www.bilibili.com/video/BV1GB4y1X72R)

DETR çš„ç‰¹ç‚¹ï¼šç®€å•ï¼ç«¯åˆ°ç«¯ï¼no anchorï¼åœ¨æ€§èƒ½ï¼ˆè¡¨ç°/é€Ÿåº¦ï¼‰ä¸Šå’Œ Faster RCNN ç›¸ä¼¼ã€‚è™½ç„¶å’Œå½“æ—¶æœ€å¥½çš„æ–¹æ³•ç›¸å·®10ä¸ªç‚¹ï¼Œä½†æ˜¯è¿™ä¸ªæ¡†æ¶å¤ªå¥½äº†ï¼Œæ˜¯ä¸€ä¸ªæŒ–å‘æ€§è´¨çš„æ–‡ç« ï¼Œæ‰€ä»¥è¿™ä¹Ÿæ˜¯ä»¶å¥½äº‹å„¿

## Intro

ç°é˜¶æ®µç›®æ ‡æ£€æµ‹å™¨å¾ˆå¤§ç¨‹åº¦ä¸Šéƒ½å—é™äºåå¤„ç† **NMS** çš„æ–¹æ³•ï¼Œä¸ç®¡æ˜¯ anchor-based or anchor-freeï¼ŒRCNN or SSD or YOLOï¼Œéƒ½æœ‰è¿™ä¸ªé—®é¢˜ï¼Œè¿™ä¹Ÿè®©ç›®æ ‡æ£€æµ‹åœ¨ç›®å‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•é‡Œéƒ½æ˜¯æ¯”è¾ƒå¤æ‚çš„ï¼Œåœ¨ä¼˜åŒ–å’Œè°ƒå‚çš„éš¾åº¦ä¸Šéƒ½æ¯”è¾ƒå¤§

DETR çš„ç½‘ç»œæµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤º

<img src="DETR/image-20220919001758670.png" alt="image-20220919001758670" style="zoom:67%;" />

ç”¨è¯­è¨€æ€»ç»“å¦‚ä¸‹ï¼š

1. ä½¿ç”¨ CNN æŠ½å–å›¾åƒç‰¹å¾
2. ä½¿ç”¨ Transformer encoder è·å¾—å…¨å±€ç‰¹å¾
3. ä½¿ç”¨ Transformer decoder è·å¾—é¢„æµ‹æ¡†
4. å°†é¢„æµ‹æ¡†å’Œ ground truth boxes åšåŒ¹é…å¹¶è®¡ç®— loss

Transformer å¤§è‡´ç»“æ„å¦‚ä¸‹

<img src="C:\Data\Projects\notes\archive\DETR\image-20221215140623383.png" alt="image-20221215140623383" style="zoom:50%;" />

å›¾ä¸­ Decoder è¿™è¾¹çš„ç»“æ„æ˜¯ä¸å¤Ÿå®Œæ•´çš„ï¼Œæˆ–è€…è¯´ä¸å¤Ÿæ­£ç¡®çš„ã€‚å®é™…ä¸Š Decoder å…³äº query çš„è¾“å…¥æœ‰ä¸¤ä¸ªï¼š1

1. Query itself
2. Query positional encoding, **i.e. Object Query**

å›¾ä¸­æ²¡æœ‰æŠŠ Query æœ¬èº«ç»™ç”»å‡ºæ¥ï¼Œè®ºæ–‡é‡Œæ˜¯**åˆå§‹åŒ–ä¸º0**ï¼Œæ‰€ä»¥å›¾ä¸­ç›´æ¥çœç•¥ä¸ç”»äº†ï¼Œå¯¼è‡´ Decoder ä¸‹ä¾§çš„ `+` å·æ„ä¹‰ä¸æ¸…æ™°

### ä¸€äº›ç»“è®º

å—ç›Šäº transformer çš„å…¨å±€å»ºæ¨¡èƒ½åŠ›ï¼ŒDETR å¯¹äºå¤§ç‰©ä½“çš„æ£€æµ‹èƒ½åŠ›éå¸¸å¼ºï¼Œä½†æ˜¯å¯¹å°ç‰©ä½“çš„æ¯”è¾ƒå·®ï¼Œå¹¶ä¸” DETR æ”¶æ•›çš„é€Ÿåº¦éå¸¸æ…¢ã€‚æ”¹è¿›æ–¹æ³•åœ¨ Deformable DETR ä¸­æå‡ºï¼Œä¾ç„¶æ˜¯ä½¿ç”¨å¤šå°ºåº¦çš„ç‰¹å¾å›¾è°± + Deformable attention

å‰äººä¹Ÿæœ‰ä½¿ç”¨äºŒåˆ†å›¾åŒ¹é…çš„æ–¹æ³•ï¼Œæˆ–è€…ä½¿ç”¨ RNN åš encoder-decoder æ¥è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œä½†æ˜¯éƒ½æ²¡æœ‰ç”¨ transformer æ‰€ä»¥æ€§èƒ½ä¸Šä¸å»ã€‚æ‰€ä»¥è¯´ DETR çš„æˆåŠŸï¼Œä¹Ÿæ˜¯ transformer çš„æˆåŠŸ

##  Model

### Bipartite Matching Loss

è®ºæ–‡è®¤ä¸ºç»“æ„éƒ½æ˜¯æ¯”è¾ƒç®€å•å¥½ç†è§£çš„ï¼Œæ‰€ä»¥å…ˆè®²äº†æŸå¤±å‡½æ•°è¿™ä¸€å—ï¼šå¦‚ä½•ä½¿ç”¨äºŒåˆ†å›¾åŒ¹é…æ¥è®¡ç®—æŸå¤±

DETR é¢„æµ‹è¾“å‡ºæ˜¯ä¸€ä¸ªå›ºå®šå€¼ï¼Œå³é¢„æµ‹å›ºå®šçš„ N(=100) ä¸ªé¢„æµ‹

å…³äºäºŒåˆ†å›¾åŒ¹é…ç®—æ³•ï¼ˆåŒˆç‰™åˆ©ç®—æ³•ï¼‰ï¼Œæˆ‘åœ¨ä¹‹å‰çš„åšå®¢ **å›¾è®ºç®—æ³•** é‡Œæœ‰ä¸€äº›æ€»ç»“å¯ä»¥å‚è€ƒï¼Œåœ¨ DETR çš„åœºæ™¯ä¸‹è¯¥åŒ¹é…ç®—æ³•çš„ä½œç”¨ä¸ºï¼šå°† N ä¸ª prediction ä¸ N ä¸ª gt è¿›è¡Œé…å¯¹ï¼ˆæ²¡æœ‰ N ä¸ª gt åˆ™éœ€è¦ paddingï¼‰ã€‚é¢„æµ‹æœ‰äº† gt è¿‡åå°±å¯ä»¥è®¡ç®—æŸå¤±å‡½æ•°äº†

é…å¯¹ä½¿ç”¨çš„ cost matrix è®¡ç®—å…¬å¼å¦‚ä¸‹
$$
\hat{\sigma}=\underset{\sigma \in \mathfrak{S}_{N}}{\arg \min } \sum_{i}^{N} \mathcal{L}_{\operatorname{match}}\left(y_{i}, \hat{y}_{\sigma(i)}\right) \\
\mathcal{L}_{\operatorname{match}}\left(y_{i}, \hat{y}_{\sigma(i)}\right)=
-\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \hat{p}_{\sigma(i)}\left(c_{i}\right)+\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \mathcal{L}_{\mathrm{box}}\left(b_{i}, \hat{b}_{\sigma(i)}\right)\\

\mathcal{L}_{\mathrm{box}}=\lambda_{\text {iou }} \mathcal{L}_{\text {iou }}\left(b_{i}, \hat{b}_{\sigma(i)}\right)+\lambda_{\mathrm{L} 1}\left\|b_{i}-\hat{b}_{\sigma(i)}\right\|_{1}
$$
å…¶ä¸­ $\sigma$ å¯ä»¥çœ‹ä½œä¸€ä¸ªæ’åˆ—æˆ–è€…æ˜ å°„ï¼Œ$\sigma(i)$ ä»£è¡¨ç¬¬ i ä¸ª gt æ‰€åŒ¹é…çš„é¢„æµ‹çš„ indexï¼Œbox æŸå¤±ä½¿ç”¨çš„æ˜¯ GIoU æŸå¤±å’Œ L1 æŸå¤±çš„åŠ æƒã€‚æ³¨æ„åˆ°ç©º gt å’Œä»»ä½• prediction çš„ cost éƒ½æ˜¯ 0ï¼Œæ‰€ä»¥æœ¬è´¨ä¸Šå°±æ˜¯ N ä¸ª prediction å’Œ M ä¸ª gt ä¹‹é—´çš„åŒ¹é…ï¼Œç”¨äºç¡®å®š M ä¸ªæ­£æ ·æœ¬ prediction å’Œ N - M ä¸ªè´Ÿæ ·æœ¬ prediction

### Detection Loss

åŒ¹é…å®Œæˆåï¼Œå°±å¯ä»¥è®¡ç®—æŸå¤±å‡½æ•°
$$
\mathcal{L}_{\text {Hungarian }}(y, \hat{y})=\sum_{i=1}^{N}\left[-\log \hat{p}_{\hat{\sigma}(i)}\left(c_{i}\right)+\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \mathcal{L}_{\text {box }}\left(b_{i}, \hat{b}_{\hat{\sigma}}(i)\right)\right]
$$
è®ºæ–‡æåˆ°åœ¨è®¡ç®—åˆ†ç±»æŸå¤±æ—¶ï¼Œå¯¹äºç©ºç±» gt $\varnothing$ çš„åˆ†ç±»æŸå¤±è¦é™¤ä»¥ 10 ç”¨äºå¹³è¡¡æ­£è´Ÿæ ·æœ¬ï¼Œåœ¨å®ç°ä¸Šæ˜¯ç›´æ¥æŒ‡å®š `F.cross_entropy(..., weight=)` å®Œæˆ

å¦å¤–è¿˜æ˜¯ä½¿ç”¨äº†ä¸­é—´ç›‘ç£ï¼Œæˆ–è€…ç§°ä¸ºè¾…åŠ©æŸå¤±ã€‚å³æŠŠ decoder çš„ä¸­é—´ block çš„è¾“å‡ºä¹Ÿä½œä¸ºé¢„æµ‹ç»“æœï¼Œå¹¶è®¡ç®—æ£€æµ‹æŸå¤±

DC5ï¼Œæ˜¯ dilated convolution at resnet stage 5 çš„ä¸€ä¸ªç®€ç§°ï¼Œåœ¨ DETR ä¸­ä½¿ç”¨çš„å…·ä½“æè¿°ä¸º

> Following [21, FCN], we also increase the feature resolution by adding a dilation to the last stage of the backbone and removing a stride from the first convolution of this stage.

ä½†æ˜¯åœ¨åé¢çš„ DETR-based ç›®æ ‡æ£€æµ‹å™¨ä¸­æ²¡æœ‰å¸¸ç”¨

### DETR é—®é¢˜

å¤§ç‰©ä½“å’Œå°ç‰©ä½“æ•ˆæœç›¸å·®å¤§

object query çš„é¢„æµ‹ç»“æœå¯è§†åŒ–

## Deformable DETR

### Multi-Scaele Deformable Attention

ä¸€å¼€å§‹çœ‹ Deformable DETR çš„æ—¶å€™ï¼Œæ³¨æ„åŠ›å¾ˆå®¹æ˜“é›†ä¸­åˆ° Deformable ä¸Šï¼Œä½†æˆ‘è§‰å¾— Multi-Scale åŒæ ·éå¸¸é‡è¦ã€‚ä¸ºäº†å½»åº•å¼„æ¸… Multi-Scale Deformable Attentionï¼Œæˆ‘æƒ³ä»å¦‚ä¸‹å‡ ä¸ªé—®é¢˜å…¥æ‰‹

1. å¦‚ä½•è¡¨ç¤º multi-scale feature
2. å¦‚ä½•è¡¨ç¤º multi-scale feature's positional embedding 
3. å¦‚ä½•è¡¨ç¤º reference points
4. å¦‚ä½•å®Œæˆ multi-scale deformable attention

#### Multi-scale Feature

è¿™éå¸¸å¥½ç†è§£ï¼Œå°±æ˜¯ä» backbone **ResNet 50** ä¸­è¾“å‡ºçš„ä¸­é—´ç‰¹å¾å±‚ï¼Œè¶Šæ·±çš„ç‰¹å¾å±‚ï¼Œåˆ†è¾¨ç‡è¶Šå°

```python
# output feature dict
features = self.backbone(images.tensor)

# project backbone features to the reuired dimension of transformer
multi_level_feats = self.neck(features)
```

æœ€ç»ˆ neck æŠŠè¿™äº›ç‰¹å¾å›¾è°±æ˜ å°„åˆ°åŒä¸€ä¸ªç»´åº¦ï¼Œä»¥è¾“å…¥åˆ° transformer ä¸­åšç‚¹ç§¯

<img src="C:\Data\Projects\notes\archive\DETR\image-20221216190202156.png" alt="image-20221216190202156" style="zoom:50%;" />

#### Multi-scale Positional Embedding

**Encoder é˜¶æ®µ**

ç›´æ¥å¤„ç† multi-scale positional embeddingsï¼Œå°±æ˜¯é€ä¸ª level è·å¾—

```python
for feat in multi_level_feats:
    multi_level_masks.append(
        F.interpolate(img_masks[None], size=feat.shape[-2:]).to(torch.bool).squeeze(0)
    )
    multi_level_position_embeddings.append(self.position_embedding(multi_level_masks[-1]))
```

è§£é‡Šï¼špositional embedding æ˜¯æ ¹æ®ä¸€ä¸ª 2D mask ç›´æ¥ç”Ÿæˆçš„ï¼Œå¯¹äºä¸åŒ scale çš„ mask æ˜¯ç›´æ¥æ ¹æ® img mask æ’å€¼è·å¾—ï¼ˆimg mask ä¸­çš„éé›¶å€¼å³è¡¨ç¤ºè¯¥åƒç´ ç‚¹è¢«å¿½ç•¥ï¼‰ 

ä¸ºäº†å¯¹ä¸åŒ scale çš„ positional embedding è¿›è¡ŒåŒºåˆ†ï¼Œå†åŠ å…¥ scale embedding

```python
self.level_embeds = nn.Parameter(torch.Tensor(self.num_feature_levels, self.embed_dim))
# for each scale, pos_embed (B, H*W, C)
for lvl, pos_embed in enumerate(multi_level_pos_embeds):
	lvl_pos_embed = pos_embed + self.level_embeds[lvl].view(1, 1, -1)
```

**Decoder é˜¶æ®µ**

ä¸Šè¿°çš„ multi-scale positional embedding æ˜¯ç»™ encoder query ä½¿ç”¨ï¼Œåœ¨ decoder ä¸­ query ä¸å†æ˜¯å¤æ‚çš„å¤šå°ºåº¦ç‰¹å¾å›¾è°±ï¼Œè€Œå°±æ˜¯ä¸€èˆ¬çš„ embeddingï¼Œæ‰€ä»¥ä½¿ç”¨çš„ query positional embedding å°±æ˜¯ DETR ä¸­çš„ object queryï¼Œä¹Ÿæ˜¯ä¸€èˆ¬çš„ embedding

æ‰€è°“ä¸€èˆ¬çš„ embeddingï¼ŒæŒ‡çš„æ˜¯éé¢„è®¾ï¼Œå¦‚ sine embedding å³ä¸ºé¢„è®¾å¥½çš„

#### Reference Points

**Encoder é˜¶æ®µ**

reference points å°±æ˜¯æ¯ä¸ªåƒç´ ç‚¹çš„**å½’ä¸€åŒ–åæ ‡**ã€‚æ¯ä¸€ä¸ª scale çš„ reference points ä¸ºä¸€ä¸ªå¼ é‡ï¼Œå½¢çŠ¶ä¸º (H, W, 2)ï¼Œé‚£ä¹ˆå¤šä¸ª scale çš„ reference points åˆèµ·æ¥åº”è¯¥æ˜¯ `(B, h1w1 + h2w2 + ..., 2)` æ‰å¯¹ï¼Œä½†å®é™…ä¸Šçš„ä»£ç å¹¶ä¸æ˜¯è¿™ä¹ˆåšçš„

```python
    def get_reference_points(spatial_shapes, valid_ratios, device):
        """
        Args:
            spatial_shapes (Tensor): The shape of all feature maps, has shape (num_level, 2).
            valid_ratios (Tensor): The ratios of valid points on the feature map, has shape (bs, num_levels, 2)
        Returns:
            Tensor: reference points used in decoder, has shape (bs, num_keys, num_levels, 2).
        """
```

æœ€ç»ˆçš„è¾“å‡ºå½¢çŠ¶æ˜¯ `(bs, num_keys, num_levels, 2)`ï¼Œå¤šäº†ä¸€ä¸ª `num_levels` ç»´åº¦ã€‚**å®é™…ä¸Šè¿™æ˜¯ä¸ºäº†å„ä¸ª scale ä¹‹é—´çš„äº¤äº’**ï¼Œå³æŸä¸ªåƒç´ çš„ reference point å­˜åœ¨äºå„ä¸ªå°ºåº¦å½“ä¸­ï¼Œè¿™æ ·å¯ä»¥åœ¨å„ä¸ª scale ä¸­è¿›è¡Œé‡‡æ ·ï¼Œæœ€åç”¨æ³¨æ„åŠ›åŠ æƒè·å¾—å¤šå°ºåº¦ç‰¹å¾

**Decoder é˜¶æ®µ**

ç”±äº Decoder ä¸­çš„ query æ˜¯ä¸€èˆ¬çš„ embeddingï¼Œæ‰€ä»¥ä¸å¯èƒ½åƒä¸Šè¿°ä¸€æ ·ç”Ÿæˆä¸ä½ç½®å¼ºç›¸å…³çš„ reference pointsï¼Œå°±åªèƒ½ç”¨ä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚å¯¹ embedding è¿›è¡Œè½¬æ¢

```python
self.reference_points = nn.Linear(self.embed_dim, 2)
```

ç„¶åå† sigmoid è¿›è¡Œå½’ä¸€åŒ–ã€‚ä½†ä¹‹åä½¿ç”¨äº†ä¸¤ä¸ªæŠ€å·§ï¼Œè¿™ä¸ªé—®é¢˜ä¹Ÿè¢«å·§å¦™åŒ–è§£äº†ï¼šquery selection & iterative box refinementï¼Œå¯ä»¥ç›´æ¥å°† proposal boxes ä½œä¸º query

#### MSDeformAttn

ä¸‹é¢æ­£å¼ä»‹ç» multi-scale deformable attention æ¨¡å—ï¼Œå…ˆç®€å•æè¿°ä¸‹ä»£ç å¹²äº†ä»€ä¹ˆäº‹æƒ…

1. åˆ¤æ–­æ˜¯å¦ä¸ºè‡ªæ³¨æ„åŠ›ï¼Œå¦‚æœæ²¡æœ‰ä¼ å…¥ value åˆ™ä½¿ç”¨ query æœ¬èº«

2. ç»™ query åŠ å…¥ positional embeddingï¼Œ**æ³¨æ„ï¼Œè¿™é‡Œæ²¡æœ‰ç»™ key åŠ å…¥ positional embeddingï¼Œæ›´ç¡®åˆ‡åœ°è¯´ï¼Œåœ¨ deformable attetion é‡Œæ²¡æœ‰ key çš„æ¦‚å¿µï¼Œkey åŠå…¶ attention æ˜¯é€šè¿‡å…¶ä»–æ–¹å¼è·å¾—**

3. value ç»è¿‡ä¸€ä¸ªçº¿æ€§å±‚ï¼Œç»´åº¦ä¸å˜ï¼Œå¹¶ä¸” mask æ‰ä¸éœ€è¦è¿›è¡Œæ³¨æ„åŠ›çš„ç‚¹ã€‚ç„¶åå† view ä¸ºå¤šå¤´çš„å½¢å¼

4. å°† query é€åˆ° `self.sampling_points` çº¿æ€§å±‚ï¼Œè¿›è¡Œåç§»é‡é¢„æµ‹ï¼Œä¹Ÿå¯¹åç§»é‡è¿›è¡Œå½’ä¸€åŒ–

   ```python
   self.sampling_offsets = nn.Linear(embed_dim, num_heads * num_levels * num_points * 2)
   ```

5. å°† query é€åˆ° `self.attention_weight` çº¿æ€§å±‚ï¼Œå¹¶ç”¨ softmax è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°

   ```python
   self.attention_weights = nn.Linear(embed_dim, num_heads * num_levels * num_points)
   ```

6. æœ‰äº†å‰é¢çš„å‡†å¤‡å·¥ä½œï¼Œå°±èƒ½å¤Ÿæ„‰å¿«è®¡ç®—å¯å˜æ³¨æ„åŠ›äº†ï¼Œpure pytorch å®ç°ä¸º `multi_scale_deformable_attn_pytorch`ï¼Œæ³¨æ„è¿™é‡Œçš„è¾“å‡ºå·²ç»åˆå¹¶äº†å¤šå¤´

7. è·å¾—çš„ç»“æœå†è¿‡ä¸€ä¸ªçº¿æ€§å±‚ï¼Œç»´åº¦ä¸å˜

`multi_scale_deformable_attn_pytorch` ç”¨ç®€æ´çš„è¯­è¨€æ¦‚æ‹¬ä¸ºï¼š

1. å¯¹æ¯ä¸€ä¸ª scale/levelï¼Œè®¡ç®—æ‰€æœ‰ sampling points åœ¨è¯¥ level æ’å€¼å¾—åˆ°çš„ç‰¹å¾å‘é‡
2. å¯¹æ’å€¼å¾—åˆ°çš„ multi scale + multi points çš„ç‰¹å¾å‘é‡è¿›è¡Œæ³¨æ„åŠ›åŠ æƒæ•´åˆ
3. åˆå¹¶å¤šä¸ª head çš„ç‰¹å¾å‘é‡

```python
def multi_scale_deformable_attn_pytorch(value, value_spatial_shapes,
                                        sampling_locations, attention_weights):
    """CPU version of multi-scale deformable attention.

    Args:
        value (Tensor): The value has shape
            (bs, num_keys, mum_heads, embed_dims//num_heads)
        value_spatial_shapes (Tensor): Spatial shape of
            each feature map, has shape (num_levels, 2),
            last dimension 2 represent (h, w)
        sampling_locations (Tensor): The location of sampling points,
            has shape
            (bs ,num_queries, num_heads, num_levels, num_points, 2),
            the last dimension 2 represent (x, y).
        attention_weights (Tensor): The weight of sampling points used
            when calculate the attention, has shape
            (bs ,num_queries, num_heads, num_levels, num_points),

    Returns:
        Tensor: has shape (bs, num_queries, embed_dims)
    """

    bs, _, num_heads, embed_dims = value.shape
    _, num_queries, num_heads, num_levels, num_points, _ =\
        sampling_locations.shape
    value_list = value.split([H_ * W_ for H_, W_ in value_spatial_shapes],
                             dim=1)
    sampling_grids = 2 * sampling_locations - 1
    sampling_value_list = []
    for level, (H_, W_) in enumerate(value_spatial_shapes):
        # bs, H_*W_, num_heads, embed_dims ->
        # bs, H_*W_, num_heads*embed_dims ->
        # bs, num_heads*embed_dims, H_*W_ ->
        # bs*num_heads, embed_dims, H_, W_
        value_l_ = value_list[level].flatten(2).transpose(1, 2).reshape(
            bs * num_heads, embed_dims, H_, W_)
        # bs, num_queries, num_heads, num_points, 2 ->
        # bs, num_heads, num_queries, num_points, 2 ->
        # bs*num_heads, num_queries, num_points, 2
        sampling_grid_l_ = sampling_grids[:, :, :,
                                          level].transpose(1, 2).flatten(0, 1)
        # bs*num_heads, embed_dims, num_queries, num_points
        sampling_value_l_ = F.grid_sample(
            value_l_,
            sampling_grid_l_,
            mode='bilinear',
            padding_mode='zeros',
            align_corners=False)
        sampling_value_list.append(sampling_value_l_)
    # (bs, num_queries, num_heads, num_levels, num_points) ->
    # (bs, num_heads, num_queries, num_levels, num_points) ->
    # (bs, num_heads, 1, num_queries, num_levels*num_points)
    attention_weights = attention_weights.transpose(1, 2).reshape(
        bs * num_heads, 1, num_queries, num_levels * num_points)
    output = (torch.stack(sampling_value_list, dim=-2).flatten(-2) *
              attention_weights).sum(-1).view(bs, num_heads * embed_dims,
                                              num_queries)
    return output.transpose(1, 2).contiguous()

```

### Decoder

çœ‹å®Œäº† Deformable Attentionï¼Œå®é™…ä¸Šåœ¨ Decoder ä¸Šçš„ä¸œè¥¿ä¹Ÿå¾ˆä¸ä¸€æ ·ã€‚ä»æ­¤å¼€å§‹ Decoder ä¸å†ä¿æŒåŸå§‹ DETR çš„ç®€æ´æ€§ï¼Œä¸ºäº†è®©æ”¶æ•›æ›´å¿«ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨æ›´å¼ºçš„å…ˆéªŒå‡è®¾

#### Query Selection

æ„Ÿè§‰æœ‰ beam search çš„æ„æ€ï¼šä¸éœ€è¦æ¯æ¬¡å¯¹å…¨éƒ¨çš„é€‰æ‹©è¿›è¡Œæš´åŠ›æœç´¢ï¼Œè€ŒåŸºäºæ’åé å‰çš„é€‰æ‹©ç»§ç»­è¿›è¡Œæœç´¢ã€‚è¿™ä¸ªæ–¹æ³•ä¹Ÿè¢«ç§°ä¸º DETR two stageï¼Œä¸¤é˜¶æ®µæ–¹æ³•ã€‚**Query ç°åœ¨ä¸æ˜¯æ¥è‡ªäºéšæœºåˆå§‹åŒ–çš„ embeddingï¼Œè€Œæ˜¯æ¥è‡ªäº Encoder Output + Preset Anchor**

Query Selection æ˜¾ç„¶éœ€è¦å®Œæˆä¸¤ä»¶äº‹ï¼š

1. ç”Ÿæˆ query / proposalã€‚æ–¹æ³•æ˜¯åŸºäºé¢„è®¾ anchor çš„ proposal é¢„æµ‹ã€‚æ¯ä¸€ä¸ªé¢„è®¾ anchor å¯¹åº”ä¸€ä¸ª encoder output pixelï¼Œç„¶åç”Ÿæˆä¸€ä¸ªé€‰æ¡†æå…¶å¯¹åº”åˆ†ç±»ã€‚æ‰€ä»¥è¯´è¿™é‡Œçš„ **query å°±æ˜¯ proposal**
2. æ’åº query / proposalã€‚å¾—åˆ†é«˜ proposal çš„æ’åºé å‰ï¼Œå¹¶æ³¨æ„åˆ†ç±»ä»»åŠ¡ä¸ºäºŒåˆ†ç±»ï¼Œå³åªåˆ†å‰æ™¯å’ŒèƒŒæ™¯ï¼Œä½†è¿™ä¸ªäºŒåˆ†ç±»ä»»åŠ¡å®Œæˆå¾—å¾ˆå¦™ï¼Œå’ŒåŸæ¥çš„ 80 ç±»ç›´æ¥è¿›è¡Œäº†ä¸€ä¸ªç»Ÿä¸€ï¼šç›´æ¥æŠŠæ‰€æœ‰çš„ label æ ‡ç­¾æŒ‡å®šä¸º0ï¼Œå³å¯å®ŒæˆäºŒåˆ†ç±»

å®Œæˆä¸Šè¿°ä»»åŠ¡éœ€è¦è°ƒç”¨ä¸‹æ–¹å‡½æ•° `gen_encoder_output_proposals`ï¼Œè¿™é‡Œä»…ç»™å‡ºç®€å•æ³¨é‡Š

```python
    def gen_encoder_output_proposals(self, memory, memory_padding_mask, spatial_shapes):
        """
        Args:
            - memory: output of encoder, (B, S, C)
            - memory_padding_mask: (B, S)
            - spatial_shapes: (B, 2)
        Return:
            - output_memory: **MASKED** and projected new memory (B, S, C)
            - output_proposals: reference point **UNsigmoid** anchors (B, S, 4)
        """
```

æ­¤æ—¶è¿˜å¾—åˆ°ä¸€ä¸ªå¥½å¤„ï¼Œæ—¢ç„¶æˆ‘ä»¬çš„ query å·²ç»å’Œä½ç½®ç›¸å…³äº†ï¼Œé‚£ä¹ˆå…¶ reference points ä¹Ÿèƒ½å¤Ÿç›´æ¥ä½¿ç”¨è¯¥ proposal çš„ä½ç½®å’Œå¤§å°

å¦å¤–ä¸€ç‚¹ï¼Œå¯¹äºåˆå§‹åŒ–çš„ anchorï¼Œç½‘ç»œå¯¹å…¶å€¼å¹¶ä¸æ•æ„Ÿ

#### Iterative Box Refinement

è¿™é‡Œæ„Ÿè§‰æ˜¯æ®‹å·®ï¼Œæˆ–è€… step by step çš„æ€æƒ³ï¼Œæ•´ä¸ª trick éå¸¸å¯¹æˆ‘çš„å£å‘³ğŸ‘æˆ‘ä¸äº†è§£ Diffusion Modelï¼Œä¸çŸ¥é“è¿™ç§ step by step æ˜¯ä¸æ˜¯ç±»ä¼¼çš„

åœ¨ DETR ä¸­ decoder æ¯ä¸€å±‚éƒ½ä¼šå»é¢„æµ‹æœ€ç»ˆçš„é€‰æ¡†ï¼Œæ¯ä¸€å±‚çš„é¢„æµ‹å¯èƒ½å·®è·éå¸¸å¤§ï¼Œè¿™æ ·çš„è®­ç»ƒè¿‡ç¨‹æ˜¾ç„¶ä¸å¤Ÿç¨³å®šã€‚é—®é¢˜æå‡ºæ¥äº†ï¼Œæ”¹è¿›æ–¹æ³•ä¹Ÿæ˜¯è·ƒç„¶çº¸ä¸Šï¼šè¦æ±‚æ¯ä¸€å±‚çš„é¢„æµ‹æ˜¯åŸºäºä¸Šä¸€å±‚çš„é¢„æµ‹ï¼Œè¿™æ ·å°±åªé¢„æµ‹ä¸€ä¸ªå˜åŒ–é‡

åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æ ¹æ®è¿™ä¸ªé¢„æµ‹å»æ›´æ–°æˆ‘ä»¬çš„ reference pointsï¼Œå› ä¸ºé¢„æµ‹çš„æ¡†ä¼šæœç€ gt å»æ¢ç´¢ï¼Œreference points ä¸å…¶ä¸€èµ·æ›´æ–°ä¼šæ”¶æ•›æ›´å¿«ï¼Œè€Œä¸æ˜¯ä»å¤´åˆ°å°¾ä½¿ç”¨åŒä¸€å¥— reference pointsï¼Œå¹¶ä¸”æ­¤æ—¶ referece points ä¸º reference boxï¼Œæœ€åä¸€ä¸ªç»´åº¦æ˜¯ 4

æ³¨æ„ï¼Œæ¯ä¸ª scale/level çš„ `bbox_embed or class_embed` éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œä¸‹é¢æ˜¯æ¯ä¸€ä¸ª decoder block çš„æ¨ç†è¿‡ç¨‹

```python
for layer_idx, layer in enumerate(self.layers):
    output = layer(q, k, v, ...)
    
    tmp = self.bbox_embed[layer_idx](output)
    new_reference_points = tmp + inverse_sigmoid(reference_points)
    new_reference_points = new_reference_points.sigmoid()
    
	reference_points = new_reference_points.detach()	# no detach is better!
    
    intermediate.append(output)
    intermediate_reference_points.append(reference_points)
```

## DeNoising Training

æ‰€è°“çš„ DeNoising å°±æ˜¯è®©ç½‘è·¯å»å®Œæˆä¸€ä¸ªè¾…åŠ©ä»»åŠ¡ï¼šç»™çœŸå®æ ‡ç­¾åŠ å…¥å™ªå£°ï¼Œè¾“å…¥ç»™ç½‘ç»œï¼Œè®©ç½‘ç»œå»è¿˜åŸçœŸå®æ ‡ç­¾ã€‚è¿™æ ·èƒ½å¤ŸåŠ é€Ÿç½‘ç»œçš„æ”¶æ•›

è¿™ä¹Ÿå¾—ç›Šäº DETR çš„æ£€æµ‹å½¢å¼ï¼Œåªè¦è¾“å…¥ä¸€ä¸ª queryï¼Œå°±èƒ½å¤Ÿè¾“å‡ºä¸€ä¸ªé¢„æµ‹ç»“æœã€‚åœ¨ deformable detr ä¸­ anchor åˆç›´æ¥æˆä¸ºäº† queryï¼Œäº‹æƒ…å°±å˜å¾—æ›´ç®€å•äº†ï¼Œç›´æ¥æŠŠå™ªå£°æ ‡ç­¾å’Œ query ä¸€èµ· concat è¿æ¥

è®ºæ–‡æåˆ°ä½†å®é™…ä¸Šè¿™ç§ denoising æ€æƒ³æ˜¯å¯ä»¥éå¸¸å¹¿æ³›åœ°è¿ç”¨çš„

noise ä½¿ç”¨çš„æ˜¯é«˜æ–¯å™ªå£°ï¼Œæœ‰ä¸€ä¸ªè¶…å‚æ•°æ¥æ§åˆ¶èŒƒå›´

å­¦ä¹ ç‚¹ï¼š

1. attention mask çš„ä½¿ç”¨ï¼Œç¦æ­¢ match query çœ‹åˆ° gt
2. çµæ´»ä½¿ç”¨ç´¢å¼•
