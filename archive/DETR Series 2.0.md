# DETR Series 2.0

è¿™ä¸€æ¬¡å¿…å®šæ‹¿ä¸‹ DETR ç³»åˆ—ä»¥åŠ Deformable DETR ä»¥åŠ Vision Transformer çš„æ„å»ºè¿‡ç¨‹ğŸ¤”

ä¸€å¼€å§‹æ•´ç†éƒ½æ˜¯ä»¥ä¸€ç§çº¿æ€§çš„æ€ç»´åœ¨åšï¼Œçœ‹ä¸€ç‚¹ï¼Œæ•´ç†ä¸€ç‚¹ã€‚è¿™æ ·å¯èƒ½æ•´ç†å®Œè¿‡åæ²¡æœ‰ä¸€ä¸ªé€»è¾‘çš„æ€»ç»“ï¼Œå›çœ‹æ—¶ä¹Ÿæ¯”è¾ƒç©ºæ´ï¼Œä½†è¿™ä¹Ÿæ˜¯æ²¡åŠæ³•çš„äº‹æƒ…ï¼Œæ¯•ç«Ÿæ˜¯ç¬¬ä¸€æ¬¡æ•´ç†

äºæ˜¯åœ¨ç¬¬äºŒéè¿›è¡Œæ€»ç»“æ—¶å°±éœ€è¦æ³¨æ„æ€»ç»“çš„é€»è¾‘é€šé¡ºé—®é¢˜

è¿™ç¯‡ç¬”è®°æ‰€æœ‰çš„ä»£ç éƒ½å‚è€ƒè‡ª detrex ä»¥ä¿è¯ç»Ÿä¸€æ€§

## Blocks

### MultiHead Attention

Attention çš„åŠŸèƒ½å°±æ˜¯è®© query å»è¯¢é—® (key, value) ç„¶ååŠ æƒè¾“å‡º
$$
\operatorname{softmax}\left(\frac{\mathbf{Q K}^{\top}}{\sqrt{d}}\right) \mathbf{V} \in \mathbb{R}^{n \times v}
$$
åœ¨ detrex å½“ä¸­ç›´æ¥è°ƒç”¨äº† `nn.MultiheadAttentnion` å¹¶åŠ å…¥ identity connection & posotional embeddingï¼Œå®Œæˆäº†æ•´ä½“åŒ…è£… `MultiheadAttention`

```python
    def forward(
        self,
        query: torch.Tensor,
        key = None,
        value = None,
        identity = None,
        query_pos = None,
        key_pos = None,
        attn_mask = None,
        key_padding_mask = None,
    ) -> torch.Tensor:

        if key is None:
            key = query
        if value is None:
            value = key
        if identity is None:
            identity = query

        if query_pos is not None:
            query = query + query_pos
        if key_pos is not None:
            key = key + key_pos

        out = self.attn(
            query=query,
            key=key,
            value=value,
            attn_mask=attn_mask,
            key_padding_mask=key_padding_mask,
        )[0]
		# self.proj_drop = nn.Dropout(r)
        return identity + self.proj_drop(out)
```

åœ¨ä½¿ç”¨ `nn.MultiheadAttentnion` æ—¶æœ‰ä¸¤ä¸ªæŠ€å·§å’Œä¸¤ä¸ªæ³¨æ„ç‚¹ï¼š

1. `attn_mask`ï¼šå…¶å½¢çŠ¶ä¸º `(num_query, num_key)`ï¼Œæ˜¯ bool çŸ©é˜µï¼Œç”¨äºå»é™¤ä¸å¸Œæœ›è®¡ç®—çš„éƒ¨åˆ†å…ƒç´ 
2. `key_padding_mask`ï¼šå…¶å½¢çŠ¶ä¸º `(bs, num_query)`ï¼Œç”¨äºå¿½ç•¥ä¸éœ€è¦è®¡ç®—çš„ keyï¼Œæœ¬è´¨ä¸Šå¯ç”± `attn_mask` æ›¿ä»£ï¼Œä½†æ˜¯è¯¥æ¥å£æ›´æ–¹ä¾¿
3. å¯ä»¥è¿›è¡Œä¸¤æ¬¡ dropoutï¼Œä¸€æ¬¡åœ¨ attentnionï¼Œä¸€æ¬¡åœ¨ linear projection
4. Positional embedding ä»…å¯¹ query å’Œ key å­˜åœ¨ï¼Œå¯¹ value ä¸å­˜åœ¨

è¿™é‡Œæ”¾ä¸€ä¸ª `MultiHead-SelfAttention` çš„æ— æ©ç å®ç°ç‰ˆæœ¬ï¼Œæ˜¯é¢è¯•å¸¸è€ƒï¼Œå‚è€ƒ timm çš„å®ç°

```python
class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):
        super().__init__()
        assert dim % num_heads == 0, 'dim should be divisible by num_heads'
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x):
        B, N, C = x.shape
        # qkv: (3, B, num_heads, N, head_dim)
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        # make torchscript happy (cannot use tensor as tuple)
        q, k, v = qkv.unbind(0)

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x
```

Self-Attention çš„ qkv éƒ½æ˜¯ä¸€æ ·çš„ï¼Œå¦‚æœè¦åš cross-attentnion åªè¦æŠŠé‚£ä¸ªå¤§çš„çº¿æ€§å±‚ `self.proj` æ¢æˆ3ä¸ª qkv å„è‡ªçš„çº¿æ€§å±‚å³å¯ 

### Deformable Attention

Deformable Attention å¯ä»¥è¯´è·Ÿ Attention ç›¸å·®å¾ˆå¤§ï¼ŒåŸºæœ¬ä¸Šæ²¡ä»€ä¹ˆè”ç³»ğŸ¤£å…¶æœ¬è´¨å°±æ˜¯å¯å˜å·ç§¯ï¼

ç”¨è¯­è¨€æ¥ç®€è¦å½¢å®¹ï¼š

1. ç»™å®š queryï¼Œé€šè¿‡ query è®¡ç®— sampling offsets & attention weights
2. ç»™å®š reference points for queryï¼Œé€šè¿‡ reference points + sampling offsets è·å¾—é‡‡æ ·ç‚¹ä½ç½®
3. ç»™å®š valueï¼Œé€šè¿‡é‡‡æ ·ç‚¹å’Œ attention weights è·å¾—æœ€ç»ˆè¾“å‡º

å…¶ä¸­éœ€è¦è€ƒè™‘ multi-scale value çš„æƒ…å†µï¼Œè§„å®š reference points åœ¨å„ä¸ª scale çš„é‡‡æ ·ç‚¹ç›¸åŒï¼ˆæœ‰æ²¡æœ‰å¯èƒ½ç›´æ¥å¢åŠ é‡‡æ ·ç‚¹æ˜¯ä¸€æ ·çš„æ•ˆæœï¼‰ï¼Œå¯ç®€ç­”ç†è§£ä¸ºå¦‚ä¸‹å›¾ï¼Œå…¶ç‰¹å¾å½¢çŠ¶ä¸º (B, N1+N2+N3+N4, C)

<img src="DETR Series 2.0/image-20230403214527125.png" alt="image-20230403214527125" style="zoom:50%;" />

è¦å®Œæˆ multi-scale çš„åŠŸèƒ½éœ€è¦çŸ¥é“æ¯ä¸€ä¸ª scale çš„ pixel æ•°é‡ï¼Œç„¶åä½¿ç”¨å¾ªç¯å®Œæˆå¯¹æ¯ä¸€ä¸ª scale çš„è¯¢é—®

```python

class MultiScaleDeformableAttention(nn.Module):
    """ Modified from Deformable-DETR for 1D
    """

    def __init__(
        self,
        embed_dim: int = 64,
        num_heads: int = 4,
        num_levels: int = 1,
        num_points: int = 4,
        dropout: float = 0.1,
    ):
        super().__init__()

        self.dropout = nn.Dropout(dropout)

        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.num_levels = num_levels
        self.num_points = num_points
        self.sampling_offsets = nn.Linear(embed_dim, num_heads * num_levels * num_points * 2)
        self.attention_weights = nn.Linear(embed_dim, num_heads * num_levels * num_points)
        self.value_proj = nn.Linear(embed_dim, embed_dim)
        self.output_proj = nn.Linear(embed_dim, embed_dim)
        
    def forward(
        self,
        query,	                       # (B, N, C)
        spatial_shapes = None,         # (levels, 2), sum is sequence len N
        reference_points = None,       # (B, N, level, 2)
        value = None,
        identity = None,
        query_pos = None,
        **kwargs):
        
        if value is None:	# True, when self-attetion
            value = query
        if identity is None:	# True
            identity = query
        if query_pos is not None:
            query = query + query_pos

        bs, num_query, _ = query.shape
        bs, num_value, _ = value.shape

        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value

        value = self.value_proj(value)

        value = value.view(bs, num_value, self.num_heads, -1)
        
        sampling_offsets = self.sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2
        )
        attention_weights = self.attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points
        )
        attention_weights = attention_weights.softmax(-1)
        attention_weights = attention_weights.view(
            bs,
            num_query,
            self.num_heads,
            self.num_levels,
            self.num_points,
        )
        
        # bs, num_query, num_heads, num_levels, num_points, 2
        if reference_points.shape[-1] == 2:
            offset_normalizer = torch.stack([spatial_shapes[..., 1], spatial_shapes[..., 0]], -1)
            sampling_locations = (
                reference_points[:, :, None, :, None, :]
                + sampling_offsets / offset_normalizer[None, None, None, :, None, :]
            )
        elif reference_points.shape[-1] == 4:
            sampling_locations = (
                reference_points[:, :, None, :, None, :2]
                + sampling_offsets
                / self.num_points
                * reference_points[:, :, None, :, None, 2:]
                * 0.5
            )
        
        output = multi_scale_deformable_attn_pytorch(
                 value, spatial_shapes, sampling_locations, attention_weights)
        output = self.output_proj(output)

        return self.dropout(output) + identity
```

åœ¨ä½¿ç”¨ Deformable Attention æ—¶æœ‰ä¸¤ä¸ªç‰¹ç‚¹å’Œä¸¤ä¸ªç–‘é—®ï¼š

1. æ•´ä¸ªè®¡ç®—è¿‡ç¨‹æ²¡æœ‰ `key` çš„å­˜åœ¨ï¼ä»…ç”± `query` å’Œ `value` è®¡ç®—
2. é€šè¿‡çº¦æŸé‡‡æ ·ç‚¹çš„æ•°é‡ï¼Œdeformable attention å¯ä»¥æ˜¾è‘—å‡å°‘è®¡ç®—é‡
3. ä¸ºä»€ä¹ˆè¦å¯¹ reference points è¿›è¡Œè¿™æ ·çš„å½’ä¸€åŒ–ï¼Ÿå¯èƒ½æ˜¯å¸Œæœ›å¯¹å°ºå¯¸æœ‰ä¸€å®šæ„ŸçŸ¥
4. sampling offsets ä»…ç”± query äº§ç”Ÿï¼Œæ²¡æœ‰ä¸ key è¿›è¡Œäº¤æµï¼Œè¿™ä¼šä¸ä¼šæŸå¤±ä¸€å®šæ€§èƒ½

è¾“å…¥ä¸­ä»æœ‰ä¸¤ä¸ªæœªè¯´æ˜çš„å˜é‡ï¼š

1. å¦‚ä½•ç”Ÿæˆ reference points
   1. åœ¨ self-attention ä¸­ï¼Œreference points å³ä¸ºæ¯ä¸ª pixel æœ¬èº«çš„åæ ‡
   2. åœ¨ cross-attention ä¸­ï¼Œreference points æœ‰ä¸¤ç§æ–¹æ³•ï¼š
      - ä½¿ç”¨ä¸€ä¸ª `nn.Linear(dim, 2)` å¤„ç† queryï¼Œç„¶åå† sigmoid ç”Ÿæˆ
      - åœ¨ä¸¤é˜¶æ®µä¸­ï¼Œæ¥æºäºç¬¬ä¸€é˜¶æ®µçš„ proposal
2. å¦‚ä½•ç”Ÿæˆ query positional embedding
   1. åœ¨ self-attention ä¸­ï¼Œä½¿ç”¨å¸¸è§„çš„ sin positional embedding + learnable level embedding
   2. åœ¨ cross-attention ä¸­ï¼Œå¯ä¸ä½¿ç”¨ positional embeddingï¼Œå› ä¸ºæœ‰äº† reference pointsã€‚ä½†åŠ å…¥ä¹‹åæ•ˆæœæ›´ä½³ï¼Œå³ä½¿ç”¨ä¸€ä¸ª `nn.Linear(dim, dim)` å¯¹ reference points è¿›è¡Œç¼–ç ï¼Œè¿™å°±æ˜¯ Dynamic Anchor çš„ä¸»è¦å‡çº§ï¼ˆä¸¤è¡Œä»£ç å°±è¡Œï¼‰

### æ„å»ºTransformer

æ„å»º transformer è¿˜è¦æœ‰ä¸€ä¸ªç¯èŠ‚ï¼Œå°±æ˜¯ FFNï¼Œå³ä¸¤ä¸ªçº¿æ€§å±‚

```python
class FFN(nn.Module):
    def __init__(
        self,
        embed_dim=256,
        feedforward_dim=1024,
        output_dim=None,
        num_fcs=2,
        activation=nn.ReLU(inplace=True),
        ffn_drop=0.0,
        fc_bias=True,
        add_identity=True,
    ):
        super(FFN, self).__init__()
        assert num_fcs >= 2, "num_fcs should be no less " f"than 2. got {num_fcs}."
        self.embed_dim = embed_dim
        self.feedforward_dim = feedforward_dim
        self.num_fcs = num_fcs
        self.activation = activation

        output_dim = embed_dim if output_dim is None else output_dim

        layers = []
        in_channels = embed_dim
        for _ in range(num_fcs - 1):
            layers.append(
                nn.Sequential(
                    nn.Linear(in_channels, feedforward_dim, bias=fc_bias),
                    self.activation,
                    nn.Dropout(ffn_drop),
                )
            )
            in_channels = feedforward_dim
        layers.append(nn.Linear(feedforward_dim, output_dim, bias=fc_bias))
        layers.append(nn.Dropout(ffn_drop))
        self.layers = nn.Sequential(*layers)
        self.add_identity = add_identity

    def forward(self, x, identity=None) -> torch.Tensor:

        out = self.layers(x)
        if not self.add_identity:
            return out
        if identity is None:
            identity = x
        return identity + out
```

æ³¨æ„ï¼Œè¿™é‡Œçš„ `FFN` ä¾ç„¶ä¿ç•™äº† identity connectionï¼Œå’Œä¸Šè¿° attention ä¸€æ ·ã€‚ä¸‹é¢å°±å¯ä»¥é€šè¿‡ç»„åˆå„ä¸ªç½‘ç»œå±‚æ¥æ­å»º transformer block å•¦ğŸ˜

ä¸€èˆ¬æ¥è®² transformer block å°±ä¸¤ç§å½¢å¼

```python
operation_order=("self_attn", "norm", "ffn", "norm")
operation_order=("self_attn", "norm", "cross_attn", "norm", "ffn", "norm")
```

å‰è€…ä¸º encoderï¼Œåè€…ä¸º decoderï¼Œnorm å±‚ä¸º `nn.LayerNorm`

## Deformable DETR æµç¨‹

### init

éœ€è¦æ„æ¶çš„æ¨¡å—å¦‚ä¸‹ï¼š

1. `self.backbone & self.neck` ç”¨äºæå–å›¾åƒç‰¹å¾ï¼Œé€šå¸¸ç”± resnet or swin æå–ï¼Œç„¶åç”¨å·ç§¯å°†å„ä¸ªåˆ†è¾¨ç‡çš„è¾“å‡ºç»Ÿä¸€åˆ°ç›¸åŒ channel æ•°é‡
2. `self.position_embedding` ç”¨äºäº§ç”Ÿå›¾åƒä½ç½®åµŒå…¥
3. `self.transformer` å³ Deformabel DETR çš„æ ¸å¿ƒæ¨¡å—ï¼Œè´Ÿè´£ç¼–ç å’Œè§£ç 
4. `self.class_embed & self.bbox_embed` ç”¨äºé¢„æµ‹ç±»åˆ«å’Œä½ç½®æ®‹å·®ï¼Œå…¶ä¸­ `class_embed` ä¸ºä¸€å±‚ Linearï¼Œ`bbox_embed` ä¸º3å±‚ MLPã€‚è¿™é‡Œæœ‰ä¸¤ä¸ªåŒºåˆ«ï¼š
   1. å¦‚æœæœ‰ box refine trickï¼Œåˆ™æ¯ä¸€ä¸ª decoder çš„ä¸­é—´è¾“å‡ºä½¿ç”¨ç‹¬ç«‹çš„ `class_embed & bbox_embed`ï¼Œç”¨ `copy.deepcopy` å®Œæˆå¤åˆ¶ï¼›
   2. å¦‚æœæœ‰ two stageï¼Œéœ€è¦å¤šä½™ä¸€ä¸ª `class_embed & bbox_embed` é€šè¿‡ encoder è¾“å‡ºè·å¾— proposal
5. `self.criterion` ç”¨äºè®¡ç®—æŸå¤±å‡½æ•°

### forward

1. å›¾åƒé¢„å¤„ç†ï¼Œè·å¾— $(B, 3, H, W)$ çš„ `ImageList`ï¼Œå¹¶è®°å½•äº†æ¯ä¸€å¼ å›¾ç¼©æ”¾å‰åçš„ image size

2. åˆ›å»º `image_masks` ç”¨äºåé¢è¿›è¡Œ `query_key_padding_mask`

3. åˆ›å»º `multi_level_positional_embeddings`

4. åˆå§‹åŒ– `query_embeds`ï¼š

   1. å¦‚æœä¸ºä¸¤é˜¶æ®µï¼Œquery æ˜¯ç”±ç¬¬ä¸€é˜¶æ®µçš„ proposal äº§ç”Ÿï¼Œæ‰€ä»¥åˆå§‹åŒ–ä¸º None
   2. å¦‚æœä¸ºå•é˜¶æ®µï¼Œquery åˆ™ç”± `nn.Embedding(num_query, dim)` äº§ç”Ÿï¼Œè¿™é‡Œä¹Ÿè¯´æ˜ `nn.Embedding.weight` èƒ½å¤Ÿç®€å•æ›¿ä»£ `nn.Parameter`

5. å°†å›¾åƒè¾“å…¥åˆ° transformer å½“ä¸­è·å¾— logits

   ```python
           (
               inter_states,
               init_reference,
               inter_references,
               enc_state,
               enc_reference,
           ) = self.transformer(
               multi_level_feats, multi_level_masks, multi_level_position_embeddings, query_embeds
           )
   ```

   å„ä¸ªè¾“å‡ºåˆ†åˆ«ä»£è¡¨ï¼š

   1. `inter_states` decoder å„ä¸ªå±‚è¾“å‡ºçš„ logits
   2. `init_reference` ä¸ºç¬¬ä¸€é˜¶æ®µäº§ç”Ÿçš„ proposal/reference points (+ denoising ground truth å¦‚æœä¸º DINO
   3. `inter_references` decoder å„ä¸ªå±‚è¾“å‡ºçš„ proposal/reference points
   4. `enc_state` ç¬¬ä¸€é˜¶æ®µäº§ç”Ÿçš„ logits
   5. `enc_reference` ä¸ºç¬¬ä¸€é˜¶æ®µäº§ç”Ÿçš„ proposal/reference pointsï¼Œä¸ `init_reference` ç­‰ä»·ï¼

6. å†æŠŠ decoder çš„ä¸­é—´è¾“å‡ºåˆé¢„æµ‹ä¸€éç”¨äºè®¡ç®—æŸå¤±ã€‚è¿™æ˜¯å› ä¸º decoder ä¸­é—´è¾“å‡ºçš„ reference points æ˜¯ detached tensorï¼Œæ‰€ä»¥ä¸èƒ½ç›´æ¥è®¡ç®—æ¢¯åº¦

7. è®¡ç®—æ¯ä¸€å±‚çš„æŸå¤±

## DINO Improves

### Box Refinement

Box refinementç®—æ³•æ˜¯ä¸€ç§è¿­ä»£å¾®è°ƒç­–ç•¥ï¼Œå®ƒç±»ä¼¼äº Cascade R-CNN æ–¹æ³•ï¼Œå¯ä»¥åœ¨æ¯ä¸ªè§£ç å™¨å±‚ä¸Šå¯¹è¾¹ç•Œæ¡†è¿›è¡Œå¾®è°ƒï¼Œæ‰€ä»¥åœ¨åˆ›å»ºçš„ `self.box_embed & self.class_embed` æ˜¯å„è‡ªç‹¬ç«‹çš„ï¼Œä¸å…±äº«å‚æ•°

### Look Forward Twice

å®é™…ä¸Šå°±æ˜¯æŠŠ box refinement ä¸­çš„ reference points æ²¡æœ‰ä»ä¸­ detach å‡ºæ¥ï¼ˆä¸€è¡Œä»£ç å°±è¡Œï¼‰ï¼Œå®é™…ä¸Šå°±æ˜¯å¢åŠ äº†æ¢¯åº¦çš„è®¡ç®—å¤æ‚åº¦ä»¥æå‡æ•ˆæœ

### Two Stage

ä¸¤é˜¶æ®µæ–¹æ³•å°±æ˜¯åˆ©ç”¨ encoder æå‡º proposal ä½œä¸º query çš„ reference points

ç”±äº two stage çš„å‡ºç°ï¼Œæ‰€æœ‰çš„ reference points ç”±2ç»´çš„ç‚¹ï¼Œå˜æˆäº†4ç»´çš„é€‰æ¡†ï¼Œè¿™åœ¨ deformable attention é‡Œæœ‰åšç®€è¦å¤„ç†ï¼Œä½†é‡ç‚¹ä»ç„¶è¿˜æ˜¯é€‰æ¡†çš„ä¸­å¿ƒ

ä»£ç ä¸­çš„ valid ratios åº”è¯¥å¯ä»¥ç§»é™¤ï¼Œç”šè‡³å¯èƒ½æ˜¯æœ‰å®³çš„ï¼Œå¹¶ä¸”ä»£ç é‡Œæœ‰ä¸€äº› bug æ²¡æœ‰ä¿®å¤ï¼Œå…ˆé™¤ä»¥åä¹˜ä»¥ï¼ŒåŸºæœ¬ä¸ŠæŠµæ¶ˆäº†

### Mixed Query Selection

ä¹Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯åªè¦ proposal çš„ä½ç½®ä½œä¸º reference pointsï¼Œä¸ç”¨ proposal ä½œä¸º queryã€‚çœŸæ­£çš„ content query ä¾ç„¶æ˜¯ learnable parameters

### Contrastive Denoising

Denoising æ€æƒ³éå¸¸ç®€å•ï¼šå°†ç»è¿‡å™ªå£°å¤„ç†çš„ gt ä½œä¸º queryï¼Œè¾“å…¥åˆ° decoder å½“ä¸­å»é‡å»º gtã€‚å…¶å‡†å¤‡è¿‡ç¨‹å¦‚ä¸‹ï¼š

1. ç¡®å®š `dn_groups`ï¼Œå°±æ˜¯æ¯ä¸€ä¸ª gt éœ€è¦å¤šå°‘ä¸ªå™ªå£°é€‰æ¡†ã€‚ä¸€ä¸ª group ç”± positive å’Œ negative ä¸¤ä¸ªéƒ¨åˆ†ç»„æˆ

2. åˆ›å»º `known_labels & known_bbox` å…¶å½¢çŠ¶ä¸º `labels & bbox` åœ¨ç¬¬ä¸€ä¸ªç»´åº¦é‡å¤ $(dn\_groups\times 2\times num\_gt)$ã€‚2ä»£è¡¨ pos & negï¼Œå®é™…ä¸Š negative åŒºåˆ«äº positive å™ªå£°å°±æ˜¯ box çš„ç¼©æ”¾æ›´å¤§ä¸€äº›

3. å¯¹ labels è¿›è¡Œéšæœºå™ªå£°ï¼Œå³å°†éƒ¨åˆ†ç±»åˆ«éšæœºæ›¿æ¢ä¸ºå…¶ä»–ç±»åˆ«

4. å¯¹ boxes è¿›è¡Œéšæœºå™ªå£°ï¼Œå³å¯¹é€‰æ¡†è¿›è¡Œéšæœºä½ç§»å’Œç¼©æ”¾

5. åˆ›å»º `input_query_label & input_query_box`ï¼š

   1. `input_query_label` å½¢çŠ¶ä¸º $(B, dn\_groups \times2\times pad\_size, C)$ï¼Œå…¶ä¸­ `pad_size` æ˜¯ä¸€ä¸ª batch æ‰€æœ‰æ ·æœ¬ä¸­ gt æ•°é‡çš„æœ€å¤§å€¼ï¼Œ`C` ä¸º embed dim (= 128)ï¼Œä½¿ç”¨ä¸€ä¸ª `nn.Embed` è¿›è¡Œè½¬æ¢
   2. `input_query_batch` å½¢çŠ¶ä¸º $(B, dn\_groups \times2\times pad\_size, 4)$ï¼Œä½œä¸ºå¯å˜æ³¨æ„åŠ›çš„ reference points

6. åˆ›å»º attention maskï¼Œå› ä¸º gt å™ªå£°ä¸èƒ½è¢«çœŸæ­£çš„ query æ‰€çœ‹è§ï¼Œä½†æ˜¯ gt å™ªå£°å¯ä»¥çœ‹è§çœŸæ­£çš„ queryï¼Œå„ä¸ª gt å™ªå£° groups ä¹‹é—´ä¸èƒ½ç›¸äº’çœ‹è§ï¼Œæœ€åçš„ mask å½¢çŠ¶å¯å¦‚å›¾æ‰€ç¤º

   <img src="DETR Series 2.0/image-20230407151520353.png" alt="image-20230407151520353" style="zoom:50%;" />

   ç°è‰²éƒ¨åˆ† `attention_mask=True`

å¹¶ä¸”ç”±äº `pad_size` çš„å­˜åœ¨ï¼Œåœ¨ä¹‹åè®¡ç®—æŸå¤±æ—¶ï¼Œä¼šæœ‰é›¶å¡«å……çš„ query åšå‡ºé¢„æµ‹ç»“æœï¼Œæˆ‘è®¤ä¸ºéœ€è¦æŠŠè¿™äº›é¢„æµ‹ä»æŸå¤±è®¡ç®—ä¸­å»é™¤ï¼Œä½†æ˜¯æºä»£ç ä¸­æ²¡æœ‰åšè¿™ä¸€æ­¥ï¼Œå¯èƒ½æ˜¯å› ä¸ºå½±å“ä¸å¤§ï¼Ÿ

## Loss

è¿™ä¸€éƒ¨åˆ†æˆ‘è¦è¯¦ç»†æ•´ç†ä¸€ä¸‹ä»£ç ï¼Œæ˜¯éå¸¸é€šç”¨çš„ç»“æ„

### Matcher

æ•´ä½“æ€è·¯ä¸ºï¼šåˆ©ç”¨åŒˆç‰™åˆ©åŒ¹é…æ³•è·å¾—æœ€å°æŸå¤±åŒ¹é…ã€‚å…³é”®åœ¨äºè®¡ç®—æŸå¤±çŸ©é˜µ Cost Matrix

1. ä½¿ç”¨ cross entropy style æˆ–è€… focal loss style æ¥è®¡ç®—åˆ†ç±»æŸå¤±ï¼Œç€é‡ç†è§£ cross entropy style å°±å¥½

   ```python
        if self.cost_class_type == "ce_cost":
            cost_class = -out_prob[:, tgt_ids]
            
        elif self.cost_class_type == "focal_loss_cost":
            alpha = self.alpha
            gamma = self.gamma
            neg_cost_class = (1 - alpha) * (out_prob**gamma) * (-(1 - out_prob + 1e-8).log())
            pos_cost_class = alpha * ((1 - out_prob) ** gamma) * (-(out_prob + 1e-8).log())
            cost_class = pos_cost_class[:, tgt_ids] - neg_cost_class[:, tgt_ids]
   ```

   è¿™é‡Œ `out_prob` å³ä¸ºé¢„æµ‹çš„å¯èƒ½æ€§ï¼Œå…¶å½¢çŠ¶ä¸º $(B\times num\_queries, num\_classes)$ï¼Œé€šè¿‡å–å¾— `tgt_ids` æ¥è·å¾—å¯¹åº”ç±»åˆ«çš„æŸå¤±

   å¹¶ä¸”è¿™é‡Œçš„ `focal_loss` ä¼¼ä¹æ˜¯è®¡ç®—é”™äº†ï¼Œ[issue](https://github.com/IDEA-Research/detrex/issues/196) ä¹Ÿæ²¡æœ‰å¾ˆå¥½çš„å›å¤ï¼Œè¯´æ˜¯å€Ÿç”¨çš„ deformabel detr çš„æºä»£ç 

2. è®¡ç®— L1 è·ç¦»å’Œ `generalized_box_iou` 

   ```python
           # Compute the L1 cost between boxes
           cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)
   
           # Compute the giou cost betwen boxes
           cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))
   ```

   GIoU  `return iou - (area - union) / (area + 1e-6)`

   **éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œå°†æ‰€æœ‰ batch sample éƒ½åˆåˆ°ä¸€å—äº†ï¼åœ¨åé¢ç”¨åˆ‡ç‰‡è§£å†³** 

3. åˆ©ç”¨åŠ æƒè®¡ç®—æŸå¤±çŸ©é˜µ

   ```python
           # Final cost matrix
           C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou
           C = C.view(bs, num_queries, -1).cpu()
   ```

4. åŒˆç‰™åˆ©åŒ¹é…

   ```python
           sizes = [len(v["boxes"]) for v in targets]
           indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]
   ```

   ç„¶åå°† `indices` è½¬ä¸º tensorï¼Œ`indices` æœ¬èº«ä¸ºä¸€ä¸ª list of tuple (index_i, index_j)

   ```python
           return [
               (torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64))
               for i, j in indices
           ]
   ```

### Last Layer Loss

#### bboxes

ä¸ºäº†è®¡ç®—æŸå¤±ï¼Œé¦–å…ˆåº”è¯¥é€šè¿‡ matcher ç®—å‡ºçš„ index è·å¾—åŒ¹é…çš„ boxesï¼Œç›¸å½“äºå† concat èµ·æ¥ï¼Œå¹¶åŠ ä¸Š batch idx

```python
        ...
    	idx = self._get_src_permutation_idx(indices)
        src_boxes = outputs["pred_boxes"][idx]
        target_boxes = torch.cat([t["boxes"][i] for t, (_, i) in zip(targets, indices)], dim=0)
        
    def _get_src_permutation_idx(self, indices):
        # permute predictions following indices
        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])
        src_idx = torch.cat([src for (src, _) in indices])
        return batch_idx, src_idx
```

æœ‰äº†åŒ¹é…ç»“æœè¿‡åå°±ç›´æ¥è®¡ç®— L1 å’Œ GIoU

```python

        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction="none")

        losses = {}
        losses["loss_bbox"] = loss_bbox.sum() / num_boxes

        loss_giou = 1 - torch.diag(
            generalized_box_iou(
                box_cxcywh_to_xyxy(src_boxes),
                box_cxcywh_to_xyxy(target_boxes),
            )
        )
        losses["loss_giou"] = loss_giou.sum() / num_boxes
```

#### classification

åˆ†ç±»ä¹Ÿæ˜¯ä¸€æ ·çš„ï¼Œå…ˆè®¡ç®—åŒ¹é…çš„æ ‡ç­¾ï¼Œç„¶åæ„é€  one hot å‘é‡ï¼Œæœ€åè®¡ç®— focal loss

```python
    def loss_labels(self, outputs, targets, indices, num_boxes):

        src_logits = outputs["pred_logits"]

        idx = self._get_src_permutation_idx(indices)
        target_classes_o = torch.cat([t["labels"][J] for t, (_, J) in zip(targets, indices)])
        target_classes = torch.full(
            src_logits.shape[:2],	# shape (B, num_queries)
            self.num_classes,
            dtype=torch.int64,
            device=src_logits.device,
        )
        target_classes[idx] = target_classes_o

        # Computation classification loss
        if self.loss_class_type == "ce_loss":
            loss_class = F.cross_entropy(
                src_logits.transpose(1, 2), target_classes, self.empty_weight
            )
        elif self.loss_class_type == "focal_loss":
            # src_logits: (b, num_queries, num_classes) = (2, 300, 80)
            # target_classes_one_hot = (2, 300, 80)
            target_classes_onehot = torch.zeros(
                [src_logits.shape[0], src_logits.shape[1], src_logits.shape[2] + 1],
                dtype=src_logits.dtype,
                layout=src_logits.layout,
                device=src_logits.device,
            )
            target_classes_onehot.scatter_(2, target_classes.unsqueeze(-1), 1)
            target_classes_onehot = target_classes_onehot[:, :, :-1]
            loss_class = (
                sigmoid_focal_loss(
                    src_logits,
                    target_classes_onehot,
                    num_boxes=num_boxes,
                    alpha=self.alpha,
                    gamma=self.gamma,
                )
                * src_logits.shape[1]
            )

        losses = {"loss_class": loss_class}
```

æ„é€  One hot å‘é‡å¯ä»¥ç”¨ scatter ä¹Ÿå¯ä»¥ç”¨ `F.one_hot`

#### full loss

å®Œæ•´çš„ api è°ƒç”¨å¦‚ä¸‹

```python
        for loss in self.losses:
            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))
            
            
    def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):
        loss_map = {
            "class": self.loss_labels,
            "boxes": self.loss_boxes,
        }
        assert loss in loss_map, f"do you really want to compute {loss} loss?"
        return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)
```

### Aux Loss

ä¸­é—´å±‚æŸå¤±è¾“å‡ºå°±æ˜¯ Last Layer Loss çš„å¾ªç¯ï¼Œå®Œå…¨ä¸€è‡´ï¼

```python
        if "aux_outputs" in outputs:
            for i, aux_outputs in enumerate(outputs["aux_outputs"]):
                indices = self.matcher(aux_outputs, targets)
                if return_indices:
                    indices_list.append(indices)
                for loss in self.losses:
                    l_dict = self.get_loss(loss, aux_outputs, targets, indices, num_boxes)
                    l_dict = {k + f"_{i}": v for k, v in l_dict.items()}
                    losses.update(l_dict)
```

### Two Stage Loss

ä¾ç„¶ä¹Ÿæ˜¯åŒæ ·çš„æŸå¤±è®¡ç®—

```python
        if "enc_outputs" in outputs:
            enc_outputs = outputs["enc_outputs"]
            if self.two_stage_binary_cls:
                for bt in targets:
                    bt["labels"] = torch.zeros_like(bt["labels"])
            indices = self.matcher(enc_outputs, targets)
            for loss in self.losses:
                l_dict = self.get_loss(loss, enc_outputs, targets, indices, num_boxes)
                l_dict = {k + "_enc": v for k, v in l_dict.items()}
                losses.update(l_dict)
```

ä¸è¿‡å¯ä»¥è®¾ç½® `binary_cls`ï¼Œè¿™å°±å®Œå…¨é é½äº†ä¼ ç»Ÿçš„ä¸¤é˜¶æ®µæ–¹æ³•ï¼šç¬¬ä¸€ä¸ªé˜¶æ®µåªé¢„æµ‹å‰æ™¯ï¼Œä¸é¢„æµ‹ç±»åˆ«ã€‚**è¿™é‡Œç›´æ¥å°†æ ‡ç­¾å…¨éƒ¨è®¾ç½®ä¸º 0 å°±å¯å®Œæˆè¯¥ç›®æ ‡ï¼**å®é™…ä¸Šç¬¬ä¸€é˜¶æ®µçš„æœ€é‡è¦ä½œç”¨è¿˜æ˜¯æä¾› reference points æ‰€ä»¥è¿™ä¸ªç±»åˆ«ä¸é‡è¦

### DN Loss

DN å”¯ä¸€ä¸åŒçš„æ˜¯ä¸éœ€è¦ Matcher è¿›è¡ŒåŒ¹é…ï¼Œå…¶æ­£è´Ÿæ ·æœ¬éƒ½å·²ç»åˆ†é…å¥½äº†

```python
target_idx = [[0, 1, ..., n],
              [0, 1, ..., n]
              ...(repeat dn_num times)
              [0, 1, ..., n]]	# n ground truth, (dn_num, gt_num_all)
output_idx = (torch.tensor(range(dn_num)) * single_padding	# each starting point
                    ).long().cuda().unsqueeze(1) + t	# (dn_num, gt_num_all)

dn_idx = (output_idx, target_idx)	# match indices
              
            for loss in self.losses:
                losses.update(
                    self.get_loss(loss, output_known_lbs_bboxes, targets, dn_idx, num_boxes * dn_num))
              
              # aux loss
              for i in range(aux_num):
              	...
```

å¹¶ä¸”è¯¥åˆ†é…ç»“æœå¯¹ä¸­é—´å±‚çš„é¢„æµ‹ç»“æœä¾ç„¶å¦‚æ­¤ï¼Œä»å§‹è‡³ç»ˆä¸æ”¹å˜ï¼
