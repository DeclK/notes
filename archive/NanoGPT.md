# NanoGPT

å­¦ä¹ ç¬”è®° [NanoGPT](https://github.com/karpathy/nanoGPT)

é™¤äº†å­¦ä¹  GPT ä»¥å¤–è¿˜è®¡åˆ’å­¦ä¹ ï¼š1. Llama & Llama2 è®ºæ–‡ä»¥åŠæ¨¡å‹ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­åº”è¯¥å°±æŠŠ Causal Language Model ç»™å­¦äº†ï¼›2. Mistral æ¨¡å‹

TODO: minBPE

TODO: Scaling laws from OpenAI & Chinchilla

## Concept

### NanoGPT

- Tokenize

  nanoGPT é€‰æ‹©äº†æœ€ç®€å•çš„ tokenizationï¼Œä»¥å•ä¸ªå­—æ¯ä½œä¸º tokenã€‚å¯¹äºå…¶ä»–çš„ tokenization æ–¹æ³•ï¼ŒAndrej åªæåˆ°äº†ä¸¤ä¸ªåº“ç”¨äº tokenizeï¼šsentencepiece & tiktoken

- Datasetï¼ŒAndrej ä½¿ç”¨äº†ä¸€ä¸ªç®€å•çš„èå£«æ¯”äºšé›†æ–‡æœ¬

- åœ¨ Training çš„æ—¶å€™ä¸ä¼šå°†æ‰€æœ‰çš„æ–‡æœ¬å…¨éƒ¨è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼Œè€Œæ˜¯éšæœºé‡‡æ ·ä¸€æ®µæ–‡æœ¬ï¼Œæ–‡æœ¬é•¿åº¦ä¸º `block_size` ä¸ªå­—æ¯ï¼Œè¿™æ ·ä¹Ÿå¾ˆæ–¹ä¾¿å®šä¹‰ `batch_size`

  ç”Ÿæˆçš„ input & targets éƒ½ä¸º `(batch_size, block_size)` å½¢çŠ¶çš„å¼ é‡ï¼Œå…¶å…ƒç´ ä¸º token idã€‚targets å³ä¸º next token idï¼Œä¸ºäº†è®© targets çš„å½¢çŠ¶å¯¹ä¸Š inputsï¼Œåœ¨è·å¾—æ–‡æœ¬æ•°æ®æ—¶éœ€è¦å¤šè·å¾—ä¸€ä¸ª tokenï¼Œä¹Ÿå°±æ˜¯ `block_size + 1`

- `BigramLanguageModel`

  Andrej é¦–å…ˆä½¿ç”¨äº†ä¸€ä¸ªæœ€ç®€å•çš„ Bigram æ¨¡å‹æ¥è¯´æ˜ GPT çš„æ•´ä¸ªæµç¨‹ï¼ŒåŒ…å«è®­ç»ƒå’Œç”Ÿæˆ

  ```python
  import torch
  import torch.nn as nn
  import torch.nn.functional as F
  
  class BigramLanguageModel(nn.Module):
      def __init__(self, vocab_size):
          super(BigramLanguageModel, self).__init__()
          self.embeddings = nn.Embedding(vocab_size, vocab_size)
      
      def forward(self, idx, targets):
          """ Bigram language model only considers the previous token
          and predicts the next token.
          Args:
              - idx: (B, T) tensor of token idx, T is time (seq length)
              - targets: (B, T) tensor of token idx
          """
          logits = self.embeddings(idx)   # (B, T, vocab_size)
          loss = F.cross_entropy(logits, targets)
          return logits, loss
  
      @torch.no_grad()
      def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
          """ Generate the sequence max_new_tokens times.
          Args:
              - idx: (B, T)
              - max_new_tokens: int
          """
          for _ in range(max_new_tokens):
              #get the logits for the index in the sequence
              logits, _ = self(idx)
  
              # pluck the logits at the final step and scale by desired temperature
              logits = logits[:, -1, :] / temperature
              
              # optionally crop the logits to only the top k options
              if top_k is not None:
                  v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                  logits[logits < v[:, [-1]]] = -float('Inf')
  
              # get the probabilities and sample
              probs = F.softmax(logits, dim=-1)
              idx_next = torch.multinomial(probs, num_samples=1)
              idx = torch.cat((idx, idx_next), dim=1)
  
          return idx
  ```

- æ¥ä¸‹æ¥èŠ±äº†å¤§é‡æ—¶é—´ä»‹ç»äº† self-attention with maskï¼Œä¸ºçš„å°±æ˜¯è®©å½“å‰ token ä¸çœ‹è§æœªæ¥çš„ token

- Tempreture å±•ç¤º

  ```python
  # A demostration on temperature
  import torch
  import matplotlib.pyplot as plt 
  
  temperature = [0.1, 1.0, 10]
  temperature = torch.tensor(temperature) # (5,)
  
  x = torch.linspace(-1, 1, 5)   # (N,)
  x_with_temp = x / temperature[:, None]  # (5, 10)
  y = torch.softmax(x_with_temp, dim=-1)  # (5, 10)
  y = y.numpy()
  
  # draw the plot
  fig, ax = plt.subplots()
  for i in range(len(temperature)):
      ax.plot(x, y[i], label=f'temperature={temperature[i]:.1f}', marker='o')
  ax.legend()
  # label x and y
  ax.set_xlabel('logits before temperature scaling')
  ax.set_ylabel('softmax with temperature scaling')
  plt.show()
  
  ```

  <img src="NanoGPT/image-20240319202547223.png" alt="image-20240319202547223" style="zoom: 67%;" />

  æ¸©åº¦è¶Šé«˜ï¼Œè¶Šå€¾å‘äºå¹³å‡é‡‡æ ·ï¼Œæ¸©åº¦è¶Šä½è¶Šå€¾å‘äºæœ€å¤§å€¼

- æ¥ä¸‹æ¥é€æ­¥å®Œæˆ transformer blockï¼š

  - self attentionï¼Œloss single head 2.4, loss multi head 2.28

  - feed forwardï¼Œloss 2.24

  - residualï¼Œloss 2.08

  - layer normï¼Œloss 2.06

  - more blocks & dropout, loss 1.49 (reproduced is the same)

    ```txt
    Clown:
    So is sweetest lay the
    backed but our savegred my dagged
    heard it is unquiet.
    
    PAULIXENES:
    I will we have stars, I must be spring it.
    ```

  Andrej åœ¨è¿™é‡Œå¼ºè°ƒäº†ï¼Œ**åœ¨è¿‡å»å‡ å¹´æ—¶é—´é‡Œï¼Œå¤§å®¶å¯¹åŸå§‹ transformer çš„ç»“æ„æ”¹å˜éå¸¸å°‘ï¼Œåªæœ‰ä¸€ä¸ªæ”¹å˜ï¼šå°† layer norm æå‰ï¼Œå³ï¼šä½¿ç”¨ pre-norm**ã€‚åœ¨è¾“å‡ºåˆ° `lm_head` ä¹‹å‰ä¹Ÿä½¿ç”¨äº† layer norm

- æœ€åçš„è¾“å‡ºç»“æœ

- Positional encoding é‡‡ç”¨ç®€å•çš„ nn.Embedding

- ç®€å•ä»‹ç» encoder-decoder æ¶æ„ï¼Œå› ä¸º GPT æ˜¯ decoder-only æ¶æ„ï¼ŒK & V comes from sideã€‚Decoder Only æ¶æ„ä¸éœ€è¦ encoderï¼Œè¿™æ˜¯å› ä¸º encode éƒ¨åˆ†ä¹Ÿå¯ä»¥é€šè¿‡ prompt + decode å®Œæˆï¼Œå¯ä»¥è®¤ä¸º encode & decoder èåˆäº†ğŸ¤”è®©æ•´ä¸ª transformer architecture å˜æˆäº†ä¸€ä¸ªé€šç”¨æœºå™¨ï¼Œé€šè¿‡ä¸åŒçš„ prompt å®Œæˆæ— é™çš„å¯èƒ½æ€§

- å’Œ ChatGPT è”ç³»èµ·æ¥

  ç®€å•çš„ GPT æ¨¡å‹åªèƒ½å¤Ÿç»­å†™æ–‡å­—ï¼Œä¸ºäº†è®© GPT æˆä¸ºæ›´æœ‰ç”¨çš„å·¥å…·ï¼Œåœ¨é€šè¿‡ç¬¬ä¸€é˜¶æ®µçš„ pre-train (predict next token) è¿‡åï¼Œéœ€è¦è¿›è¡Œç¬¬äºŒé˜¶æ®µçš„å¾®è°ƒï¼Œè¿™ä¸ªå¾®è°ƒé˜¶æ®µä¹Ÿåˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤

  1. æ”¶é›†æ ·æœ¬æ•°æ®ï¼Œè¿™äº›æ•°æ®ä¸ä¼šå¾ˆå¤š (1e3 é‡çº§)ï¼Œé€šè¿‡åœ¨è¿™äº›æ ·æœ¬ä¸Šè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒ (Supervised-Fine-Tuning)ï¼Œè¿™ä¹Ÿæ˜¯æœ€ç®€å•çš„ align è¿‡ç¨‹ï¼Œè®©æ‰€è®­ç»ƒçš„ GPT å›ç­”å€¾å‘äºä½ æ‰€æ”¶é›†çš„æ•°æ®æ ·å¼
  2. è®­ç»ƒä¸€ä¸ª Reward model æ¥è¯„ä¼°å“ªä¸ªç”Ÿæˆç»“æœæ˜¯æ›´å¥½çš„
  3. è®­ç»ƒä¸€ä¸ª PPO model æ¥ç”Ÿæˆæ›´å¥½çš„é‡‡æ ·ç­–ç•¥ä½¿å¾—é‡‡æ ·ç»“æœæ›´ç¬¦åˆ reward model å–œå¥½ 

  ![image-20240319211257999](NanoGPT/image-20240319211257999.png)

### Llama

é€šè¿‡ NanoGPT å¯ä»¥å¯¹ GPT çš„æ•´ä¸ªè®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹æœ‰æ¸…æ™°çš„è®¤çŸ¥ï¼Œä½†æ˜¯éšç€æŠ€æœ¯çš„å‘å±•ï¼Œè¿˜è¯ç”Ÿäº†ä¸å°‘æ–°çš„ç»“æ„ï¼ŒåŒ…æ‹¬ KVCache, GQA, RoPE, RMSNorm, SwiGLUã€‚å¯ä»ä¸‹å›¾è·å¾—ç®€å•å¯¹æ¯”

<img src="NanoGPT/image-20250211210240094.png" alt="image-20250211210240094" style="zoom:80%;" />

- åœ¨ç»™å®šè®¡ç®—èµ„æº (training budget) çš„æ¡ä»¶ä¸‹ï¼Œæœ€å¥½è¡¨ç°çš„æ¨¡å‹ä¸æ˜¯å‚æ•°é‡æœ€å¤§çš„æ¨¡å‹ï¼Œè€Œæ˜¯è¾ƒå°æ¨¡å‹åœ¨æ›´å¤šçš„æ•°æ®ä¸Šè®­ç»ƒåï¼Œæ•ˆæœæ›´å¥½ã€‚ç»“æœå‡ºè‡ª Chinchilla

  These efforts are based on the assumption that more parameters will lead to better performance. However, recent work from Hoffmann et al. (2022) shows that, for a given compute budget, the best performances are not achieved by the largest models, but by smaller models trained on more data.

- åŸºäº Chinchilla çš„å¯å‘ï¼ŒLlama æƒ³è¦è®­ç»ƒå¯¹æ¨ç†æ›´å‹å¥½çš„æ¨¡å‹ï¼Œæ‰€ä»¥é€‰æ‹©äº†æ›´é«˜æ•ˆçš„æ¨ç†ç»“æ„å’Œâ€œè¾ƒå°â€çš„æ¨¡å‹ï¼Œä»è€Œè®­ç»ƒæ›´å¤šçš„ tokensï¼Œè¾¾åˆ°æ›´å¥½çš„æ•ˆæœ

- Tokenizer: BPE from sentencepiece

- Dataset: åŒ…å« 1.4T tokens å¼€æºæ•°æ®é›†

  ![image-20240320113851413](NanoGPT/image-20240320113851413.png)

- Archetecture

  ç›¸æ¯”äºåŸå§‹ Transformer ç»“æ„ï¼ŒLlama çš„æ”¹åŠ¨åªæœ‰3ç‚¹ï¼š

  1. Pre-normï¼Œinspired from GPT-3ï¼Œç”¨äºæ”¹å–„è®­ç»ƒç¨³å®šæ€§ï¼Œå¹¶ä¸”ä½¿ç”¨ RMSNorm è€Œä¸æ˜¯ LayerNormï¼Œå› ä¸º RMSNorm åœ¨è®¡ç®—ä¸Šæ›´åŠ ç®€å•æ‰€ä»¥æ›´å¿«ï¼ˆunverified, no ablationï¼‰ï¼Œå¹¶ä¸”ç²¾åº¦ä¸Šæ²¡æœ‰æ”¹å˜

  2. SwiGLUï¼Œinspired from PaLM

     [GELU](https://kexue.fm/archives/7309) [å¤§æ¨¡å‹åŸºç¡€ï½œæ¿€æ´»å‡½æ•°ï½œä»ReLU åˆ°SwiGLU](https://zhuanlan.zhihu.com/p/650237644) [GLU Variants](https://arxiv.org/abs/2002.05202v1)

     Swish functionï¼Œ$\sigma$ is sigmoid function, also called SiLU
     $$
     Swish_\beta (x)=xÂ·\sigma(\beta x)
     $$
     GLU (Gated Linear Unit)
     $$
     GLU(x,W_1,b_1,W_2,b_2)=\sigma(xW_1+b_1)Â·(xW_2+b_2)
     $$
     ç®€å•æ¥è¯´æ˜¯ä¸€ä¸ªé—¨æ§å•å…ƒæœ‰ä¸¤ä¸ªçº¿æ€§å±‚

     ```python
     class GLU(nn.Module):
         def forward(self, x):
             return  torch.sigmoid(self.linear_1(x)) * self.linear_2(x)
     ```

     è€Œ SwiGLU å°±æ˜¯æŠŠ sigmoid å‡½æ•°æ¢æˆäº† Swish
     $$
     SwiGLU(x,W_1,b_1,W_2,b_2)=Swish_{\beta}(xW_1+b_1)Â·(xW_2+b_2)
     $$

     ```python
     def swish(x, beta=1.):
         return x * torch.sigmoid(beta * x)
     
     class SwiGLU(nn.Module):
         def forward(self, x):
             return  swish(self.linear_1(x)) * self.linear_2(x)
     ```

     ä½†ä¸€èˆ¬å¤§å®¶è¯´çš„ SwiGLU å…¶å®è¿˜è¦åŠ ä¸€å±‚ linear ä¸€å±‚ normï¼Œä»¥æ›¿æ¢ Transformer ä¸­çš„ FFN

     ```python
     class SwiGLU(nn.Module):
         def forward(self, x):
             x_gate = self.fc1_g(x)
             x = self.fc1_x(x)
             x = self.act(x_gate) * x	# silu
             x = self.drop1(x)
             x = self.norm(x)
             x = self.fc2(x)
             x = self.drop2(x)
             return x
     
     class FFN(nn.Module):
         def forward(self, x):
             x = self.fc1(x)
             x = self.act(x)	# relu
             x = self.drop1(x)
             x = self.norm(x)
             x = self.fc2(x)
             x = self.drop2(x)
             return x
     ```

     also, it seems dropout should be in front of norm

  3. Rotary Embedding, inspired from RoFormer

     [Transformerå‡çº§ä¹‹è·¯ï¼š2ã€åšé‡‡ä¼—é•¿çš„æ—‹è½¬å¼ä½ç½®ç¼–ç ](https://zhuanlan.zhihu.com/p/359502624)

     Needs more time

     > è¿™æ˜¯ä¸€ç§é…åˆAttentionæœºåˆ¶èƒ½è¾¾åˆ°â€œç»å¯¹ä½ç½®ç¼–ç çš„æ–¹å¼å®ç°ç›¸å¯¹ä½ç½®ç¼–ç â€çš„è®¾è®¡ã€‚è€Œä¹Ÿæ­£å› ä¸ºè¿™ç§è®¾è®¡ï¼Œå®ƒè¿˜æ˜¯ç›®å‰å”¯ä¸€ä¸€ç§å¯ç”¨äºçº¿æ€§Attentionçš„ç›¸å¯¹ä½ç½®ç¼–ç 

- Optimizer

  AdamW + Cosine learning rate schedule with warm up + grad clip

  <img src="NanoGPT/image-20240320173839598.png" alt="image-20240320173839598" style="zoom: 67%;" />

- Training Infra

  ä¸ºäº†è®©è®­ç»ƒæ›´åŠ é«˜æ•ˆï¼ŒLlama ä½¿ç”¨äº† xformers å®ç°çš„ flash attentionï¼Œå¹¶ä¸”ä½¿ç”¨ checkpointing æŠ€æœ¯ä¿ç•™é‚£äº›é‡è®¡ç®—å¾ˆè´¹æ—¶çš„ activations (output of linear)ï¼Œå¹¶ä¸”è¿˜ä½¿ç”¨äº†ä¸€äº› auto parallel æŠ€æœ¯å¢å¤§ GPU åˆ©ç”¨ç‡

  Llama ä½¿ç”¨äº† 2048 A100 è®­ç»ƒ 1.4T tokensï¼Œæ€»å…±èŠ±è´¹ 21 å¤©

### Llama 2

[ä¸€æ–‡è¯»æ‡‚Llama 2ï¼ˆä»åŸç†åˆ°å®æˆ˜ï¼‰](https://www.zhihu.com/tardis/zm/art/653303123)

- åœ¨ç»“æ„ä¸Šåªæœ‰ä¸€ä¸ªåŒºåˆ«ï¼šä½¿ç”¨äº† GQA (Group Query Attention)

  å…ˆäº†è§£ [MQA](https://zhuanlan.zhihu.com/p/634236135)ï¼Œå…¶æœ¬è´¨å°±æ˜¯æ‰€æœ‰çš„ head å…±ç”¨ä¸€å¥— key & valueï¼Œä¹‹å‰æ˜¯å„ä¸ª head éƒ½æœ‰ä¸åŒçš„ key & valueï¼Œè¿™å¤§å¤§å‡å°‘ kv çš„å­˜å‚¨

  è€Œ GQA ç›¸å½“äºæ˜¯è¿‡åº¦ï¼Œä¹Ÿå°±æ˜¯ä¸æ˜¯æ‰€æœ‰çš„ head å…±äº«ä¸€å¥—ï¼Œè€Œæ˜¯ group ä¸ª head å…±äº«ä¸€å¥—

  åœ¨å®ç°ä¸Šæ˜¯é€šè¿‡æŒ‡å®š `num_key_value_heads` æ§åˆ¶ kv head æ•°é‡ï¼šå¦‚æœè¯¥å€¼ä¸º 1ï¼Œåˆ™ä¸º MQAï¼Œå¦‚æœè¯¥å€¼ä¸º `num_head` é‚£å°±æ˜¯æ™®é€šçš„ Multi-Head Attention

- åœ¨è®­ç»ƒä¸Šæ•°æ®å¤šäº† 40%ï¼Œä¸Šä¸‹æ–‡é•¿åº¦ä¹Ÿç¿»å€ï¼ˆCredit to GQAï¼‰

### Qwen

- Architecture

  1. untied embedding

     åœ¨è¯­è¨€æ¨¡å‹ä¸­ï¼Œé€šå¸¸ä¼šå°† word embedding layer å’Œæœ€åçš„ `lm_head` å…±äº«å‚æ•°ï¼Œè¿™æ˜¯éå¸¸è‡ªç„¶çš„æƒ³æ³•ï¼Œå› ä¸ºä»–ä»¬æ„æˆé€†æ˜ å°„ï¼Œåœ¨ Llama ä¸­å°±æ˜¯è¿™æ ·åšçš„

     - (N, ) token idx -> (nn.Embed) -> (N, C) token emb
     - (N, C) token emb -> (nn.Embed.T) -> (N, N) logits -> sample and generate

     ```python
     class LlamaForCausalLM(LlamaPreTrainedModel):
         _tied_weights_keys = ["lm_head.weight"]
     
         def __init__(self, config):
             super().__init__(config)
             ...
             self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
     
     ```

     è€Œ untied embedding å…¶å®å°±æ˜¯ `lm_head` é‡‡å–ç›¸åŒå•ç‹¬çš„æƒé‡ï¼Œä¸å’Œ word embedding å…±äº«

  2. åœ¨è¿›è¡Œ rotary positional embedding æ—¶ä½¿ç”¨ fp32 ç²¾åº¦ï¼Œå®Œæˆåå†è½¬ä¸º fp16

  3. åœ¨ attention qkv linear ä¸­åŠ å…¥ biasï¼Œåˆ©äº rotary embedding åšå¤–æ¨

  å…¶ä½™æ¨¡å‹é…ç½®å’Œ Llama ä¸€è‡´

### Qwen-2

- [Qwen2 huggingface](https://huggingface.co/docs/transformers/main/en/model_doc/qwen2) [Qwen1.5-0.5B](https://huggingface.co/Qwen/Qwen1.5-0.5B/tree/main)

  ä¼¼ä¹ Qwen1.5 å’Œ Qwen2 æ˜¯ä¸€å›äº‹

  å‡ ä¹æ²¡æœ‰åœ¨æ¨¡å‹ä¸Šè¿›è¡Œæ›´æ”¹ï¼Œæ”¯æŒäº† sliding window attention & GQA
  
  å…³äº [Sliding Window Attention](https://paperswithcode.com/method/sliding-window-attention) [zhihu](https://zhuanlan.zhihu.com/p/659105978) æ˜¯å¯¹ K,V è¿›è¡Œæ»‘åŠ¨æ³¨æ„åŠ›è®¡ç®—ï¼Œquery æ˜¯å…¨éƒ¨è¾“å…¥

### Prefill & Decode

[LLM Inference Series: 2. The two-phase process behind LLMsâ€™ responses](https://medium.com/@plienhar/llm-inference-series-2-the-two-phase-process-behind-llms-responses-1ff1ff021cd5)

[Mastering LLM Techniques: Inference Optimization](https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/) è¿™ä¸€ç¯‡åšå®¢å‡ ä¹æ¶µç›–äº†ç›®å‰(2023/11/17)å¤§éƒ¨åˆ†çš„æ¨¡å‹æ¨ç†ä¼˜åŒ–ç­–ç•¥

- Prefill is encode phase in GPT

  ä½†æ˜¯ç”±äº GPT æ˜¯ decoder only æ¶æ„ï¼Œæ‰€ä»¥ encode phase ä¹Ÿæ˜¯ç”± decoder å®Œæˆçš„ã€‚æ›´å…·ä½“çš„æ¥è¯´ pre-fill æ‰€æŒ‡çš„ï¼š**å°±æ˜¯ä» prompt è¾“å…¥åˆ° GPT ä¸­ç”Ÿæˆç¬¬ä¸€ä¸ª token çš„è¿‡ç¨‹**
  
  > Generating the first token of the completion by running the tokenized prompt through the network.
  
  è¿™ä¸ªè¿‡ç¨‹ä¹Ÿä¼šå«åš initiation phase
  
  æ ¸å¿ƒï¼šåœ¨å¤„ç† prompt çš„æ—¶å€™ï¼Œä¼šå¤©ç„¶åœ°ç”Ÿæˆç¬¬ä¸€ä¸ª tokenã€‚å› ä¸º GPT åšçš„å°±æ˜¯ next token predictionï¼Œåœ¨è¿›è¡Œå‰å‘è¿ç®—çš„è¿‡ç¨‹ä¸­å°±ä¼šè·å¾—æœ€åä¸€ä¸ª token çš„ next tokenã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼šè¿™ä¸ª token æ²¡æœ‰éšè—çŠ¶æ€çš„ (hidden state) çš„
  
- Decode phase

  decode è¿‡ç¨‹éå¸¸æ¸…æ™°ï¼šä»ç¬¬ä¸€ä¸ª token å¼€å§‹ï¼Œä¸æ–­åœ°é€šè¿‡è‡ªå›å½’ç”Ÿæˆæ–°æ–‡æœ¬ï¼Œç›´åˆ°é‡åˆ°ç»“æŸ token çš„æ•´ä¸ªè¿‡ç¨‹

### KV-Cache

[Transformers KV Caching Explained](https://medium.com/@joaolages/kv-caching-explained-276520203249)

KV-Cache èƒ½å¤Ÿå­˜åœ¨çš„æ ¹æœ¬åŸå› ï¼šGPT æ˜¯ä¸€ä¸ª Causal æ¨¡å‹ï¼Œå³ï¼šå½“å‰ç”Ÿæˆç»“æœä»…å–å†³äºè¿‡å»ï¼Œä¸ä¸æœªæ¥äº¤äº’ã€‚æ›´å…·ä½“çš„æ¥è¯´ attention mask å°†ä»¥ä¸Šä¸‰è§’çš„å½¢å¼å­˜åœ¨ï¼Œæ¶ˆé™¤å¯¹æœªæ¥ token çš„æ³¨æ„åŠ›ã€‚æ‰€ä»¥å¯¹äº Causal æ¨¡å‹ï¼Œæˆ‘ä»¬å°±å¯ä»¥æŠŠè¿‡å»çš„çŠ¶æ€ (key & value) ç»™ä¿å­˜ä¸‹æ¥ï¼Œå› ä¸ºä»–ä»¬æ˜¯ä¸ä¼šæ”¹å˜çš„ï¼Œäºæ˜¯å°±æœ‰äº† KV-Cache è¯ç”Ÿ

æœ‰äº† KV-Cache è¿‡åï¼Œæˆ‘ä»¬å°±ä¸å¿…è¦æ¯ä¸€æ¬¡å°†æ‰€æœ‰çš„ token è¾“å…¥åˆ° GPT å½“ä¸­ï¼Œè€Œæ˜¯åªç”¨å°†æ–° token è¾“å…¥ GPT å½“ä¸­ï¼Œæ–° token å’Œè¿‡å»çš„ KV-Cache è¿›è¡Œæ³¨æ„åŠ›äº¤äº’ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ª tokenï¼Œå¾ªç¯å¾€å¤

Paged KV Cache æ˜¯å¯¹ KV Cache å­˜å‚¨æ–¹å¼çš„æ”¹è¿›ï¼Œæˆ‘ä¸ªäººç†è§£ä¸ºä¸€ç§åŠ¨æ€çš„å­˜å‚¨æ–¹å¼ã€‚KV Cache ä¼šé¢„å…ˆç”³è¯·ä¸€å—ç©ºé—´æ¥å­˜å‚¨ï¼Œä½†è¿™æ ·ä¼šå¯¼è‡´å†…å­˜åˆ©ç”¨ç‡ä¸é«˜ï¼Œè€Œ Paged KV Cache å°±æ˜¯æ¥å¤šå°‘ç”³è¯·å¤šå°‘ï¼Œå°†è¿™äº› KV Cache å­˜å‚¨åœ¨ä¸è¿ç»­çš„åœ°æ–¹ï¼Œåœ¨éœ€è¦ä½¿ç”¨çš„æ—¶å€™é€šè¿‡æŸ¥è¡¨è·å¾— cache åœ°å€

### Forward & generate

å¯¹äº Huggingface Model çš„å®Œæ•´å‰å‘è¿‡ç¨‹ä»¥åŠç”Ÿæˆè¿‡ç¨‹è¿›è¡Œæ•´ç†ï¼Œä» load -> tokenize/template -> forward -> generate

### Mistral

åœ¨æ¨¡å‹ç»“æ„ä¸Šå’Œ llama åŒºåˆ«ä¸å¤§ï¼Œåªæ˜¯ä½¿ç”¨äº† GQA & Sliding window attention è€Œå·² 

## Code Implementation

å› ä¸ºå¤§å®¶éƒ½ä½¿ç”¨ç›¸ä¼¼çš„å®ç°ï¼Œæ‰€ä»¥æ·±å…¥æŒæ¡ä»£ç æ˜¯éå¸¸æœ‰å¿…è¦è€Œä¸”å¾ˆæœ‰æ”¶ç›Šçš„äº‹æƒ…ã€‚ç”±äºå¤§å®¶éƒ½ä½¿ç”¨ huggingfaceï¼Œæ‰€ä»¥é€‰æ‹©ç›´æ¥çœ‹ huggingface transformers ä»£ç ï¼Œä¸€å…±æœ‰ 1500 è¡Œï¼Œå¦‚æœæµ“ç¼©ä¸‹æ¥çš„è¯ï¼Œä¼°è®¡åªæœ‰ä¸€åŠå·¦å³

- LlamaRMSNorm

  éå¸¸å¸¸è§„çš„ pytorch å®ç°
  $$
  \bar{a}_i=\frac{a_i}{\text{RMS} (A)}g_i  \\
  \text{RMS}(A)=\sqrt[]{\frac{1}{n}\sum_{i=1}^na_i^2 + \epsilon} 
  $$

- LlamaRotaryEmbedding

  çœ‹äº†ä¸€ç¯‡å…³äºä½ç½®ç¼–ç çš„è®²è§£ï¼Œä»¥01ä½ç½®ç¼–ç å¼€å§‹ï¼Œæ¨å¯¼åˆ°ç»å¯¹ä½ç½®ç¼–ç ï¼Œæ•´ä¸ªè¿‡ç¨‹éå¸¸æ¸…æ™° [zhihu](https://zhuanlan.zhihu.com/p/352233973)

  takeaway: æœ€é«˜ç»´åº¦çš„å‘¨æœŸå†³å®šäº†æ‰€èƒ½ç¼–ç çš„æœ€å¤§é•¿åº¦
  $$
  \begin{aligned}
  p_{i, 2 j} &=\sin \left(\frac{i}{10000^{2 j / d}}\right) \\
  p_{i, 2 j+1} &=\cos \left(\frac{i}{10000^{2 j / d}}\right)
  \end{aligned}
  $$
  

  é€‰å–åº•æ•°ä¸º 10000ï¼Œåˆ™æœ€å¤§å‘¨æœŸä¸º $2\piÂ·10000$ï¼Œä¹Ÿå°±æ˜¯è¯´æœ€å¤§ç¼–ç é•¿åº¦ä¸º 6w å¤š

  æ—‹è½¬ä½ç½®ç¼–ç ä»ä½ç½®ç¼–ç çš„ç›®çš„å‡ºå‘ï¼Œä»æ•°å­¦åŸç†æ¨å¯¼å‡ºäº†æ›´å¥½çš„ç›¸å¯¹ä½ç½®ç¼–ç å½¢å¼

  å®ç°æ—‹è½¬ä½ç½®ç¼–ç å®é™…ä¸Šæ˜¯å®ç°äº†ä¸€ä¸ª cache, åœ¨å®é™…ä½¿ç”¨çš„æ—¶å€™å–æ‰€éœ€è¦çš„ä½ç½®å³å¯ï¼Œä¸‹é¢æ˜¯ç®€åŒ–çš„ä¼ªä»£ç 

  ```python
  import torch
  
  class RotaryEmbed(nn.Module):
      def __init__(self, dim, max_length, base=10000):
          # inverse of frequency (d // 2,), which is the denominator of fracition
          inv_freq = 1.0 / (self.base ** (torch.arange(dim, step=2) / dim))
          
          # token positions (N,)
          t = torch.arange(max_length)
          
          # outer product, (N, d // 2)
          freqs = torch.outer(t, inv_freq)
          # freqs = torch.einsum(i, j -> i j) is the same
          
          # sin & cos cache, (N, d)
          emb = torch.cat([freqs, freqs], dim=1)
          self.sin = emb.sin()
          self.cos = emb.cos()
          
      def forward(self, x, seq_len):
          # x is not useful, 
          return self.sin[:seq_len], self.cos[:seq_len]
  ```

  ä»£ç é‡Œä½¿ç”¨ `t` æ¥ä»£è¡¨ä½ç½®ï¼Œä»ä¸Šè¿°æè¿°ï¼Œ`t` ä¹Ÿæœ‰å‘¨æœŸçš„å«ä¹‰ï¼Œæ‰€ä»¥ç”¨æ¥ä½œä¸ºæ ‡è¯†ä¹Ÿå¾ˆåˆé€‚

  è·å¾—ä½ç½®ç¼–ç è¿‡åéœ€è¦å°†ä½ç½®ç¼–ç åŠ å…¥åˆ° query å’Œ key é‡Œé¢å»

  ```python
  def rotate_half(x):
      """Rotates half the hidden dims of the input."""
      x1 = x[..., : x.shape[-1] // 2]
      x2 = x[..., x.shape[-1] // 2 :]
      return torch.cat((-x2, x1), dim=-1)
  
  def apply_rotary_pos_emb(q, k, cos, sin, position_ids):
      """
      q: (B, N, H, C)
      k: (B, N, H, C)
      cos: (max_length, C)
      sin: (max_length, C)
      position_ids: (N,)
      """
      cos = cos[position_ids].unsqueeze(1)	# (N, 1, C)
      sin = sin[position_ids].unsqueeze(1)
      q_embed = (q * cos) + (rotate_half(q) * sin)
      k_embed = (k * cos) + (rotate_half(k) * sin)
      return q_embed, k_embed
  ```

  `position_ids` æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„å‚æ•°ï¼Œä»£è¡¨äº†æˆ‘ä»¬éœ€è¦å–å“ªäº›ä½ç½®ã€‚ä¸ºä»€ä¹ˆä¸ç›´æ¥ä» q & k çš„é•¿åº¦ç›´æ¥ç”Ÿæˆ `position_ids`ï¼Ÿå› ä¸º q, k çš„ä½ç½®ä¸ä¸€å®šæ˜¯ä»é›¶å¼€å§‹çš„ï¼Œæœ‰æ—¶å€™æ–°è¾“å…¥çš„ q & v å‰å·²ç»æœ‰äº†å…¶ä»–å†…å®¹ï¼Œè¿™åœ¨å¤šè½®å¯¹è¯ä»¥åŠä½¿ç”¨ kv cache çš„æ—¶å€™å¾ˆæœ‰ç”¨

- `update_causal_mask`

  éœ€è¦åˆ†ä¸ºä¸¤ç§æƒ…å†µï¼šprefill or decodeï¼Œæ¢å¥è¯è¯´æ˜¯å¦æ˜¯ç¬¬ä¸€æ¬¡è¾“å…¥

  ```python
  import torch
  
  def _update_causal_mask(attention_mask, inputs_embeds):
      """
      Args:
      - attention_mask: (B, M), indicating padding situation
      - inputs_embeds: (B, N, C)
          M == N if it's first input (prefill)
          N == 1, if it's decoding, M is all input tokens (past and newly generated)
      Returns:
      - causal_mask: (B, 1, N, M)
      	(B, 1, N, N) if prefill
      	(B, 1, 1, M) if decode
      """
      N = inputs_embeds.shape[1]
      M = attention_mask.shape[1]
      seq_len = N
      target_len = M
      min_value = torch.finfo(torch.float32).min
      
      # fill the minimum value (N, M), M=N if it's first prompt
      causal_mask = torch.full((seq_len, target_len), min_value)
      
      if seq_len != 1:
          # prefill, (N, M=N) causal attend
          causal_mask = torch.triu(calsual_mask, diagonal=1)
      else:
          # decode, (N=1, M) attend all previous tokens
          causal_mask *= 0
          
      causal_mask = rearange('N M -> B 1 N M', causal_mask)
      # process padding
      padding_mask = (attention_mask == 0.0)
      padding_mask = rearange('B M -> B 1 N M', padding_mask)
      causal_mask.masked_fill(padding_mask, min_value)
      
      return causal_mask
  ```

  è¿™é‡Œçš„ causal mask æ— æ³•åº”å¯¹åƒ medusa ä¸€æ ·ï¼Œåœ¨ decode ä¸€æ¬¡è¾“å…¥å¤šä¸ª token çš„æƒ…å†µï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆåœ¨ medusa é‡Œé¢å•ç‹¬å¤„ç†äº† causal mask çš„æƒ…å†µ

- Llama Attention

  attention å·²ç»å†ç†Ÿæ‚‰ä¸è¿‡äº†ï¼Œä¸è¿‡ llama attention ç›¸æ¯”äº vanilla attention è¿˜æœ‰ä¸å°‘é¢å¤–åŠŸèƒ½ï¼š

  1. Group Query Attention

  2. KV Cache

     åœ¨ Llama ä¸­æ˜¯ä½¿ç”¨ä¸€ä¸ª List of Tensor æ¥å®ç°çš„ï¼Œlist ä¸­ç¬¬ i ä¸ªå…ƒç´ ä»£è¡¨ç¬¬ i ä¸ª transformer layer çš„ kv cacheï¼Œå…¶å½¢çŠ¶ä¸º `(B, M', H, C)`ï¼Œå…¶ä¸­ `M'` å³ä»£è¡¨å­˜å‚¨çš„å†å² token æ•°é‡

  3. Causal Mask

  4. Rotary Embedding

  ```python
  def forward(hidden_states, attention_mask, position_ids, past_key_value):
      """
      Args:
      - hidden_states: (B, N, C) or (B, N=1, C)
      	N=1 means when decoding, only 1 new token is given
      - attention_mask: (B, 1, N, N) or (B, 1, N=1, M)
      	M is the number of all input tokens: past + N=1
      	This is the causal_mask introduced above
      - position_ids: (N,) or (1,)
      - past_key_value: KV Cache
      """
      # get q k v
      q = self.q_proj(hidden_states)	# (B, N, head_dim * num_head)
      k = self.k_proj(hidden_states)	# (B, N, head_dim * kv_head)
      v = self.v_proj(hidden_states)	# (B, N, head_dim * kv_head)
      
      q = rearange(q, 'B N (H C) -> B H N C', H=num_head)
      k = rearange(k, 'B N (H C) -> B H N C', H=kv_head)
      v = rearange(v, 'B N (H C) -> B H N C', H=kv_head)
      
      # get q k position embedding
      cos, sin = self.rotary_emb(position_ids)
      # apply pos emb
      q, k = apply_rotary_pos_emb(q, k, cos, sin, position_ids)
      
      # repeat kv if kv_head < num_head
      groups = num_head // kv_head
      k = repeat(k, 'B H N C -> B (H G) N C', G=groups)
      v = repeat(v, 'B H N C -> B (H G) N C', G=groups)
      
      # update current kv to cache, return cated prev + current kv
      # now KV shape is (B H M C), M = M' + N
      K, V = past_key_value.update(k, v, self.layer_idx)
      
      # scale dot product attention, (B H N C)
      attn_output = F.scale_dot_product_attention(
      	q, K, V,
          attention_mask,
          self.attn_drop # 0.
      )
      
      # view back
      attn_output = rearange(attn_output, 'B H N C -> B N (H C)', H=num_head)
      
      # out project
      attn_output = self.out_proj(attn_output)
      
      return attn_output, past_key_value
      
  ```

- LlamaDecoderLayer

  ç”Ÿä¸‹æ¥å°±æ˜¯å°†ç»„å»ºä¸²æ¥èµ·æ¥å°±è¡Œ

  ```python
  def forward(hidden_states, attention_mask, position_ids, past_key_value):
      residual = hidden_states
      
      # pre-norm
      hidden_states = self.norm1(hidden_states)
      
      # attention
      hidden_states, past_key_value = self.self_attn(
      	hidden_states = hidden_states,
          attention_mask = attention_mask,
          position_ids = position_ids,
          past_key_value = past_key_value
      )
      
      # residual
      hidden_states = residual + hidden_states
      
      # norm + FFN + add
      residual = hidden_states
      hidden_states = self.norm2(hidden_states)
      hidden_states = self.mlp(hidden_states)
      hidden_states = hidden_states + residual
      
      return hidden_states, past_key_value
  ```

- å†ä¹‹åå°±æ˜¯å †å  decode layerï¼Œå¹¶ä¸”è¿›è¡Œä¸€äº›å‰åå¤„ç†ï¼Œè¿™å°±æ˜¯ `LlamaModel` çš„å®ç° 

  ```python
  def forward(input_ids, attention_mask, position_ids, past_key_values, labels):
      # embed token
      input_embeds = self.embed_tokens(input_ids)
      
      # init past_key_value
      past_key_values = DynamicCache.from_legacy_cache(past_key_value)
      
      # create causal_mask according to attention_mask & number of input tokens
      causal_mask = self._update_causal_mask(attention_mask, inputs_embeds)
      
      # decode
      hidden_states = input_embeds
      for decode_layer in self.layers:
          hidden_states, past_key_value = decode_layer(
          		hidden_states,
              	attention_mask = causal_mask,
              	position_ids = position_ids,
              	past_key_value = past_key_value,
          )
          
      return BaseModelOutputWithPast(
      	last_hidden_state = hidden_states,
          past_key_value = past_key_value
      )
  ```

- æœ€åå°±æ˜¯ç®€å•çš„å°† LlamaModel å’Œ `lm_head` è¿›è¡Œç»„åˆï¼Œå®Œæˆæ•´ä¸ªé¢„æµ‹è¿‡ç¨‹ã€‚å¦‚æœä¼ å…¥æ ‡ç­¾çš„è¯ï¼Œè¿˜è¿›è¡Œä¸€ä¸‹ loss è®¡ç®—

  ```python
  def forward(input_ids, attention_mask, position_ids, past_key_value, labels=None):
      outputs = self.model(input_ids,
                          attention_mask,
                          position_ids, 
                          past_key_value)
      
      # logits, (B, N, C)
      logits = self.lm_head(outputs.hidden_states)
      
  	# loss
      if labels:
          shift_logits = logits[:, :-1, :] # (B, N - 1, C)
          shift_labels = labels[:, 1:]	# (B, N - 1)
          loss = CrossEntropy(shift_logits, shift_labels)
          
  	return CausalLMOutpuWithPast(
      	loss=loss,
          logits=logits,
          past_key_value=outputs.past_key_value,
          hidden_states=outputs.last_hidden_state
      )
  ```

  åˆå§‹çš„ `attention_mask` å…¶å®æ˜¯ç”± tokenizer æä¾›çš„ï¼Œä»¥åŠ `position_ids` çš„ç”Ÿæˆè¿‡ç¨‹æ˜¯åœ¨ `model.generate` é‡Œé¢è¿›è¡Œ

- ModelOutput

  å®é™…ä¸Šæ˜¯ä¸€ä¸ª dataclass ä½†åŒæ—¶èƒ½å¤Ÿä½¿ç”¨å­—å…¸çš„å…³é”®å­—è·å¾—å…¶ä¸­çš„æ•°æ®

## Question

- Swish ä½¿ç”¨äº† $xÂ·\sigma(x)$ï¼Œè¿™æ ·çš„è¯é‡çº²å°±å˜äº†å•Šï¼ŒGLU é—¨æ§å•å…ƒå‰åçš„é‡çº²æ²¡æœ‰å˜åŒ–ï¼Œä½†æ˜¯ SwiGLU é‡çº²å˜æˆäº† $x^2$â€‹â€‹ï¼Œçœ‹æ¥æˆ‘å¯¹ ReLU or æ¿€æ´»å€¼çš„ç†è§£ä¸å¤Ÿæ­£ç¡®

  > From GPT4: Using ReLU or Swish in a gating unit might initially seem counterintuitive because they lack the inherent [0, 1] gating range of the sigmoid. However, if the learning process can benefit from the unbounded activation values or from the different slopes in positive and negative regions (as is the case for Swish), these functions can still be effective. Essentially, they provide new mechanisms to control the information flow that can lead to better model performance in some tasks.
  >
  > Moreover, the potential issue of having unbounded values can sometimes be mitigated by learned model parameters (in the affine transformations) or subsequent layers that can scale these activations appropriately.

- æ—‹è½¬ä½ç½®ç¼–ç çš„æ•°å­¦åŸç†ä»¥åŠå¦‚ä½•è¿›è¡Œå¤–æ¨ï¼Œè¿™éƒ¨åˆ†å¯èƒ½å…ˆå»æŠŠæ•°å­¦åŸºç¡€å¼„æ‰å®ï¼Œç„¶åå†æ¥çœ‹ä¼šæœ‰æ›´å¥½çš„æ•ˆæœã€‚è¿™æ˜¯ä¸€ä¸ªé•¿æœŸé¡¹ç›®

- scale dot product attention ç›¸æ¯” vanilla attention æ¥è¯´æ˜¯æ›´åŠ  memory efficentï¼Œå¦å¤– flash attention æ›´éœ€è¦ä¸€äº›æ”¹åŠ¨ï¼Œå­¦ä¹ ä»–ä»¬çš„ç”¨æ³•å’ŒåŒºåˆ«å¯èƒ½éœ€è¦åŠå¤©æ—¶é—´

  ä½†æ˜¯ vanilla attention ä¸ scale dot product attention ä¹‹é—´åŒºåˆ«ä¸å¤§ï¼Œmask ä¹Ÿæ˜¯ä¸€è‡´çš„
