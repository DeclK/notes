## EAGLE Speculative Decoding From Zero to Hero

EAGLE æŠ•æœºé‡‡æ ·ä½œä¸ºç›®å‰æœ€å¼ºçš„æŠ•æœºé‡‡æ ·æ–¹æ³•ï¼Œå€¼å¾—æ•´ç†ä¸€ç•ªã€‚æˆ‘å°†ä»æŠ•æœºé‡‡æ ·åŸºç¡€å¼€å§‹ï¼Œé€æ­¥åœ°è®²è§£ EAGLE æŠ•æœºé‡‡æ ·çš„æ”¹è¿›è¿‡ç¨‹ï¼Œå¹¶é…åˆä»¥å®é™…ä»£ç å¸®åŠ©ç†è§£ã€‚åœ¨æœ¬æ–‡ä¸­å°†ä»¥ SD ä»£è¡¨ Speculative Decoding çš„ç¼©å†™

Reference [github](https://github.com/SafeAILab/EAGLE)

## æŠ•æœºé‡‡æ ·åŸºç¡€

### LLM decode & the challenge

LLM åœ¨ decode é˜¶æ®µé‡‡ç”¨çš„æ˜¯è‡ªå›å½’å¼ decodeï¼Œè¿™æ ·çš„æ–¹å¼æ¯æ¬¡ input tokens æ•°ç›®åªèƒ½ä¸º1ï¼Œç„¶åè·å¾— next tokenï¼Œä»¥æ­¤å¾ªç¯å¾€å¤ç›´è‡³é‡åˆ° end of text tokenã€‚ç”¨ç®€æ´çš„ä¼ªä»£ç å¯ä»¥è¡¨ç¤ºä¸ºå¦‚ä¸‹

```python
# decode_token is a single integer (1,)
while True:
    # llm forward
    hidden_states = transformer(decode_token_id)
    logits = lm_head(hidden_states)
    
    # sample next token
    next_token_id = sampler(logits)
    
    # check end
    if next_token == end_of_text_token:
        break
    
    # continue
    decode_token_id = next_token_id
```

è¿™æ ·çš„æ–¹å¼æœ‰ä¸€ä¸ªæ˜æ˜¾çš„é—®é¢˜ï¼šæ•´ä¸ªè¿‡ç¨‹è®¡ç®—å¼ºåº¦ä½ï¼Œæˆä¸º memory bound åœºæ™¯ã€‚ä¸€ä¸ªæ›´å½¢è±¡çš„ä¾‹å­æ¥è¯´æ˜ï¼šinput tokens = 8 å’Œ input tokens = 1 çš„ decode æ—¶é—´æ˜¯éå¸¸æ¥è¿‘çš„ï¼Œå› ä¸ºæ­¤æ—¶è®¡ç®—åŸºæœ¬ä¸Šä¸è€—æ—¶ï¼Œè€—æ—¶çš„æ˜¯è¯»å–/å†™å…¥æƒé‡å’Œæ¿€æ´»å€¼

### Core of SD

æŠ•æœºé‡‡æ ·çš„æ ¸å¿ƒæ€æƒ³ï¼Œå°±æ˜¯ç”¨ä¸€ä¸ª draft model å»çŒœæµ‹ï¼ˆspeculateï¼‰åé¢çš„ tokens æ˜¯ä»€ä¹ˆï¼ŒçŒœå®Œè¿‡åç”¨åŸæ¨¡å‹ï¼ˆæœ¬æ–‡æœ‰æ—¶å€™ä¹Ÿä¼šç§°åŸæ¨¡å‹ä¸º base modelï¼‰å»è¿›è¡ŒéªŒè¯ï¼Œå¦‚æœçŒœå¯¹äº†ï¼Œé‚£å°±æ­£å¥½æ¥å—ï¼›å¦‚æœçŒœé”™äº†ï¼Œå°±æ‰”æ‰çŒœé”™çš„ tokenã€‚åŠ é€Ÿæ•ˆæœæ¥è‡ªäºå¹¶è¡Œè®¡ç®—äº†å¤šä¸ª input tokensï¼ŒçŒœå¯¹å¾—è¶Šå¤šï¼ŒåŠ é€Ÿè¶Šæ˜æ˜¾ã€‚å…·ä½“æ¥è¯´ï¼š**æŠ•æœºé‡‡æ ·å°‘é‡çš„ draft model çŒœæµ‹æ—¶é—´ + ç”¨ä¸€æ¬¡ decode çš„æ—¶é—´ç”¨äºéªŒè¯ï¼Œè¾“å‡ºäº†å¤šä¸ªæ­£ç¡®çš„ next tokensï¼Œæ‰€ä»¥èŠ‚çœäº†å¤šæ¬¡ decode çš„æ—¶é—´ï¼Œè·å¾—äº†åŠ é€Ÿæ•ˆæœ**

ä¸‹é¢æˆ‘å°†ç”¨ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ï¼Œæ¥å±•ç¤ºæ•´ä¸ªæŠ•æœºé‡‡æ ·çš„è¿‡ç¨‹ã€‚å‡è®¾ prompt ä¸ºä¸‰ä¸ªå•è¯ï¼š`How can I`ï¼Œå¹¶ä¸”å‡è®¾ LLM æœ€ç»ˆçš„ decode è¾“å‡ºä¸º `learn eagle speculative decoding well?`ï¼Œæˆ‘ç”¨ä»¥ä¸‹å›¾ç¤ºæ¥è¡¨ç¤ºä¸€èˆ¬çš„ LLM decoding è¿‡ç¨‹

<img src="EAGLE Speculative Decoding/image-20250328225651677.png" alt="image-20250328225651677" style="zoom:80%;" />

**æˆ‘ä»¬å¯ä»¥ç®€å•åœ°æŠŠ LLM (i.e. Base Model) çœ‹ä½œæ˜¯ä¸€ä¸ª next token prediction machine**ï¼Œä¹Ÿå°±æ˜¯è¯´ä½ ç»™è¿›å»ä»»ä½• tokenï¼Œå®ƒéƒ½ä¼šåŸºäºå†å²çŠ¶æ€æ¥é¢„æµ‹ä¸‹ä¸€ä¸ª token æ˜¯ä»€ä¹ˆã€‚è¿™ä¸ªæƒ³æ³•å°†ä¼šç®€åŒ– LLM æ¨¡å‹ï¼Œå¸®åŠ©æˆ‘ä»¬ç†è§£æ•´ä¸ª speculative decoding è¿‡ç¨‹ã€‚ç®€è¦æè¿°ä¸‹ä¸Šå›¾çš„è¿‡ç¨‹ï¼šåœ¨ prefill ä¸­è¾“å…¥ `How can I` é¢„æµ‹å‡ºäº† 3 ä¸ª next tokensï¼Œä½†æ˜¯æˆ‘ä»¬åªå…³æ³¨æœ€åä¸€ä¸ªï¼Œå³ç”± `I` é¢„æµ‹å¾—åˆ°çš„ next token `learn`ã€‚ç„¶åè¿›å…¥ decode é˜¶æ®µï¼Œæˆ‘ä»¬ä½¿ç”¨ `learn` å»é¢„æµ‹å¾—åˆ°ä¸‹ä¸€ä¸ª token `eagle`ï¼Œç”¨ `eagle` å»é¢„æµ‹å¾—åˆ° `speculative`ï¼Œ...ï¼Œå¦‚æ­¤å¾ªç¯ä¸‹å»è·å¾—æœ€ç»ˆå®Œæ•´çš„å¥å­ `How can I learn eagle speculative decoding well?`

OKï¼Œç°åœ¨æ¥çœ‹çœ‹ç”¨æŠ•æœºé‡‡æ ·æ•´ä¸ªè¿‡ç¨‹å¯èƒ½æ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿè¿™é‡Œå°±å¼€å§‹å¼•å…¥ draft model äº†ï¼Œè¯¥ draft model ä¹Ÿæ˜¯ä¸€ä¸ª LLMï¼Œåªä¸è¿‡æ¯”åŸæ¥çš„ baes model è¦å°å¾ˆå¤šï¼Œ**ä½†æ˜¯ä¸å¦¨ç¢å…¶æœ¬è´¨æ˜¯ä¸€ä¸ª next token prediction machine**

<img src="EAGLE Speculative Decoding/image-20250328231636101.png" alt="image-20250328231636101" style="zoom:80%;" />

Draft model æ‹¿åˆ°ä¸€ä¸ªåˆå§‹çš„ token `learn`ï¼Œå¼€å§‹äº†è‡ªå·±çš„è‡ªå›å½’è¿‡ç¨‹ï¼Œè·å¾—äº†è®¸å¤š draft tokens `eagle speculative decoding better? ...`ã€‚è¿™äº› draft tokens å°±æ˜¯ draft model å»çŒœæµ‹ base model æ¥ä¸‹æ¥ä¼šç”Ÿæˆçš„è¯ã€‚é—®é¢˜æ¥äº†ï¼Œæ€ä¹ˆçŸ¥é“ draft model çŒœå¾—å¯¹ä¸å¯¹å‘¢ï¼Ÿè¿™å°±éœ€è¦ä¸€ä¸ªéªŒè¯è¿‡ç¨‹ã€‚éªŒè¯è¿‡ç¨‹çš„ç¬¬ä¸€æ­¥å°±æ˜¯å°†è¿™äº› draft tokens è¿åŒ initial token **ä¸€èµ·**è¾“å…¥åˆ° base model ä¸­ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º

<img src="EAGLE Speculative Decoding/image-20250329002507107.png" alt="image-20250329002507107" style="zoom:80%;" />

å½“è¿™äº› tokens è¾“å…¥åˆ° base model è¿‡åï¼Œæ¯ä¸€ä¸ª token éƒ½ä¼šäº§ç”Ÿè‡ªå·±å¯¹åº”çš„ next tokenï¼ˆå†æ¬¡å¼ºè°ƒï¼Œbase model çš„æœ¬è´¨æ˜¯ä¸€ä¸ª next token prediction machineï¼‰ã€‚æ­¤å¤–ä½ å¯ä»¥æ³¨æ„åˆ°è¿™ä¸ªå›¾ä¸­æ²¡æœ‰è™šçº¿è¿›è¡Œè¿æ¥ï¼Œ**è¿™è¡¨ç¤ºè¯¥è¿‡ç¨‹å¹¶éè‡ªå›å½’çš„ï¼Œè€Œæ˜¯å¹¶è¡Œçš„**ã€‚ç°åœ¨æˆ‘ä»¬æœ‰äº† base model é¢„æµ‹çš„ next tokenï¼Œæˆ‘ä»¬å°±å¯ä»¥å’Œç”± draft model æ‰€äº§ç”Ÿçš„ draft tokens è¿›è¡Œå¯¹æ¯”ï¼Œçœ‹ä¸‹æ˜¯å¦çŒœå¯¹äº†

<img src="EAGLE Speculative Decoding/image-20250329002637632.png" alt="image-20250329002637632" style="zoom:80%;" />

å¯ä»¥çœ‹åˆ°ï¼Œ**ç»¿è‰²çš„çº¿æ¡ä»£è¡¨æˆ‘ä»¬çš„ draft tokens å’ŒçœŸå®çš„ base model æ‰€é¢„æµ‹çš„ next tokens æ˜¯ä¸€æ ·çš„ï¼Œçº¢è‰²çº¿æ¡å°±ä»£è¡¨äºŒè€…å¹¶ä¸åŒ¹é…**ã€‚æ‰€ä»¥è¯´åœ¨è¿™ä¸ª case å½“ä¸­ draft model çŒœå¯¹äº†ä¸‰ä¸ªè¯ `eagle speculative decoding`ã€‚ä½†æ˜¯æœ€åä¸€ä¸ªè¯ `well?`æ²¡æœ‰çŒœå¯¹ï¼ŒçŒœçš„æ˜¯ `better?`ï¼Œå¹¶ä¸”ä¸€æ—¦ä¸€ä¸ªè¯çŒœé”™è¿‡åï¼Œåé¢çš„æ‰€æœ‰ draft tokens æˆ‘ä»¬éƒ½è®¤ä¸ºæ˜¯é”™è¯¯çš„ï¼Œæ‰€ä»¥ç›´æ¥å¿½ç•¥æ‰

è™½ç„¶æœ€åä¸€ä¸ªè¯æ²¡æœ‰çŒœå¯¹ï¼Œä½†æ˜¯ç”±äº `decoding` è¿™ä¸ªè¯æ˜¯çŒœå¯¹çš„ï¼Œé‚£ä¹ˆç”± base model é¢„æµ‹å¾—åˆ°çš„ `well?` å°±æ˜¯æ­£ç¡®çš„ next tokenã€‚æ­£æ˜¯è¿™ä¸ªæ€§è´¨ï¼Œä¿è¯äº†æŠ•æœºé‡‡æ ·çš„ä¸‹é™å°±æ˜¯ï¼š**ä¸€å®šè·å¾—ä¸€ä¸ªæ­£ç¡®çš„ next token**ã€‚è¿™ç§æƒ…å†µæˆ‘ç”¨ä¸‹å›¾è¡¨ç¤º

<img src="EAGLE Speculative Decoding/image-20250329002706795.png" alt="image-20250329002706795" style="zoom:67%;" />

å¯ä»¥çœ‹åˆ°ï¼Œå³ä½¿ draft tokens ç”Ÿæˆçš„æ˜¯ garbage å¯¼è‡´ä¸€ä¸ªéƒ½å¯¹ä¸ä¸Šã€‚ä½†ç”±äº `learn` è¿™ä¸ªè¯æ˜¯ initial tokenï¼Œç”± base model æ­£å¸¸ç”Ÿæˆï¼Œæ‰€ä»¥ `eagle` è¿™ä¸ªè¯ä¸€å®šæ˜¯æ­£ç¡®çš„ next token

æ³¨æ„ï¼šåœ¨å®Œæˆäº† verify è¿‡åï¼Œæˆ‘ä»¬éœ€è¦å°†æ­£ç¡®çš„ tokens è¿›è¡Œæ¥æ”¶ï¼Œé”™è¯¯çš„ tokens è¿›è¡Œåˆ é™¤ã€‚è¿™é‡Œä¸»è¦æ˜¯å¯¹ KV Cache è¿›è¡Œæ“ä½œï¼Œå°†é”™è¯¯çš„ KV Cache è¿›è¡Œåˆ é™¤ï¼Œä¿ç•™æ­£ç¡® tokens çš„ KV Cache

æœ€åï¼Œç”Ÿæˆè¿˜æ²¡æœ‰ç»“æŸï¼Œå› ä¸ºæˆ‘ä»¬ç°åœ¨è·å¾—çš„æœ€æ–°çš„ next token æ˜¯ `well?`ï¼Œè¿˜æ²¡æœ‰é‡åˆ° `<endoftext>`ï¼Œæ‰€ä»¥ç»§ç»­è¿›è¡Œä¸‹ä¸€è½®çš„æŠ•æœºé‡‡æ ·ï¼š

1. å°† initial token (`well?` in this case) è¾“å…¥åˆ° draft model å½“ä¸­ï¼Œç”Ÿæˆ draft tokens
2. å°† initial token & draft tokens è¾“å…¥åˆ° base model ä¸­ï¼Œç”Ÿæˆ base model é¢„æµ‹çš„ next token
3. å°† base model é¢„æµ‹çš„ next token å’Œ draft tokens è¿›è¡Œå¯¹æ¯”éªŒè¯
4. å¦‚æœæ²¡æœ‰å‡ºç° `<endoftext>` å›åˆ° Step1

<img src="EAGLE Speculative Decoding/image-20250328235339912.png" alt="image-20250328235339912" style="zoom:80%;" />

### Extend to sampling situations

åœ¨ä¸Šé¢çš„è®¨è®ºä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨äº† base model çš„å‰å‘è¾“å‡ºå»éªŒè¯ draft tokens çš„æ­£ç¡®æ€§ï¼Œå®Œæˆè¿™ä¸€æ“ä½œæœ‰ä¸€ä¸ªéšè—æ¡ä»¶ï¼šbase model & draft model åœ¨é¢„æµ‹ next token çš„æ—¶å€™æ˜¯ç¡®å®šæ€§çš„ï¼Œç”¨ä¸“ä¸šæœ¯è¯­æ¥è¯´å°±æ˜¯ï¼štemperature ä¸º 0ã€‚å¦‚æœè¿™ä¸ªæ¡ä»¶æ— æ³•æ»¡è¶³ï¼Œé‚£ä¹ˆä¸Šè¿°çš„é”™ä½å¯¹æ¯”æ˜¯æ²¡æœ‰æ„ä¹‰çš„ï¼ˆæ— æ³•å¯¹æ¯”ä¸ç¡®å®šçš„ä¸œè¥¿ï¼‰ã€‚`temperature = 0` åœ¨å…·ä½“å®ç°ä¸­å°±æ˜¯ç›´æ¥ä½¿ç”¨ `argmax(probability)` æ¥å®Œæˆå¯¹ token çš„é€‰å–

åœ¨ `temperature != 0` çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°±éœ€è¦é€šè¿‡é‡‡æ ·æ¥è·å¾— next tokenã€‚æ­¤æ—¶ draft token & base model è¾“å‡ºå˜ä¸ºä¸ç¡®å®šæ€§çš„ï¼Œä½†æˆ‘ä»¬ä»ç„¶å¯ä»¥éªŒè¯æ‰€ç”Ÿæˆçš„ draft token åˆ†å¸ƒæ˜¯å¦ç¬¦åˆ base model åº”è¯¥ç”Ÿæˆçš„ token åˆ†å¸ƒã€‚æ¥ä¸‹æ¥å°±éœ€è¦åšä¸‹æ¦‚ç‡è®ºäº†ğŸ¤”

å®šä¹‰ï¼šdraft token é¢„æµ‹ next token `x` çš„åˆ†å¸ƒä¸º `q(x)`ï¼Œè€Œ base model é¢„æµ‹ next token `x` çš„åˆ†å¸ƒä¸º `p(x)`

ç›®æ ‡ï¼šä»åˆ†å¸ƒ `q(x)` å‡ºå‘ï¼Œæœ€ç»ˆè·å¾—åˆ†å¸ƒ `p(x)`

ç®—æ³•ï¼šä¸‹å›¾æ¥è‡ªäº EAGLE-3 paperï¼Œæè¿°äº†å¤š token çš„æŠ•æœºé‡‡æ ·ç®—æ³•ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šå…ˆä» `q(x)` åˆ†å¸ƒä¸­é‡‡æ ·ï¼Œä»¥ä¸€å®šæ¦‚ç‡å»æ¥æ”¶é‡‡æ ·åˆ°çš„ `x`ï¼Œå¦‚æœ `x` è¢«æ‹’ç»ï¼Œåˆ™åœ¨ä¸€ä¸ªæ–°åˆ†å¸ƒé‡æ–°é‡‡æ ·ä¸€ä¸ª `x`ã€‚è¯¥è¿‡ç¨‹é‡‡æ ·è·å¾—çš„ `x` åœ¨æ•°å­¦ä¸Šç­‰ä»·äºç›´æ¥ä» `p(x)` ä¸­ç›´æ¥é‡‡æ ·

<img src="EAGLE Speculative Decoding/image-20250330223102475.png" alt="image-20250330223102475"  />

æˆ‘å°†ä¸Šå›¾çš„ç®—æ³•æŠ½è±¡ä¸º python ä¼ªä»£ç ï¼Œå¹¶åªè€ƒè™‘ single round

```python
def speculative_sampling(p, q):
    x = sample_from_distribution(q)
    ratio = p(x) / q(x)

    r = uniform(0, 1)
    if r < ratio: # accept the x according to ratio
        return x
    if r >= ratio: # reject x, resample from a new distribution
        new_p = lambda x: (p(x) - min(p(x), q(x))) / sum([p(x) - min(p(x), q(x)) for x in sample_space])
        new_x = sample_from_distribution(new_p)
        return new_x
```

è¯¥é‡‡æ ·è¿‡ç¨‹çš„æ­£ç¡®æ€§åœ¨ä¸‹é¢çš„ section ç»™å‡ºã€‚ç°åœ¨æˆ‘ä»¬é‡æ–°æ¥å®¡è§†æŠ•æœºé‡‡æ ·å’Œä¹‹å‰çš„ verify è¿‡ç¨‹ï¼šå·¦ä¾§å³ä¸ºå½“å‰è®¨è®ºçš„æŠ•æœºé‡‡æ ·ï¼Œè€Œå³ä¾§å³ä¸ºç®€å•çš„ verify è¿‡ç¨‹

<img src="EAGLE Speculative Decoding/image-20250330232028249.png" alt="image-20250330232028249" style="zoom:80%;" />

å¯ä»¥çœ‹åˆ°å·¦ä¾§ï¼Œ base model æ²¡æœ‰ç”Ÿæˆ tokenï¼Œè€Œæ˜¯ç”Ÿæˆçš„å¯¹åº”çš„æ¦‚ç‡åˆ†å¸ƒï¼Œæˆ‘ä»¬å°†åˆ©ç”¨è¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒ `p(x)`ï¼Œæ¥å’Œå¯¹åº”çš„ draft token åˆ†å¸ƒ `q(x)` è¿›è¡ŒæŠ•æœºé‡‡æ ·ã€‚ä¹‹å‰çš„ verify è¿‡ç¨‹ï¼Œå˜ä¸ºäº†ç°åœ¨çš„æ˜¯å¦æ¥æ”¶é‡‡æ ·ç»“æœï¼š

1. è‹¥æ¥æ”¶å½“å‰ draft tokenï¼Œåˆ™ç»§ç»­éªŒè¯ä¸‹ä¸€ä¸ª draft token
2. è‹¥æ‹’ç»å½“å‰ draft tokenï¼Œåˆ™ç”¨æ–°åˆ†å¸ƒé‡æ–°é‡‡æ ·ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„ tokenï¼Œåœ¨æ­¤ä¹‹åæ‰€æœ‰çš„ draft token å…¨éƒ¨èˆå¼ƒ

### Proof the correctness of SD

æŒ‰ç…§ä¸Šè¿°æ–¹æ³•é‡‡æ ·äº§ç”Ÿçš„ `x` åœ¨æ•°å­¦ä¸Šæ˜¯ç­‰ä»·äº `p(x)` çš„ã€‚è¯æ˜æ¥è‡ªæŠ•æœºé‡‡æ ·è®ºæ–‡ [Fast Inference from Transformers via Speculative Decoding](https://openreview.net/pdf?id=C9NEblP8vS)

<img src="EAGLE Speculative Decoding/image-20250330224150492.png" alt="image-20250330224150492" style="zoom:80%;" />

ä¸Šè¿°è¯æ˜ä¸­ï¼Œæ²¡æœ‰æåˆ° $\beta$ çš„å®šä¹‰ï¼Œåœ¨è®ºæ–‡ä¸­å®šä¹‰ä¸ºï¼šé‡‡æ ·ç»“æœè¢«æ¥æ”¶çš„æ¦‚ç‡ã€‚é‡‡æ ·ç»“æœè¢«æ¥æ”¶çš„æ¦‚ç‡æ˜¯ä¸€ä¸ªæœŸæœ›å€¼ï¼Œå¦‚ä¸‹

<img src="EAGLE Speculative Decoding/image-20250330224536397.png" alt="image-20250330224536397" style="zoom: 80%;" />

æˆ‘å†ç¿»è¯‘ä¸€ä¸‹è¿™ä¸ªæœŸæœ›ï¼š

1. å½“ `q(x) <= p(x)` æ—¶ï¼Œé‡‡æ ·ç»“æœä¸€å®šä¼šè¢«æ¥æ”¶ï¼Œæ¦‚ç‡ä¸º 1
2. å½“ `q(x) > p(x)` æ—¶ï¼Œé‡‡æ ·ç»“æœä»¥æ¦‚ç‡ `p(x) / q(x)` æ¥æ”¶

## EAGLE-1 Chain

ç¬¬ä¸€ç« èŠ‚çš„æŠ•æœºé‡‡æ ·åŸç†æ˜¯â€œç¥â€ï¼Œè€Œå…¶ä»–çš„æ–¹æ³•éƒ½æ˜¯â€œå½¢â€ã€‚å¯¹äº EAGLE æ¥è¯´ï¼Œå…¶ç‰¹è‰²å°±æ˜¯åœ¨ draft tokens ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼ŒåŠ å…¥äº† `hidden_states` ä½œä¸ºé¢å¤–çš„ä¿¡æ¯ï¼Œæ¥å¸®åŠ© draft model æ›´å¥½çŒœæµ‹ã€‚

<img src="EAGLE Speculative Decoding/image-20250401223536273.png" alt="image-20250401223536273" style="zoom:80%;" />

åœ¨ä¸Šå›¾ä¸­ä½¿ç”¨æ·±è‰²æ–¹å—æ¥è¡¨ç¤º hidden statesï¼Œå¹¶ä¸”ç”¨ä¸åŒé¢œè‰²æ ‡è®°è¯¥ hidden states æ˜¯æ¥è‡ªäº base model è¿˜æ˜¯ draft modelã€‚å›¾ç¤ºä»…æè¿°äº†ç¬¬ä¸€æ¬¡ eagle decode åšäº†å“ªäº›äº‹æƒ…ï¼š

1. ä½¿ç”¨ input tokens & hidden states å®Œæˆ EAGLE prefillã€‚å¯ä»¥çœ‹åˆ° input tokens å’Œ hidden states åœ¨ EAGLE prefill æ—¶æ˜¯æœ‰ä¸€ä¸ªä½ç§»çš„ï¼Œæ‰€ä»¥ç¬¬ä¸€ä¸ª token æ— æ³•è¿›è¡Œ EAGLE prefillï¼Œå› ä¸ºç¼ºå°‘å¯¹åº”çš„ hidden states
2. ç”¨ base model prefill æ‰€äº§ç”Ÿçš„ç¬¬ä¸€ä¸ª initial token ä»¥åŠå¯¹åº”çš„ hidden states æ¥å¼€å¯ EAGLE decode

æˆ‘ç”¨ç®€å•çš„ä¼ªä»£ç è¡¨ç¤ºå¦‚ä¸‹ï¼š

```python
####################################################### 
# Models would not show in input params
# - embed: token embedding map
# - fc: a linear to fuse embedding & hidden_states (2C, C)
# - draft_model_transformer
# - base_model_transformer
# - lm_head
#######################################################

def eagle_prefill(input_tokens, 
                  input_hidden_states):
    """
    Args:
            - input_tokens: (N,) prompt
            - input_hidden_states: (N,) hidden states of input tokens
    Returns:
            - None, changes happen in the KV Cache
    """
    
    input_feat = fc(
        torch.concat(embed(input_tokens), 
                     input_hidden_states)
    )        # (N, C)
    
    # prefill of eagle
    draft_model_transformer(input_feat)
    

def eagle_decode(init_token,
                 last_hidden_states,
                 draft_len = 8):
    """
    Args:
            - init_token: initial token, (1,)
            - last_hidden_states: the hidden_states which produce the initial token (1, C)
    Return:
            - draft_tokens, (draft_len,)
    """
    next_token = init_token
    draft_tokens = [init_token]
    
    for i in range(draft_len):
        # get input feat
        input_embed = embed(next_token)        # (1, C)
        input_feat = fc(torch.concat([input_embed, last_hidden_states], dim=-1)) # (1, C)
        
        # get hidden states
        last_hidden_states = draft_model_transformer(input_feat)
        
        # get logits
        logits = lm_head(last_hidden_states) # (1, vocab_size)
        
        # sample
        next_token = sample(logits)
        
        draft_tokens.append(next_token)
        
    # pop out the last token so every draft token would have kv cache in draft model
    draft_tokens.pop()
        
    return draft_tokens
```

ä¹‹åå°±æ˜¯ verify draft tokensï¼Œå¹¶åˆ é™¤ base model ä¸­é”™è¯¯ token çš„ kv cacheï¼Œè€Œå¯¹äº draft model kv cache è€Œè¨€æœ‰ä¸¤ç§é€‰æ‹©ï¼š

1. å’Œ base model ä¸€æ ·ï¼Œåˆ é™¤é”™è¯¯çš„ kv cache å³å¯
2. æ¸…é™¤æ‰æœ¬è½® decode é™¤äº†ç¬¬ä¸€ä¸ª token ä»¥å¤–çš„æ‰€æœ‰ kv cacheï¼Œç„¶åé‡æ–°ç”¨æ­£ç¡® token çš„ base model hidden states ç”Ÿæˆæ–°çš„ draft model kv cache

é€‰æ‹©2åœ¨ EAGLE ä¸­è¢«ç§°ä¸º **stable kv**ï¼Œå…¶ç¡®ä¿åœ¨ decode ä¹‹å‰ï¼Œdraft model ä¸­çš„ kv cache å…¨éƒ¨ç”± base model hidden states ï¼ˆæ·±è“è‰²æ–¹æ¡†ï¼‰ç”Ÿæˆã€‚è€Œé€‰æ‹©1åˆ™ä¼šä¿ç•™ç”± draft model hidden states ï¼ˆæ·±ç»¿è‰²æ–¹æ¡†ï¼‰äº§ç”Ÿçš„ kv cacheï¼Œè¿™å°±é€ æˆäº† kv cache æ¥æºçš„æ··åˆï¼ˆæˆ‘ç§°ä¸º unstable kvï¼‰ã€‚è¿™ä¸¤ä¸ªé€‰æ‹©è¿™ä¸º EAGLE-3 åŸ‹ä¸‹äº†ä¼ç¬”ï¼šé€‰æ‹©1æ— éœ€è¿›è¡Œæ–°çš„ prefillï¼Œè™½ç„¶èŠ‚çœäº†æ—¶é—´ï¼Œä½†æ˜¯æŠ•æœºå‘½ä¸­ç‡ä¼šä¸‹é™ã€‚åŸå› åœ¨äºï¼šè®­ç»ƒæ—¶ draft model å…¨éƒ¨éƒ½æ˜¯åœ¨ stable kv çš„æƒ…å†µä¸‹è®­ç»ƒçš„ï¼Œå¦‚æœä½¿ç”¨ unstable kv ä¼šè®©è¯¯å·®åœ¨é•¿åºåˆ—ä¸­æŒç»­ç´¯ç§¯ï¼Œå½±å“é¢„æµ‹æ•ˆæœ

ä¸ºäº†å‘½ä¸­æ•ˆæœï¼Œæˆ‘ä»¬é€‰æ‹© stable kvã€‚å¹¶å°† verify draft tokens & stable kv cache çš„å›¾ç¤ºè¡¨ç¤ºå¦‚ä¸‹

<img src="EAGLE Speculative Decoding/image-20250401223604049.png" alt="image-20250401223604049" style="zoom:80%;" />

æˆ‘ç”¨ç®€å•çš„ä¼ªä»£ç è¡¨ç¤ºå¦‚ä¸‹ï¼Œä¸ºæ–¹ä¾¿æè¿°ï¼Œæˆ‘é»˜è®¤ä½¿ç”¨ temperature = 0

```python
def verify_draft_tokens(draft_tokens):
    """
    Args:
        - draft_tokens: (draft_len,)
    Returns:
        - accept_len
        - next_token_id: (1,)
        - last_hidden_states: (1, C)
        - hidden_states: (draft_len, C) for re-prefill draft model
    """
    hidden_states = base_model_transformer(embed(draft_tokens))
    logits = lm_head(hidden_states)           # (draft_len, vocab_size)
    
    new_tokens = argmax(logits, dim=1)        # (draft_len,)
    
    # shift verify & calculate accept length
    mask = draft_tokens[1:] == new_tokens[:-1] # (draft_len - 1,)
    # calculate accept length
    accept_len = sum(cumprod(mask))     # (1,)
    
    # pick up the next token id and cooresponding hidden states for next eagle decode
    next_token_id = new_tokens[accept_len]
    last_hidden_states = hidden_states[accept_len]
    
    return accept_len, next_token_id, last_hidden_states, hidden_states


def manage_kv_cache(base_model_kv_cache,
                   	draft_model_kv_cache,
                    hidden_states,	# (draft_len, C)
                    draft_tokens,	# (draft_len,)
                    accept_len):
    
    draft_token_len = draft_tokens.shape[0]
    pop_out_num = draft_token_len - (accept_len + 1)
    
    # clear wrong kv cache in base model kv cache
    base_model_kv_cache.popn(pop_out_num)
    
    # clear all except the 1st token in draft model kv cache
    draft_model_kv_cache.popn(draft_token_len - 1)
    
    # use accepted tokens to re-prefill draft model
    accept_tokens = draft_tokens[1: accept_len + 1]
    accept_hidden_states = hidden_states[0: accept_len]
    
    # use accepted tokens to re-prefill draft model
    if accept_len > 0:
        accept_tokens = draft_tokens[1: accept_len + 1]
        accept_hidden_states = hidden_states[0: accept_len]
        eagle_prefill(accept_tokens, accept_hidden_states)
```

è‡³æ­¤æ‰€æœ‰ EAGLE çš„å…³é”®ä»£ç å·²ç»æ•´ç†æ¸…æ¥šï¼Œå°†æ•´ä¸ª generate è¿‡ç¨‹ä¸²èµ·æ¥å°±è¡Œ

```python
def generate(input_tokens):
    # prefill of base model
    input_hidden_states = base_model_transformer(embed(input_tokens))    # (N, C)
    
    # prefill of eagle model
    eagle_prefill(input_tokens[1:], input_hidden_states[:-1])
    
    # get initial token and hidden states
    initial_token = argmax(lm_head(input_hidden_states[-1:]), dim=-1) # (1,)
    last_hidden_states = input_hidden_states[-1:]        # (1, C)
    
    output_tokens = []
    end_of_text = False
    next_token_id = initial_token
    # into the generate cycle
    while not end_of_text:
        # draft and verify and manage kv cache
        draft_tokens = eagle_decode(next_token_id, last_hidden_states)
        accept_len, next_token_id, last_hidden_states, hidden_states = \
                verify_draft_tokens(draft_tokens)
        manage_kv_cache(hidden_states, draft_tokens, accept_len)
        
        # collect accept tokens
        accept_tokens = draft_tokens[: accept_len + 1]
        
        # check end of text
        if eos_token in accept_tokens:
            eos_token_idx = get_idx(accept_tokens, eos_token)
            accept_tokens = accept_tokens[:eos_token_idx]
            end_of_text = True
            
        output_tokens.extend(accept_tokens)
    return output_tokens
```

## EAGLE-2 Tree

EAGLE-2 å¯¹äº EAGLE-1 çš„æ”¹è¿›åœ¨äºå°† draft model ä» Chain æ¨¡å¼æ”¹ä¸º Tree æ¨¡å¼ï¼ˆå®é™…ä¸Š Tree æ¨¡å¼æ˜¯ Medusa ç‡å…ˆæå‡ºï¼ŒEAGLE-1 ä¹Ÿé‡‡ç”¨äº† Tree æ¨¡å‹ï¼ŒEAGLE-2 æ˜¯å¯¹ Medusa Tree æ¨¡å¼çš„æ”¹è¿›ï¼‰

1. Chain æ¨¡å¼å°±æ˜¯ä¸€èˆ¬çš„ auto-regressive è¿‡ç¨‹ï¼Œæ¯ä¸€æ¬¡æ ¹æ® logits åª sample å‡ºä¸€ä¸ª tokenï¼Œ
2. Tree æ¨¡å¼åˆ™åœ¨æ¯ä¸€æ¬¡ auto-regressive è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸€æ¬¡æ ¹æ® logits ä¼šè¾“å‡ºå¤šä¸ª tokenï¼Œé€šå¸¸ç”¨ topk æ¥æ‰¾æ¦‚ç‡æœ€é«˜çš„ k ä¸ª token

Tree æ¨¡å¼çš„ intuitive æ¥è‡ªäºï¼šæ¯ä¸€æ¬¡å¤šçŒœå‡ ä¸ª tokenï¼Œé€šè¿‡å¤šç§å¯èƒ½çš„ç»„åˆï¼ˆè€Œä¸æ˜¯ä¸€ç§ï¼‰ï¼Œæ¥æé«˜å‘½ä¸­çš„æ¦‚ç‡ã€‚ä¸è¿‡è¿™æ ·çš„è¿‡ç¨‹è®¾è®¡åˆ°å¯¹ kv cache & mask çš„å¤æ‚ç®¡ç†ï¼Œä»¥åŠéœ€è¦ä½¿ç”¨ tree attentionï¼ŒäºŒè€…éƒ½ä¼šæœ‰è¾ƒå¤šçš„ overheadã€‚æ‰€ä»¥ Tree æ¨¡å¼åœ¨å®é™…ä½¿ç”¨ä¸­ä¼šæ¯”è¾ƒå°‘è§ï¼Œè¯¥éƒ¨åˆ†å†…å®¹å°†ä¸ä¼šæ•´ç†å¾—éå¸¸è¯¦ç»†

ä»¥è®ºæ–‡ä¸­çš„ Topk = 2 ä¸ºä¾‹å­ï¼Œçœ‹ä¸‹åœ¨ç”Ÿæˆ draft tokens æ—¶æ˜¯å¦‚ä½•ä¸€æ¬¡è¾“å…¥&è¾“å‡ºå¤šä¸ª token çš„

<img src="EAGLE Speculative Decoding/image-20250401223737838.png" alt="image-20250401223737838" style="zoom:80%;" />

 ä¸€è¡Œä¸€è¡Œçš„çœ‹ï¼š

1. input token `It`ï¼Œinitial tokenï¼Œå¾ˆå¥½ç†è§£ï¼Œå…¶å¾—åˆ†ä¸º 1ã€‚é€šè¿‡å…¶ logits ç”Ÿæˆç¬¬äºŒè¡Œ
2. input token `is & has`ï¼Œæ˜¯é€šè¿‡ logits ä¸­é€‰å‡ºæ¥çš„ top2 tokenï¼Œä»–ä»¬å¯¹åº”çš„æ¦‚ç‡ä¸º 0.6 & 0.2ï¼Œæ¯ä¸€ä¸ªèŠ‚ç‚¹çš„ç´¯è®¡å¾—åˆ†ä¸ºå®ƒçš„æ¦‚ç‡ä¹˜ä»¥å…¶çˆ¶èŠ‚ç‚¹çš„å¾—åˆ†ï¼Œæ‰€ä»¥å…¶ç´¯è®¡å¾—åˆ†ä¸º 0.6 å’Œ 0.2
3. input token `a & the & to & a`ï¼Œé€šè¿‡ç¬¬äºŒè¡Œé€‰å‡ºçš„å„è‡ª top2ã€‚åœ¨è¿™ä¸€è¡Œä¸­ï¼Œéœ€è¦å¯¹ 4 ä¸ª token æ’åºï¼Œä»ä¸­é€‰å‡ºæ–°çš„ top2ï¼Œè¯¥ top2 token ä½œä¸ºçˆ¶èŠ‚ç‚¹ï¼Œç”Ÿæˆä¸‹ä¸€è¡Œ
4. input token `good & nice & be & do`ï¼Œä»ç¬¬ä¸‰è¡Œä¸­çš„ top2 ç”Ÿæˆçš„å„è‡ª top2ã€‚ä¾ç„¶å¯¹ 4 ä¸ª token æ’åºï¼Œä»ä¸­é€‰å‡ºæ–°çš„ top2ï¼Œè¯¥ top2 token ä½œä¸ºçˆ¶èŠ‚ç‚¹ï¼Œç”Ÿæˆä¸‹ä¸€è¡Œ
5. å¾ªç¯ 4

å¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸€æ¬¡éƒ½æ˜¯åŠ¨æ€åœ°å»é€‰æ‹©æ¯ä¸ª depth å¯èƒ½æ€§æœ€é«˜çš„ token ä½œä¸ºçˆ¶èŠ‚ç‚¹ï¼Œç„¶åå»ç”Ÿæˆæ¥ä¸‹æ¥çš„ draft tokenï¼Œè¿™æ ·å°±èƒ½æœ€å¤§æ¦‚ç‡åœ°å¢åŠ æŠ•æœºå‘½ä¸­ç‡

ä¸è¿‡é—®é¢˜æ¥äº†ï¼šåœ¨è¿›è¡Œå‰å‘è®¡ç®—ä¸­ï¼Œç”±äºå„ä¸ª token åªä¼šå’Œè‡ªå·±çˆ¶èŠ‚ç‚¹è¿›è¡Œ attention äº¤äº’ï¼Œå¯¹äºå…¶å®ƒèŠ‚ç‚¹æ˜¯ä¸ä¼šè¿›è¡Œ attention è®¡ç®—çš„ã€‚æ‰€ä»¥æ¯æ¬¡ input token éœ€è¦æ­é…æ­£ç¡®çš„ attention mask æ‰èƒ½è·å¾—æ­£ç¡® kv cache & è¾“å‡º

ä¸€èˆ¬æ¥è¯´ transformer ä¼šè‡ªåŠ¨å¸®å¿™ç”Ÿæˆ causal mask æ¥è¿›è¡Œ attention è®¡ç®—ã€‚æ‰€ä»¥æˆ‘ä»¬åªéœ€è¦å°†ç²¾åŠ›é›†ä¸­åœ¨é causal çš„éƒ¨åˆ†ï¼Œç„¶åå°†é causal çš„éƒ¨åˆ†æ›¿æ¢å³å¯

<img src="EAGLE Speculative Decoding/image-20250401223824092.png" alt="image-20250401223824092" style="zoom:80%;" />

æ¯ä¸€è¡Œå°±ä»£è¡¨äº†ä¸€æ¬¡ draft model è¿›è¡Œäº†ä¸€æ¬¡ forwardï¼Œæ¯ä¸€æ¬¡ï¼ˆé™¤äº†ç¬¬ä¸€æ¬¡ï¼‰éƒ½æ˜¯è¾“å…¥äº†ä¸¤ä¸ª tokenã€‚å¯ä»¥çœ‹åˆ°è¿™ä¸¤ä¸ª token ä¹‹é—´ä¸ä¼šè¿›è¡Œæ³¨æ„åŠ›äº¤äº’ï¼Œæ‰€ä»¥ mask ä¸€å®šæ˜¯ identity matrixï¼ˆå›¾ä¸­è“è‰²è™šçº¿æ‰€ç¤ºï¼‰ã€‚è€Œæ¯ä¸€ä¸ª token éƒ½ä¼šå’Œè‡ªå·±çš„çˆ¶èŠ‚ç‚¹æ³¨æ„ç›¸åŒçš„å†…å®¹ï¼Œæ‰€ä»¥ä¼šç›´æ¥ç»§æ‰¿çˆ¶èŠ‚ç‚¹çš„ maskï¼ˆå›¾ä¸­çº¢è‰²è™šçº¿æ‰€ç¤ºï¼‰ã€‚ä»¥ä¸Šä¸¤ä¸ªè§„å¾‹å°±èƒ½æ¯æ¬¡å¿«é€Ÿæ„å»º input attention mask æ¥è¿›è¡Œå‰å‘è®¡ç®—

ä¸Šè¿°é˜¶æ®µæ˜¯ EAGLE-2 è®ºæ–‡ä¸­çš„ Expand é˜¶æ®µï¼Œè¯¥é˜¶æ®µå·²ç»åœ¨å°½å¯èƒ½åœ°ç”ŸæˆæŠ•æœºå‘½ä¸­ç‡è¾ƒé«˜çš„ tokenï¼Œä½† tokens ä»ç„¶å¾ˆå¤šï¼Œä¸”ä¸æ˜¯æ‰€æœ‰ token éƒ½å…·æœ‰ä¸é”™çš„æ½œè´¨ï¼Œä¾‹å¦‚ä¸€äº›æ·±å±‚çš„ token å…¶å‘½ä¸­æ¦‚ç‡å·²ç»å¾ˆä½äº†ï¼Œä¸å¦‚å¤šé€‰ä¸€äº›æµ…å±‚çš„ token æ¥æé«˜å‘½ä¸­ç‡ã€‚æ‰€ä»¥æˆ‘ä»¬å¯¹æ‰€æœ‰çš„ draft tokens æ ¹æ®å…¶ç´¯è®¡å¾—åˆ†è¿›è¡Œæ’åºï¼Œå†é€‰å‡º topk ä¸ª tokensï¼Œè¿™å°±æ˜¯æœ€ç»ˆä¼šè¾“å…¥åˆ° base model è¿›è¡ŒéªŒè¯çš„ tokensã€‚åœ¨è¾“å…¥åˆ° base model ä¸­å»æ—¶ï¼Œä»ç„¶éœ€è¦ä¸€ä¸ªæ­£ç¡®çš„ attention maskï¼Œè¯¥ attention mask ä¿è¯äº† token åªä¼šæ³¨æ„åˆ°è‡ªå·±çš„çˆ¶èŠ‚ç‚¹ tokens

<img src="EAGLE Speculative Decoding/image-20250401224639668.png" alt="image-20250401224639668" style="zoom:67%;" />

ç›¸æ¯”äº generate topK å½“ä¸­çš„ attention maskï¼Œè¿™é‡Œé¢å¤šå‡ºäº† `the` tokenï¼Œ`the` token æ²¡æœ‰è¢«é€‰ä¸­ä½œä¸ºçˆ¶èŠ‚ç‚¹æ¥ç”Ÿæˆå­ tokenï¼Œä½†æ˜¯å…¶å‘½ä¸­æ¦‚ç‡ä»ç„¶è¾ƒé«˜ï¼Œæ‰€ä»¥è¢«é€‰å…¥æœ€ç»ˆçš„ draft token è¿›è¡ŒéªŒè¯

## EAGLE-3 Train

åœ¨ä¹‹å‰çš„ EAGLE-1 Chain ä¸­æˆ‘å°±åŸ‹ä¸‹äº†ä¼ç¬”ï¼šEAGLE åœ¨è®­ç»ƒå½“ä¸­åªçœ‹åˆ°äº† stable kv çš„æƒ…å†µï¼Œä¹Ÿå°±æ˜¯å…¶æ‰€ä½¿ç”¨çš„ hidden states å…¨éƒ¨æ¥è‡ªäº base modelã€‚ä½†æ˜¯ EAGLE åœ¨æ¨ç†çš„æ—¶å€™å¤§éƒ¨åˆ† hidden states å…¶å®éƒ½æ¥è‡ªäº draft model æœ¬èº«äº§ç”Ÿçš„ hidden statesã€‚è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆ EAGLE åœ¨è®­ç»ƒçš„æ—¶å€™éœ€è¦å¯¹ draft model hidden states è¿›è¡Œç›‘ç£ï¼Œè®©å…¶å°½å¯èƒ½åœ°å»å¤åŸ base model hidden statesï¼Œè¿™æ ·åœ¨æ¨ç†çš„æ—¶å€™å°½å¯èƒ½è¿˜åŸè®­ç»ƒåœºæ™¯

EAGLE-3 å°±å‘ç°ï¼šå¦‚æœä¸å¯¹è¿™ä¸ª hidden states è¿›è¡Œç›‘ç£ï¼Œç¬¬ä¸€ä¸ªçŒœæµ‹çš„ token çš„å‘½ä¸­ç‡ä¼šéšç€æ•°æ®é›†å¢åŠ è€Œæ˜¾è‘—æå‡ï¼Œä½†æ˜¯ç¬¬äºŒä¸ªçŒœæµ‹çš„ token å‘½ä¸­ç‡å°±æ€¥å‰§ä¸‹é™äº†ã€‚æ€»ç»“å…¶ä¸­åŸå› ï¼š

1. å»é™¤ hidden states ç›‘ç£è¿›è¡Œçº¦æŸï¼Œåˆ™åªæœ‰ next token prediction ç›‘ç£ï¼Œä½¿å¾— next token prediction èƒ½åŠ›å¢å¼ºï¼Œå¹¶ä¸”å…·æœ‰ scaling æ€§è´¨ã€‚ä½†è¯¥æƒ…å†µåªå­˜åœ¨äºç¬¬ä¸€ä¸ªçŒœæµ‹ tokenã€‚åä¹‹ï¼Œä¿ç•™ hidden states ç›‘ç£è¿›è¡Œçº¦æŸï¼Œå°†å½±å“ next token prediction çš„å­¦ä¹ ï¼Œæ©ç›–äº† scaling æ€§è´¨
2. ç¬¬äºŒä¸ªçŒœæµ‹ token ä½¿ç”¨çš„ hidden states æ˜¯ç”± draft model äº§ç”Ÿï¼Œå±äº unstable kv æƒ…å†µï¼Œåœ¨è®­ç»ƒä¸­æ²¡æœ‰è§åˆ°è¿‡ï¼Œå‘½ä¸­ç‡æ€¥å‰§ä¸‹é™

äºæ˜¯ä¹ï¼Œè§£å†³æ–¹æ³•å‘¼ä¹‹æ¬²å‡ºï¼šåœ¨è®­ç»ƒ EAGLE æ—¶ï¼ŒåŠ å…¥ unstable kv æƒ…å†µï¼Œè¿™æ ·å°±èƒ½å°† train & test æƒ…å†µè¿›è¡Œç»Ÿä¸€ï¼Œç„¶åå»é™¤ hidden states ç›‘ç£ï¼Œæ‹¿åˆ° scaling æ”¶ç›Š

<img src="EAGLE Speculative Decoding/image-20250401224702909.png" alt="image-20250401224702909" style="zoom:67%;" />

å¦å¤– EAGLE-3 ä½¿ç”¨çš„ base model hidden states æœ‰å¤šä¸ªï¼Œæ¥è‡ªäºä¸åŒ layerï¼Œç”¨äºè¡¨å¾ high & middle & low featureï¼Œè¿™ä¸ªæŠ€å·§ä¹Ÿæ˜¯éå¸¸æœ‰ç”¨çš„ã€‚æœ€åè®ºæ–‡é‡Œå†™çš„è¿™å¥è¯è¿˜æŒºæœ‰æ„æ€çš„

> Interestingly, EAGLE inspired the multi-token prediction technique used in the pre-training of DeepSeek v3 (Liuetal., 2024a), which in turn inspired new architectural designs in EAGLE-3.

ç„¶åæˆ‘å°±å»çœ‹äº† DeepSeek-V3 çš„ Multi-Token-Predictionï¼Œçœ‹ä¸Šå»å’Œ EAGLE-1 çš„æ€è·¯ä¼šæ¯”è¾ƒåƒï¼Œä½†æ˜¯å’Œ EAGLE-3 çš„æ€è·¯ç›¸å·®ç”šè¿œï¼š

1. DeepSeek-V3 åœ¨è®ºæ–‡ä¸­æ˜ç¡®æåˆ°äº† complete causal maskï¼Œè¿™ä¸ä¸åƒ EAGLE-3 å½“ä¸­çš„ç‰¹æ®Š maskï¼Œæ›´åƒæ˜¯ EAGLE-1 Chain å½“ä¸­çš„æ­£å¸¸ causal mask
2. DeepSeek-V3 æ‰€ä½¿ç”¨çš„ input tokens ä¸ºçœŸå®æ•°æ®ï¼Œè€Œ EAGLE-3 åœ¨è¿›è¡Œç¬¬äºŒä¸ª training step çš„æ—¶å€™å·²ç»å¼€å§‹ä½¿ç”¨ draft model æ‰€äº§ç”Ÿçš„ tokenã€‚æˆ–è®¸è¿™é‡Œåƒ DeepSeek-V3 ä¸€æ ·ä½¿ç”¨ ground truth input tokens ä¼šæ›´å¥½
3. DeepSeek-V3 è®¨è®ºçš„æ˜¯å¤šä¸ª module æ¥é¢„æµ‹å¤šä¸ª next tokenï¼Œè€Œ EAGLE-3 ä»ç„¶æ˜¯ä½¿ç”¨ä¸€ä¸ª draft model æ¥ç”Ÿæˆå¤šä¸ª next token

## Question

- å¦‚ä½•è¿›è¡Œ batched speculative samplingï¼Ÿ

  æŒ‘æˆ˜åœ¨äºæ¯ä¸€ä¸ª sequence éƒ½æœ‰è‡ªå·±çš„ accept lengthï¼Œä»è€Œå¯¼è‡´æ— æ³•è¿›è¡Œæœ‰æ•ˆåœ° batch decode

  æ–¹æ¡ˆä¸€ï¼šæ•´ä¸ª batch åŒæ­¥ä¸ºæœ€å°çš„ accept-lengthã€‚è¿™æ ·å°±èƒ½å¤Ÿä»ç„¶åŒæ­¥æ•´ä¸ª batch ä»ç„¶å¢åŠ ç›¸åŒçš„ tokenï¼ŒæŒç»­è¿›è¡Œ batch decode

  æ–¹æ¡ˆäºŒï¼šä¹Ÿè®¸å€Ÿé‰´ continous batching æ‰æ˜¯æœ€ç»ˆçš„è§£å†³æ–¹æ¡ˆï¼Ÿæ—¢ç„¶ continous batching èƒ½å¤Ÿè§£å†³ä¸åŒ sequence length çš„ batch decodeï¼Œé‚£ä¹ˆè¿™ç§æƒ…å†µæ”¾åˆ°æŠ•æœºé‡‡æ ·ä¸Šåº”è¯¥æ˜¯æœ€ä¼˜è§£ï¼šæ¯ä¸€ä¸ª sequence éƒ½å„è‡ªæ¥æ”¶è‡ªå·±æ­£ç¡®çš„æŠ•æœº tokensï¼Œç„¶åå†å¼€å§‹ä¸‹ä¸€æ¬¡ decodeã€‚ä¸‹å›¾å‚è€ƒè‡ª [blog](https://friendli.ai/blog/llm-iteration-batching) & [zhihu](https://zhuanlan.zhihu.com/p/680123256)

  <img src="EAGLE Speculative Decoding/v2-8092ac7d9ffc1eea2d2782d9a946b79e_b.webp" alt="åŠ¨å›¾" style="zoom: 80%;" />
  
- æ˜¯å¦åº”è¯¥åœ¨ EAGLE prefill çš„æ—¶å€™æŠŠ firtst token ä¹ŸåŠ ä¸Šï¼Ÿ

  åœ¨ EAGLE prefill çš„æ—¶å€™ï¼Œç”±äºç¬¬ä¸€ä¸ª token æ²¡æœ‰å¯¹åº”çš„ hidden statesï¼Œæ‰€ä»¥æ— æ³•åœ¨è®­ç»ƒä¸­åŠ ä¸Šï¼Œå®é™…ä¸Šæˆ‘ä»¬å¯ä»¥é€šè¿‡ padding ä¸€ä¸ª `<endoftext>` token æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œè¿™æ ·æ—¢èƒ½å’Œç¬¬äºŒè½®å¯¹è¯çš„å¤„ç†ç»Ÿä¸€èµ·æ¥ï¼Œåˆèƒ½è·å¾—ç¬¬ä¸€ä¸ª token çš„ embedding ä¿¡æ¯

- Lesson from EAGLE-3?

  åœ¨ EAGLE-1 å½“ä¸­è®¤ä¸ºå¯¹ feature çš„å­¦ä¹ éå¸¸å¿…è¦çš„ï¼Œç›´æ¥åœ¨å…¶ intro ä¸­å†™é“

  > autoregression at the feature (second-to-top-layer) level is more straightforward than at the token level.

  è€Œæœ€ç»ˆå´å‘ç°ï¼Œä½¿ç”¨ feature æ˜¯å¿…è¦çš„ï¼Œè€Œå­¦ä¹ è¿™ä¸ª feature å´é˜»ç¢äº†è¿›ä¸€æ­¥çš„å­¦ä¹ ã€‚Man! What can I say! [The bitter lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) all over again:

  > And the human-knowledge approach tends to complicate methods in ways that make them less suited to taking advantage of general methods leveraging computation. 