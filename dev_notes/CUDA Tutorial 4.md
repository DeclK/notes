# CUDA Tutorial 4

## E11 基本优化

基本思路：

有效的并行算法 + 针对 GPU 架构的优化 = 最优性能

### 思路1：并行规约

两两求和，指数递减 log(n) 复杂度

注意代码：

1. 代码
2. 线程释放（warp 分割）。**减少分支发散，让 warp 尽早完工**

warp 切换设备没有代价？

许多 warps 在一起可以**隐藏延迟**？隐藏延迟可能需要一个专题

## E12 存储优化

1. 数据传输带宽（Host to Device）远小于 global memory 带宽，两个数量级

   1. 减少传输
   2. 组团传输，避免小块数据频繁传输
   3. 内存传输与计算时间重叠

2. **访存合并**

   global memory 带宽大，但是延迟高

   global memory 默认缓存于一级缓存，可以设置编译器禁用一级缓存（为什么要禁用？数据如何决定是否留在缓存？）

   [合并访问](https://face2ai.com/CUDA-F-4-3-%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%BC%8F/)

   > 内存事务：内存事务（Memory Transaction）是指GPU或其他处理器从内存中读取或写入数据的操作过程。它可以是由一个或多个内存访问请求组成的逻辑单元，用于执行读取或写入操作。
   >
   > 在内存事务中，处理器发送读取或写入请求到内存控制器，并等待内存控制器返回所需的数据或完成写入操作。一次内存事务通常包括以下几个步骤：
   >
   > 1. 发送请求；2. 地址解码；3. 数据传输；4. 完成事务

3. shared memory 用来避免不满足合并条件的访存，进行重排顺序，从而支持合并寻址

17:54