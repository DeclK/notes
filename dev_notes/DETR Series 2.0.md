# DETR Series 2.0

è¿™ä¸€æ¬¡å¿…å®šæ‹¿ä¸‹ DETR ç³»åˆ—ä»¥åŠ Deformable DETR ä»¥åŠ Vision Transformer çš„æ„å»ºè¿‡ç¨‹ğŸ¤”

ä¸€å¼€å§‹æ•´ç†éƒ½æ˜¯ä»¥ä¸€ç§çº¿æ€§çš„æ€ç»´åœ¨åšï¼Œçœ‹ä¸€ç‚¹ï¼Œæ•´ç†ä¸€ç‚¹ã€‚è¿™æ ·å¯èƒ½æ•´ç†å®Œè¿‡åæ²¡æœ‰ä¸€ä¸ªé€»è¾‘çš„æ€»ç»“ï¼Œå›çœ‹æ—¶ä¹Ÿæ¯”è¾ƒç©ºæ´ï¼Œä½†è¿™ä¹Ÿæ˜¯æ²¡åŠæ³•çš„äº‹æƒ…ï¼Œæ¯•ç«Ÿæ˜¯ç¬¬ä¸€æ¬¡æ•´ç†

äºæ˜¯åœ¨ç¬¬äºŒéè¿›è¡Œæ€»ç»“æ—¶å°±éœ€è¦æ³¨æ„æ€»ç»“çš„é€»è¾‘é€šé¡ºé—®é¢˜

è¿™ç¯‡ç¬”è®°æ‰€æœ‰çš„ä»£ç éƒ½å‚è€ƒè‡ª detrex ä»¥ä¿è¯ç»Ÿä¸€æ€§

## Blocks

### MultiHead Attention

Attention çš„åŠŸèƒ½å°±æ˜¯è®© query å»è¯¢é—® (key, value) ç„¶ååŠ æƒè¾“å‡º
$$
\operatorname{softmax}\left(\frac{\mathbf{Q K}^{\top}}{\sqrt{d}}\right) \mathbf{V} \in \mathbb{R}^{n \times v}
$$
åœ¨ detrex å½“ä¸­ç›´æ¥è°ƒç”¨äº† `nn.MultiheadAttentnion` å¹¶åŠ å…¥ identity connection & posotional embeddingï¼Œå®Œæˆäº†æ•´ä½“åŒ…è£… `MultiheadAttention`

```python
    def forward(
        self,
        query: torch.Tensor,
        key = None,
        value = None,
        identity = None,
        query_pos = None,
        key_pos = None,
        attn_mask = None,
        key_padding_mask = None,
    ) -> torch.Tensor:

        if key is None:
            key = query
        if value is None:
            value = key
        if identity is None:
            identity = query

        if query_pos is not None:
            query = query + query_pos
        if key_pos is not None:
            key = key + key_pos

        out = self.attn(
            query=query,
            key=key,
            value=value,
            attn_mask=attn_mask,
            key_padding_mask=key_padding_mask,
        )[0]
		# self.proj_drop = nn.Dropout(r)
        return identity + self.proj_drop(out)
```

åœ¨ä½¿ç”¨ `nn.MultiheadAttentnion` æ—¶æœ‰ä¸¤ä¸ªæŠ€å·§å’Œä¸¤ä¸ªæ³¨æ„ç‚¹ï¼š

1. `attn_mask`ï¼šå…¶å½¢çŠ¶ä¸º `(num_query, num_key)`ï¼Œæ˜¯ bool çŸ©é˜µï¼Œç”¨äºå»é™¤ä¸å¸Œæœ›è®¡ç®—çš„éƒ¨åˆ†å…ƒç´ 
2. `key_padding_mask`ï¼šå…¶å½¢çŠ¶ä¸º `(bs, num_query)`ï¼Œç”¨äºå¿½ç•¥ä¸éœ€è¦è®¡ç®—çš„ keyï¼Œæœ¬è´¨ä¸Šå¯ç”± `attn_mask` æ›¿ä»£ï¼Œä½†æ˜¯è¯¥æ¥å£æ›´æ–¹ä¾¿
3. å¯ä»¥è¿›è¡Œä¸¤æ¬¡ dropoutï¼Œä¸€æ¬¡åœ¨ attentnionï¼Œä¸€æ¬¡åœ¨ linear projection
4. Positional embedding ä»…å¯¹ query å’Œ key å­˜åœ¨ï¼Œå¯¹ value ä¸å­˜åœ¨

è¿™é‡Œæ”¾ä¸€ä¸ª `MultiHead-SelfAttention` çš„æ— æ©ç å®ç°ç‰ˆæœ¬ï¼Œæ˜¯é¢è¯•å¸¸è€ƒï¼Œå‚è€ƒ timm çš„å®ç°

```python
class Attention(nn.Module):
    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):
        super().__init__()
        assert dim % num_heads == 0, 'dim should be divisible by num_heads'
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = head_dim ** -0.5

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

    def forward(self, x):
        B, N, C = x.shape
        # qkv: (3, B, num_heads, N, head_dim)
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        # make torchscript happy (cannot use tensor as tuple)
        q, k, v = qkv.unbind(0)

        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x
```

Self-Attention çš„ qkv éƒ½æ˜¯ä¸€æ ·çš„ï¼Œå¦‚æœè¦åš cross-attentnion åªè¦æŠŠé‚£ä¸ªå¤§çš„çº¿æ€§å±‚ `self.proj` æ¢æˆ3ä¸ª qkv å„è‡ªçš„çº¿æ€§å±‚å³å¯ 

### Deformable Attention

Deformable Attention å¯ä»¥è¯´è·Ÿ Attention ç›¸å·®å¾ˆå¤§ï¼ŒåŸºæœ¬ä¸Šæ²¡ä»€ä¹ˆè”ç³»ğŸ¤£å…¶æœ¬è´¨å°±æ˜¯å¯å˜å·ç§¯ï¼

ç”¨è¯­è¨€æ¥ç®€è¦å½¢å®¹ï¼š

1. ç»™å®š queryï¼Œé€šè¿‡ query è®¡ç®— sampling offsets & attention weights
2. ç»™å®š reference points for queryï¼Œé€šè¿‡ reference points + sampling offsets è·å¾—é‡‡æ ·ç‚¹ä½ç½®
3. ç»™å®š valueï¼Œé€šè¿‡é‡‡æ ·ç‚¹å’Œ attention weights è·å¾—æœ€ç»ˆè¾“å‡º

å…¶ä¸­éœ€è¦è€ƒè™‘ multi-scale value çš„æƒ…å†µï¼Œè§„å®š reference points åœ¨å„ä¸ª scale çš„é‡‡æ ·ç‚¹ç›¸åŒï¼ˆæœ‰æ²¡æœ‰å¯èƒ½ç›´æ¥å¢åŠ é‡‡æ ·ç‚¹æ˜¯ä¸€æ ·çš„æ•ˆæœï¼‰ï¼Œå¯ç®€ç­”ç†è§£ä¸ºå¦‚ä¸‹å›¾ï¼Œå…¶ç‰¹å¾å½¢çŠ¶ä¸º (B, N1+N2+N3+N4, C)

<img src="DETR Series 2.0/image-20230403214527125.png" alt="image-20230403214527125" style="zoom:50%;" />

è¦å®Œæˆ multi-scale çš„åŠŸèƒ½éœ€è¦çŸ¥é“æ¯ä¸€ä¸ª scale çš„ pixel æ•°é‡ï¼Œç„¶åä½¿ç”¨å¾ªç¯å®Œæˆå¯¹æ¯ä¸€ä¸ª scale çš„è¯¢é—®

```python

class MultiScaleDeformableAttention(nn.Module):
    """ Modified from Deformable-DETR for 1D
    """

    def __init__(
        self,
        embed_dim: int = 64,
        num_heads: int = 4,
        num_levels: int = 1,
        num_points: int = 4,
        dropout: float = 0.1,
    ):
        super().__init__()

        self.dropout = nn.Dropout(dropout)

        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.num_levels = num_levels
        self.num_points = num_points
        self.sampling_offsets = nn.Linear(embed_dim, num_heads * num_levels * num_points * 2)
        self.attention_weights = nn.Linear(embed_dim, num_heads * num_levels * num_points)
        self.value_proj = nn.Linear(embed_dim, embed_dim)
        self.output_proj = nn.Linear(embed_dim, embed_dim)
        
    def forward(
        self,
        query,	                       # (B, N, C)
        spatial_shapes = None,         # (levels, 2), sum is sequence len N
        reference_points = None,       # (B, N, level, 2)
        value = None,
        identity = None,
        query_pos = None,
        **kwargs):
        
        if value is None:	# True, when self-attetion
            value = query
        if identity is None:	# True
            identity = query
        if query_pos is not None:
            query = query + query_pos

        bs, num_query, _ = query.shape
        bs, num_value, _ = value.shape

        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_value

        value = self.value_proj(value)

        value = value.view(bs, num_value, self.num_heads, -1)
        
        sampling_offsets = self.sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2
        )
        attention_weights = self.attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points
        )
        attention_weights = attention_weights.softmax(-1)
        attention_weights = attention_weights.view(
            bs,
            num_query,
            self.num_heads,
            self.num_levels,
            self.num_points,
        )
        
        # bs, num_query, num_heads, num_levels, num_points, 2
        if reference_points.shape[-1] == 2:
            offset_normalizer = torch.stack([spatial_shapes[..., 1], spatial_shapes[..., 0]], -1)
            sampling_locations = (
                reference_points[:, :, None, :, None, :]
                + sampling_offsets / offset_normalizer[None, None, None, :, None, :]
            )
        elif reference_points.shape[-1] == 4:
            sampling_locations = (
                reference_points[:, :, None, :, None, :2]
                + sampling_offsets
                / self.num_points
                * reference_points[:, :, None, :, None, 2:]
                * 0.5
            )
        
        output = multi_scale_deformable_attn_pytorch(
                 value, spatial_shapes, sampling_locations, attention_weights)
        output = self.output_proj(output)

        return self.dropout(output) + identity
```

åœ¨ä½¿ç”¨ Deformable Attention æ—¶æœ‰ä¸¤ä¸ªç‰¹ç‚¹å’Œä¸¤ä¸ªç–‘é—®ï¼š

1. æ•´ä¸ªè®¡ç®—è¿‡ç¨‹æ²¡æœ‰ `key` çš„å­˜åœ¨ï¼ä»…ç”± `query` å’Œ `value` è®¡ç®—
2. é€šè¿‡çº¦æŸé‡‡æ ·ç‚¹çš„æ•°é‡ï¼Œdeformable attention å¯ä»¥æ˜¾è‘—å‡å°‘è®¡ç®—é‡
3. ä¸ºä»€ä¹ˆè¦å¯¹ reference points è¿›è¡Œè¿™æ ·çš„å½’ä¸€åŒ–ï¼Ÿå¯èƒ½æ˜¯å¸Œæœ›å¯¹å°ºå¯¸æœ‰ä¸€å®šæ„ŸçŸ¥
4. sampling offsets ä»…ç”± query äº§ç”Ÿï¼Œæ²¡æœ‰ä¸ key è¿›è¡Œäº¤æµï¼Œè¿™ä¼šä¸ä¼šæŸå¤±ä¸€å®šæ€§èƒ½

è¾“å…¥ä¸­ä»æœ‰ä¸¤ä¸ªæœªè¯´æ˜çš„å˜é‡ï¼š

1. å¦‚ä½•ç”Ÿæˆ reference points
   1. åœ¨ self-attention ä¸­ï¼Œreference points å³ä¸ºæ¯ä¸ª pixel æœ¬èº«çš„åæ ‡
   2. åœ¨ cross-attention ä¸­ï¼Œreference points æœ‰ä¸¤ç§æ–¹æ³•ï¼š
      - ä½¿ç”¨ä¸€ä¸ª `nn.Linear(dim, 2)` å¤„ç† queryï¼Œç„¶åå† sigmoid ç”Ÿæˆ
      - åœ¨ä¸¤é˜¶æ®µä¸­ï¼Œæ¥æºäºç¬¬ä¸€é˜¶æ®µçš„ proposal
2. å¦‚ä½•ç”Ÿæˆ query positional embedding
   1. åœ¨ self-attention ä¸­ï¼Œä½¿ç”¨å¸¸è§„çš„ sin positional embedding + learnable level embedding
   2. åœ¨ cross-attention ä¸­ï¼Œå¯ä¸ä½¿ç”¨ positional embeddingï¼Œå› ä¸ºæœ‰äº† reference pointsã€‚ä½†åŠ å…¥ä¹‹åæ•ˆæœæ›´ä½³ï¼Œå³ä½¿ç”¨ä¸€ä¸ª `nn.Linear(dim, dim)` å¯¹ reference points è¿›è¡Œç¼–ç ï¼Œè¿™å°±æ˜¯ Dynamic Anchor çš„ä¸»è¦å‡çº§ï¼ˆä¸¤è¡Œä»£ç å°±è¡Œï¼‰

### æ„å»ºTransformer

æ„å»º transformer è¿˜è¦æœ‰ä¸€ä¸ªç¯èŠ‚ï¼Œå°±æ˜¯ FFNï¼Œå³ä¸¤ä¸ªçº¿æ€§å±‚

```python
class FFN(nn.Module):
    def __init__(
        self,
        embed_dim=256,
        feedforward_dim=1024,
        output_dim=None,
        num_fcs=2,
        activation=nn.ReLU(inplace=True),
        ffn_drop=0.0,
        fc_bias=True,
        add_identity=True,
    ):
        super(FFN, self).__init__()
        assert num_fcs >= 2, "num_fcs should be no less " f"than 2. got {num_fcs}."
        self.embed_dim = embed_dim
        self.feedforward_dim = feedforward_dim
        self.num_fcs = num_fcs
        self.activation = activation

        output_dim = embed_dim if output_dim is None else output_dim

        layers = []
        in_channels = embed_dim
        for _ in range(num_fcs - 1):
            layers.append(
                nn.Sequential(
                    nn.Linear(in_channels, feedforward_dim, bias=fc_bias),
                    self.activation,
                    nn.Dropout(ffn_drop),
                )
            )
            in_channels = feedforward_dim
        layers.append(nn.Linear(feedforward_dim, output_dim, bias=fc_bias))
        layers.append(nn.Dropout(ffn_drop))
        self.layers = nn.Sequential(*layers)
        self.add_identity = add_identity

    def forward(self, x, identity=None) -> torch.Tensor:

        out = self.layers(x)
        if not self.add_identity:
            return out
        if identity is None:
            identity = x
        return identity + out
```

æ³¨æ„ï¼Œè¿™é‡Œçš„ `FFN` ä¾ç„¶ä¿ç•™äº† identity connectionï¼Œå’Œä¸Šè¿° attention ä¸€æ ·ã€‚ä¸‹é¢å°±å¯ä»¥é€šè¿‡ç»„åˆå„ä¸ªç½‘ç»œå±‚æ¥æ­å»º transformer block å•¦ğŸ˜

ä¸€èˆ¬æ¥è®² transformer block å°±ä¸¤ç§å½¢å¼

```python
operation_order=("self_attn", "norm", "ffn", "norm")
operation_order=("self_attn", "norm", "cross_attn", "norm", "ffn", "norm")
```

å‰è€…ä¸º encoderï¼Œåè€…ä¸º decoderï¼Œnorm å±‚ä¸º `nn.LayerNorm`

## DINO Improves

### Box Refinement

Box refinementç®—æ³•æ˜¯ä¸€ç§è¿­ä»£å¾®è°ƒç­–ç•¥ï¼Œå®ƒç±»ä¼¼äºCascade R-CNNæ–¹æ³•ï¼Œå¯ä»¥åœ¨æ¯ä¸ªè§£ç å™¨å±‚ä¸Šå¯¹è¾¹ç•Œæ¡†è¿›è¡Œå¾®è°ƒï¼Œæ‰€ä»¥åœ¨åˆ›å»ºçš„ `self.box_embed & self.class_embed` æ˜¯å„è‡ªç‹¬ç«‹çš„ï¼Œä¸å…±äº«å‚æ•°

### Look Forward Twice

å®é™…ä¸Šå°±æ˜¯æŠŠ box refinement ä¸­çš„ reference points æ²¡æœ‰ä»ä¸­ detach å‡ºæ¥ï¼ˆä¸€è¡Œä»£ç å°±è¡Œï¼‰ï¼Œå®é™…ä¸Šå°±æ˜¯å¢åŠ äº†æ¢¯åº¦çš„è®¡ç®—å¤æ‚åº¦ä»¥æå‡æ•ˆæœ

### Two Stage

ä¸¤é˜¶æ®µæ–¹æ³•å°±æ˜¯åˆ©ç”¨ encoder æå‡º proposal ä½œä¸º query çš„ reference points

ç”±äº two stage çš„å‡ºç°ï¼Œæ‰€æœ‰çš„ reference points ç”±2ç»´çš„ç‚¹ï¼Œå˜æˆäº†4ç»´çš„é€‰æ¡†ï¼Œè¿™åœ¨ deformable attention é‡Œæœ‰åšç®€è¦å¤„ç†ï¼Œä½†é‡ç‚¹ä»ç„¶è¿˜æ˜¯é€‰æ¡†çš„ä¸­å¿ƒ

ä»£ç ä¸­çš„ valid ratios åº”è¯¥å¯ä»¥ç§»é™¤ï¼Œç”šè‡³å¯èƒ½æ˜¯æœ‰å®³çš„ï¼Œå¹¶ä¸”ä»£ç é‡Œæœ‰ä¸€äº› bug æ²¡æœ‰ä¿®å¤ï¼Œå…ˆé™¤ä»¥åä¹˜ä»¥ï¼ŒåŸºæœ¬ä¸ŠæŠµæ¶ˆäº†

### Mixed Query Selection

ä¹Ÿå¾ˆç®€å•ï¼Œå°±æ˜¯åªè¦ proposal çš„ä½ç½®ä½œä¸º reference pointsï¼Œä¸ç”¨ proposal ä½œä¸º queryã€‚çœŸæ­£çš„ content query ä¾ç„¶æ˜¯ learnable parameters

### Contrastive Denoising

ä»£ç é€»è¾‘ç†é¡ºï¼ŒæŠŠæ¯ä¸€ä¸ªå°å—çš„ç›®çš„æè¿°å‡ºæ¥

ç”¨ä¼ªä»£ç çš„å½¢å¼æ•´ç†DINO

## More

1. ä¸­é—´æŸå¤±å‡½æ•°çš„è®¡ç®—
2. ç¬¬ä¸€é˜¶æ®µæŸå¤±å‡½æ•°çš„è®¡ç®—
3. COCO dataset