# TensorRT-EDGE-LLM

è¿™æ˜¯ NVIDIA ç›®å‰æ¨å‡ºçš„ä¸€æ¬¾ç«¯ä¾§ LLM/VLM æ¨ç†æ¡†æ¶ï¼Œä¸»è¦é’ˆå¯¹äº Thor GPUï¼Œå¯ä»¥è¯´æ˜¯ exclusive support äº†ã€‚å¯¹äºæƒ³è¦å¿«é€Ÿåœ¨ Thor ä¸Šéƒ¨ç½²è‡ªå·±ç«¯ä¾§æ¨¡å‹çš„å›¢é˜Ÿæ¥è¯´å¯èƒ½æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ã€‚

å­¦ä¹ æ€è·¯ï¼š

1. æ•´ç†éƒ¨ç½²æµç¨‹
2. æ•´ç†åŸºæœ¬çš„ä»£ç é€»è¾‘ï¼Œå¦‚ä½•åˆ©ç”¨ TensorRT æ¥æ„å»ºä¸€ä¸ª llm app
3. å¦‚ä½•æµ‹è¯•æ¨¡å‹ç²¾åº¦ï¼Œå°¤å…¶å¯¹é‡åŒ–æ¨¡å‹è€Œè¨€
4. è¿›é˜¶ï¼šå¦‚ä½•ç®¡ç†ç®—å­å’Œ KVCache
5. è¿›é˜¶ï¼šå¦‚ä½•å®Œæˆ EAGLE3 æŠ•æœºé‡‡æ ·

## Get Started

å…ˆç”¨ Qwen0.6B èµ°é€šä¸€ä¸ª demo çœ‹çœ‹ï¼Œæ ¹æ® [Quick_Start_Guide](https://github.com/NVIDIA/TensorRT-Edge-LLM/blob/main/docs/source/developer_guide/01.2_Quick_Start_Guide.md)

åœ¨ guide å½“ä¸­è¯´éœ€è¦åœ¨ x86 host ä¸Šè¿›è¡Œæ¨¡å‹ exportï¼Œæˆ‘æƒ³å°è¯•ç›´æ¥åœ¨ thor ä¸Šç›´æ¥ export ä¸çŸ¥é“èƒ½ä¸èƒ½è¡Œ

åŸºç¡€é•œåƒç›´æ¥ä½¿ç”¨ NGC pytorch container [doc](https://docs.nvidia.com/jetson/agx-thor-devkit/user-guide/latest/setup_docker.html#docker-setup-test)ï¼Œç«Ÿç„¶å¯ä»¥ç›´æ¥ workï¼Œä¹‹å‰ä¸€ç›´è¦å•ç‹¬ä¸º jetson æä¸€ä¸ªé•œåƒï¼Œnv çš„åŸºå»ºè¶Šæ¥è¶Šå¥½äº†ã€‚ä½¿ç”¨å‘½ä»¤åˆ›å»ºå®¹å™¨

```shell
docker run -it --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 --runtime=nvidia image_id
```

ä½¿ç”¨ [HF-Mirror](https://hf-mirror.com/) ä¸‹è½½ Qwen0.6B æ¨¡å‹ï¼Œæ¨èä½¿ç”¨é•œåƒç½‘ç«™å¼€å‘çš„ hfd æ–¹å¼

```shell
apt install aria2
wget https://hf-mirror.com/hfd/hfd.sh
chmod a+x hfd.sh
export HF_ENDPOINT=https://hf-mirror.com
./hfd.sh Qwen/Qwen3-0.6B
```

## TensorRT Basics

è·Ÿ DeepSeek äº¤æµåŠå¤©åç”»å‡ºæ¥çš„ TensorRT çš„ workflow

```txt
flowchart TD
    subgraph ç¦»çº¿æ„å»ºé˜¶æ®µ
        A1[å®šä¹‰Networkä¸Config] --> A2[è®¾ç½®ä¼˜åŒ–Profile]
        A2 --> A3[buildSerializedNetwork]
        A3 --> A4[(åºåˆ—åŒ–Engineæ–‡ä»¶)]
    end

    subgraph è¿è¡Œæ—¶åˆå§‹åŒ–
        B1[åˆ›å»ºIRuntime] --> B2[ååºåˆ—åŒ–Engine]
        B2 --> B3[éªŒè¯é…ç½®ä¸Engineä¸€è‡´æ€§]
        B3 --> B4[åˆ›å»ºExecutionContext<br>å¹¶è®¾ç½®DeviceMemory]
        B4 --> B5[è®¾ç½®ä¼˜åŒ–Profileç´¢å¼•]
    end

    subgraph æ¯æ­¥æ¨ç†
        direction TB
        C1[è®¾ç½®åŠ¨æ€è¾“å…¥<br>åœ°å€ & å½¢çŠ¶] --> C2{è®¡ç®—å“ˆå¸Œ<br>ï¼ˆè¾“å…¥åœ°å€/å½¢çŠ¶/LoRAï¼‰}
        C2 --> C3[æŸ¥è¯¢Graphç¼“å­˜]
        C3 -- å·²å­˜åœ¨ --> C4[cudaGraphLaunch]
        C3 -- ä¸å­˜åœ¨ --> C5[enqueueV3]
        C4 --> C6[è¿”å›]
        C5 --> C6
    end

    subgraph èµ„æºæ¸…ç†
        D1[é”€æ¯GraphExec & Graph]
        D2[é”€æ¯Context/Engine/Runtime]
    end

    A4 -.->|åŠ è½½| B2
    B5 --> C1
    C6 -->|å¾ªç¯| C1
    C6 -.->|æœ€å| D1
    D1 --> D2
```

<img src="TensorRT-EDGE-LLM/deepseek_mermaid_20260212_659e03.png" alt="deepseek_mermaid_20260212_659e03" style="zoom: 10%;" />

å¯ä»¥çœ‹åˆ° workflow å¤§è‡´åˆ†ä¸º3ä¸ªéƒ¨åˆ†

1. ç¦»çº¿æ„å»º engine

   è¿™éƒ¨åˆ†ç”± llm builder æ„å»ºï¼Œæˆ–è€…ä¹Ÿå¯ä»¥ä½¿ç”¨ `trtexec` å·¥å…·ï¼Œè·å¾— `llm.engine`ã€‚å…¶ä¸­æ¯”è¾ƒéš¾ç†è§£çš„æ˜¯ `OptimiztionProfile` è¿™ä¸ªæ¦‚å¿µã€‚è¿™ä¸ªæ¦‚å¿µçš„äº§ç”Ÿæ˜¯ç”±äºè¾“å…¥ä¸ºåŠ¨æ€è¾“å…¥ï¼Œæˆ‘ä»¬ä¸çŸ¥é“åŠ¨æ€ dimension çš„å€¼åˆ°åº•æ˜¯å¤šå°‘ã€‚ä¸ºäº†ä¼˜åŒ–è¿™ä¸ªåŠ¨æ€ dimensionï¼Œæˆ‘ä»¬å¯ä»¥ç»™è¿™ä¸ª dimension è®¾ç½®ä¸‰ä¸ªå€¼ï¼š`min, max, opt`ï¼Œå…¶ä¸­æœ€å¤§æœ€å°å€¼å°±æ˜¯è¯¥å€¼è¢«å…è®¸çš„èŒƒå›´ï¼Œè€Œ `opt` å€¼åˆ™æ˜¯ç”¨äº kernel selectionï¼Œå…·ä½“æ¥è¯´ TensorRT ä¼šç›´æ¥ä½¿ç”¨è¿™ä¸ªå€¼ä½œä¸º inputï¼Œæ¥æµ‹è¯•å“ªä¸ª kernel çš„é€Ÿåº¦æ˜¯æœ€å¿«çš„ã€‚ä¸è¿‡è¿™ä¸ªå€¼å¯¹äºä»Šå¤©çš„ LLM æ¥è¯´ä¸æ˜¯å¾ˆé‡è¦ï¼Œä»Šå¤©çš„ kernel éƒ½å¯ä»¥é€šè¿‡ heuristic ç®—æ³•åœ¨çº¿è¿›è¡Œå†³å®šå“ªä¸ª kernel æ˜¯è¾ƒä¼˜çš„

   è¿™ä¸ª `OptimizationProfile` å¯ä»¥ä¿å­˜å¤šä¸ªï¼Œåœ¨ä¹‹åæ¨ç†çš„æ—¶å€™åˆ©ç”¨ engine ç”Ÿæˆå¤šä¸ª context (e.g. prefill & decode context)ï¼Œæ¯ä¸€ä¸ª context ä½¿ç”¨ä¸åŒçš„ profileï¼Œä»¥åˆ©ç”¨ä¸åŒçš„ kernel è·å¾—æ›´å¥½çš„åŠ é€Ÿæ•ˆæœ

2. è¿è¡Œæ—¶åˆå§‹åŒ–

   ä½¿ç”¨ä¹‹å‰æ„å»ºå¥½çš„ engineï¼Œæ„å»ºä¸åŒçš„ context å¹¶ç»™ä»–ä»¬é…ç½® device Memory ç”¨äºå­˜å‚¨è®¡ç®—è¿‡ç¨‹ä¸­çš„ activation & workspaceã€‚context æ‰æ˜¯ TensorRT æ¨ç†çš„çœŸå®ä¸»ä½“ï¼Œå¹¶å¯¹ä¸åŒçš„ context é…ç½®ç›¸åº”çš„çš„ optimization profileã€‚åœ¨è¿™é‡Œæˆ‘æƒ³æŠŠ engine å’Œ context çš„æ¦‚å¿µä»‹ç»å¾—æ›´æ¸…æ¥šä¸€ç‚¹ï¼š

   1. engineï¼šæ˜¯ç”± TensorRT ç¼–è¯‘ä¼˜åŒ–è¿‡åçš„**æ¨¡å‹**ï¼Œå…¶åŒ…å«**æƒé‡ã€ç½‘ç»œç»“æ„ã€ä¼˜åŒ–ç­–ç•¥**ï¼Œå¯è¢«åºåˆ—åŒ–åˆ° disk å½“ä¸­
   2. contextï¼šæ‰§è¡Œæ¨ç†çš„è¿è¡Œæ—¶ç¯å¢ƒï¼ŒåŒ…å«æ¿€æ´»å†…å­˜ã€ä¸´æ—¶å·¥ä½œç©ºé—´ã€‚å¯é…ç½®ä¸åŒçš„ profile ä»¥è°ƒç”¨æœ€ä¼˜ kernel æ¥åº”å¯¹ä¸åŒ shape çš„è¾“å…¥
   3. äºŒè€…çš„å…³ç³»ï¼šcontext ç”± engine ç”Ÿæˆï¼Œä¸ engine ç»‘å®šï¼Œæ¯æ¬¡æ¨ç†éœ€åˆ›å»ºæˆ–å¤ç”¨ã€‚ä¸€ä¸ª engine å¯è¢«å¤šä¸ª context å…±äº«

3. æ¨ç†

   é¦–å…ˆæˆ‘å…ˆä»‹ç»ä¸‹ cuda graph æ˜¯ä»€ä¹ˆ

   > From Kimi & DeepSeek
   >
   > CUDA Graph æ—¨åœ¨é™ä½**é‡å¤æ‰§è¡Œçš„ã€ç”±å¤šä¸ª CUDA æ“ä½œï¼ˆå¦‚å†…æ ¸å¯åŠ¨ã€å†…å­˜æ‹·è´ï¼‰ç»„æˆçš„åºåˆ—**æ‰€å¸¦æ¥çš„ CPU ç«¯è°ƒåº¦å¼€é”€ï¼Œå¹¶å…è®¸ GPU å¯¹æ•´ä¸ªè®¡ç®—ä»»åŠ¡è¿›è¡Œå…¨å±€ä¼˜åŒ–
   >
   > åœ¨ä¼ ç»Ÿ CUDA ç¼–ç¨‹ä¸­ï¼Œæ¯ä¸ªæ“ä½œï¼ˆkernel launchã€cudaMemcpy ç­‰ï¼‰éƒ½æ˜¯**ç‹¬ç«‹æäº¤**åˆ° GPU å‘½ä»¤é˜Ÿåˆ—çš„ï¼š
   >
   > - CPU ä¸ºæ¯ä¸ªæ“ä½œæ‰§è¡Œä¸€æ¬¡ API è°ƒç”¨ â†’ é©±åŠ¨å¤„ç† â†’ å‘½ä»¤å…¥é˜Ÿåˆ—ã€‚
   > - å½“ä»»åŠ¡æµå›ºå®šã€åå¤æ‰§è¡Œæ—¶ï¼ˆå¦‚æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­çš„æ¯ä¸€ä¸ªè¿­ä»£ï¼‰ï¼Œè¿™äº›é‡å¤çš„ API è°ƒç”¨ä¼šäº§ç”Ÿå¯è§‚çš„ CPU å¼€é”€ï¼ˆé©±åŠ¨éªŒè¯ã€ä¸Šä¸‹æ–‡åˆ‡æ¢ã€é˜Ÿåˆ—ç»´æŠ¤ç­‰ï¼‰
   >
   > CUDA Graph çš„æ ¸å¿ƒæ€æƒ³ï¼š**å°†ä¸€ç³»åˆ— CUDA æ“ä½œâ€œå›ºåŒ–â€ä¸ºä¸€ä¸ªé™æ€çš„ã€å¯é‡ç”¨çš„å›¾ç»“æ„ï¼Œä¸€æ¬¡æ€§æäº¤ç»™ GPUï¼Œä¹‹ååªéœ€ä¸€æ¡æŒ‡ä»¤å°±èƒ½æ‰§è¡Œæ•´ä¸ªå›¾**ï¼Œä»è€Œæå¤§å‡å°‘ CPU å‚ä¸åº¦

   åœ¨ä½¿ç”¨ cuda graph ä¹‹å‰éœ€è¦å…ˆæ„å»º cuda graphï¼Œç®€å•æ¥è¯´å°±æ˜¯è®© context æ¨ç†ä¸€äº›å›ºå®šå½¢çŠ¶çš„ inputï¼ŒæŠŠè¿™äº›é™æ€çš„å›¾ç»“æ„è®°å½•ä¸‹æ¥ï¼Œä¿å­˜ä¸º cuda graph

   å¦‚æœæˆ‘ä»¬æœ‰ç°æˆçš„ cuda graph (e.g. decode)ï¼Œé‚£ä¹ˆç›´æ¥ä½¿ç”¨ cuda graph è¿›è¡Œæ¨ç†ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ `cotext.enqueue` è¿›è¡Œæ¨ç†

## ONNX Export

### Basic

æˆ‘å¸Œæœ›ä»¥ trt edge ä¸ºä¸€ä¸ª best practice æ ·ä¾‹ï¼Œæ¥æ€»ç»“ onnx export æ­¥éª¤å’Œæ³¨æ„äº‹é¡¹

onnx export çš„å‘½ä»¤å…¶å®åªæœ‰ä¸€è¡Œ

```python
torch.onnx.export(
    model,
    inputs,	# dict of input
    onnx_path,
    export_params=True,
    dynamic_axes=...,
    input_names=...,
    output_names=...,
    opset_version=ONNX_OPSET_VERSION,	# 19 in our case
    do_constant_folding=True,
    dynamo=False
)
```

æ ¸å¿ƒä½œç”¨å°±æ˜¯ä½¿ç”¨ dummy inputs æ¨ç†æ¨¡å‹å‰å‘ï¼Œè®°å½• forward è¿‡ç¨‹ä¸­çš„è®¡ç®—å›¾

å…¶ä¸­æœ‰ä¸‰ä¸ªæ ¸å¿ƒçš„é—®é¢˜ï¼š

1. å¤šä¸ªè¾“å…¥å’Œè¾“å‡ºåªèƒ½æ˜¯ä»¥ tuple çš„å½¢å¼å­˜åœ¨ï¼Œå¦‚ä½•ç»™è¿™äº› tuple ä¸­çš„å…ƒç´ åŠ å…¥åå­—ï¼Œè¿™æ ·æ–¹ä¾¿æˆ‘ä»¬åœ¨ä¹‹åæ‰¾åˆ°è¿™äº›å…ƒç´ å¯¹åº”ä»€ä¹ˆå†…å®¹ï¼ˆæˆ‘ä»¬ä¸æƒ³åªé€šè¿‡é¡ºåºæ•°å­—æ¥ç¡®å®šï¼‰
2. æ¨ç†è¿‡ç¨‹ä¸­ï¼Œinput & output tensor çš„ shape éå¸¸é‡è¦ï¼Œä¼šå½±å“åˆ° TensorRT é€‰æ‹©ç®—å­çš„ç»“æœã€‚å¦‚ä½•è®¾ç½®å“ªäº› axes æ˜¯å›ºå®šçš„
3. æ²¡æœ‰è¢«æ³¨å†Œçš„ç®—å­æ— æ³•è¢«æ¨ç†ï¼Œè®¡ç®—å›¾æ— æ³•æ„å»ºï¼Œä¹Ÿæ— æ³•å¯¼å‡ºä¸º ONNX

å‰ä¸¤ä¸ªé—®é¢˜éƒ½å¾ˆå¥½è§£å†³

1. é€šè¿‡ `input_names & output_names` å¯¹è¾“å…¥å’Œè¾“å‡º tuple è¿›è¡ŒæŒ‡å®šå³å¯ï¼Œä¸€å®šä¸¥æ ¼æŒ‰ç…§ `model.forwar` çš„è¾“å…¥è¾“å‡ºé¡ºåºè¿›è¡ŒæŒ‡å®š

2. é€šè¿‡ `dynamic_axes` æŒ‡å®šåŠ¨æ€è½´

   ```python
   dynamic_axes = {
       "input_ids": {
           0: "batch_size",	# give names to dymaci axes
           1: "seq_len"
       }
   }
   ```

   ç”±æ­¤ç¡®å®šäº† `input_ids` çš„ç¬¬ 0,1 ä¸ª axis éƒ½æ˜¯åŠ¨æ€çš„ï¼Œå¹¶å–åä¸º `batch_size & seq_len`

æœ€åä¸€ä¸ªé—®é¢˜æ˜¯æœ€éš¾çš„ï¼Œéœ€è¦ç›¸å¯¹å¤æ‚çš„å·¥åºï¼š

1. é¦–å…ˆæˆ‘ä»¬éœ€è¦åœ¨ pytorch å®šä¹‰ä¸€ä¸ª dummy custom opï¼Œè¿™æ ·è®¡ç®—å›¾èƒ½å¤Ÿæ­£å¸¸æ¨ç†

   ```python
   @torch.library.custom_op("trt::attention_plugin", mutates_args=())
   def attention_plugin(...):
       # Dummy implementation for ONNX export, this is not used in the actual inference
       return attn_output, past_key_value.clone()
   ```

2. å…¶æ¬¡æˆ‘ä»¬éœ€è¦åœ¨ ONNX å½“ä¸­å®šä¹‰ä¸€ä¸ª custom opï¼Œè¿™æ · ONNX åœ¨å¯¼å‡ºè®¡ç®—å›¾çš„æ—¶å€™èƒ½ä½¿ç”¨è¯¥ op ä½œä¸ºèŠ‚ç‚¹

   ```python
   def register_attention_plugin_onnx_symbolic_functions() -> None:
       """Register symbolic functions for ONNX export."""
   
       # Register our custom symbolic functions
       register_custom_op_symbolic("trt::attention_plugin",
                                   symbolic_attention_plugin, ONNX_OPSET_VERSION)
   ```

   æ„Ÿè§‰è¿™å·²ç»æ˜¯ä¸Šå¤æ—¶ä»£çš„æ–¹å¼äº†ï¼Œç›¸å…³æ•™ç¨‹æœ€å¥½çš„å¯èƒ½è¿˜æ˜¯ OpenMMLab å‡ºçš„ï¼Œç°åœ¨æ ¹æœ¬æ‰¾ä¸åˆ°å®˜æ–¹çš„æ•™ç¨‹ï¼Œtorch éƒ½å‡†å¤‡å¼ƒç”¨è¿™äº›æ¥å£äº†

   åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­æˆ‘å‘ç°å¾ˆå¤šå¸®åŠ© onnx æ„å»ºå›¾çš„å‚æ•°ä¼¼ä¹éƒ½æ²¡æœ‰å¿…è¦ï¼Œä¾‹å¦‚ schema å¯ä»¥ä¸è¢«æ³¨å†Œ

   ```python
   onnx.defs.register_schema(attention_plugin_schema)
   ```

   å†ä¾‹å¦‚å¯ä»¥ä¸è®¾ç½®è¾“å‡º tensor type shapeï¼Œè¿™åªæ˜¯ ONNX æƒ³è¦ä½ è®¾ç½®çš„ shapeï¼Œä»¥è®©å…¶æ„Ÿåˆ°æ”¾å¿ƒğŸ˜‚ï¼Œä¸å½±å“å¯¼å‡º ONNX çš„æ­£ç¡®æ€§ã€‚å¯¹äº TensorRT æ¥è¯´è¿™äº› shape éƒ½åœ¨å¯¼å‡ºçš„è®¡ç®—å›¾ä¸­åŒ…å«äº†ï¼Œæˆ‘ä¼°è®¡æ˜¯æˆ‘ä»¬æ‰€å†™çš„ torch dummy custom op èµ·äº†ä½œç”¨

   ```python
   attn_output.setType(qkv_type.with_sizes(attn_output_sizes))
   
   # IF NOT SET, WARNING WOULD BE GIVEN
   # Warning: The shape inference of trt::AttentionPlugin type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (function UpdateReliable)
   ```

   ä½†æ˜¯ `symbolic_helper.parse_args` è¿™ä¸ªè£…é¥°å™¨å¿…é¡»è¦æ·»åŠ ï¼Œå¦åˆ™æ‰€æœ‰çš„ input å…¨éƒ¨éƒ½æ˜¯ä»¥ tensor value çš„å½¢å¼å­˜åœ¨ï¼Œåœ¨åˆ¤æ–­ if-else çš„æ—¶å€™å¿…é¡»ä½¿ç”¨ boolï¼Œå¦‚æœæ˜¯ tensor value çš„è¯åˆ¤æ–­æ°¸è¿œä¸º trueã€‚å…¶å®è¿™ä¸ªæ§åˆ¶æµå®Œå…¨å¯ä»¥æ”¾åˆ° plugin å†…éƒ¨å»åš

   ```python
   @symbolic_helper.parse_args("v", "v", "v", "v", "v", "i", "i", "b", "i", "v", "v")
   def symbolic_attention_plugin(
       g: torch.onnx._internal.torchscript_exporter.jit_utils.GraphContext,
       qkv: torch._C.Value,
       past_key_value: torch._C.Value,
       context_lengths: torch._C.Value,
       rope_rotary_cos_sin: torch._C.Value,
       kvcache_start_index: torch._C.Value,
       num_q_heads: torch._C.Value,
       num_kv_heads: torch._C.Value,
       enable_tree_attention: torch._C.Value,
       head_size: torch._C.Value,
       attention_mask: Optional[torch._C.Value] = None,
       position_ids: Optional[torch._C.Value] = None,
   ):
       """Custom attention plugin operation for ONNX export."""
   
       # Build inputs list - kvcache_start_index is now always required
       inputs = [
           qkv, past_key_value, context_lengths, rope_rotary_cos_sin,
           kvcache_start_index
       ]
       if enable_tree_attention:
           assert attention_mask is not None and attention_mask.type().kind(
           ) != 'NoneType', "attention_mask should be provided for tree attention"
           assert position_ids is not None and position_ids.type().kind(
           ) != 'NoneType', "position_ids should be provided for tree attention"
           inputs.append(attention_mask)
           inputs.append(position_ids)
   
       qkv_type = qkv.type()
       past_key_value_type = past_key_value.type()
       attn_output, present_key_value = g.op(
           "trt::AttentionPlugin",
           *inputs,
           num_q_heads_i=num_q_heads,
           num_kv_heads_i=num_kv_heads,
           head_size_i=head_size,
           enable_tree_attention_i=1 if enable_tree_attention else 0,
           outputs=2)
   ```

   å¦å¤–é™¤äº† `outputs` å¤–çš„ kwargs å…¨éƒ¨æ˜¯ plugin çš„ attributeï¼Œå¿…é¡»ä»¥ `_i` ç»“å°¾ï¼Œå¦åˆ™ä¹Ÿä¼šæŠ¥é”™

### Export LLM

EDGE-LLM åœ¨å¯¹ huggingface LLM è¿›è¡Œ export æ—¶è¿›è¡Œäº†å°è£…ï¼Œåˆ©ç”¨ `EdgeLLMModelForCausalLM` æ¥ç»Ÿä¸€æ¨¡å‹è¾“å…¥å’Œè¾“å‡ºå½¢å¼ã€‚èƒ½å¤Ÿç»Ÿä¸€èµ·æ¥å¤šäºäº†ç°åœ¨çš„ LLM çš„è¾“å…¥ã€è¾“å‡ºæ ¼å¼çš„é«˜åº¦ä¸€è‡´ï¼Œå¯¹äºæ›¾ç»çš„ cv æ¨¡å‹é‚£å«ä¸€ä¸ªç™¾èŠ±é½æ”¾ã€‚æˆ‘ä»¬å¦‚æœä½¿ç”¨è‡ªå·±çš„ LLM æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥éµå¾ª EDGE-LLM çš„å°è£…

## HF custom model

å¯ä»¥æ ¹æ® Janus çš„å†™æ³•æ€»ç»“ä¸€ä¸ªæ„å»º custom huggingface model çš„æœ€ä½³å®è·µ

## Plugin

How to build a basic plugin, BEST PRACTICE

Tricks

### TensorRT Basic Usage

trt çš„ python & cpp api éƒ½æ˜¯ç›¸ä¼¼çš„ï¼Œæˆ‘åº”è¯¥ç®€å•æ•´ç†ä¸€ä¸‹ï¼Œè¿™æ ·æ‰èƒ½åœ¨ python é‡Œå®Œæˆé‡åŒ–çš„å¯¹æ¯”



## Apis

- å…¥å£æ˜¯ `LLMInferenceRuntime::handleRequest`

Edge-llm è‡ªå·±æ„å»ºäº†ä¸€ä¸ªè½»é‡ tensor æŠ½è±¡ï¼Œæˆ‘è§‰å¾—æŒºä¸é”™çš„

è‡ªå·±æ„å»ºäº†ä¸€ä¸ª linear kvcache æ¥ç®¡ç† kv cacheï¼Œä¹ŸæŒºä¸é”™çš„

EngineRunner & InferenceRunner çš„åŠŸèƒ½

## Questions & Misc

- timer system åœ¨ llm inference example é‡Œä¹Ÿæœ‰

- å¦‚æœå…³é—­ cuda graphï¼Œè¿™æ ·æˆ‘å°±å¯ä»¥çœ‹çœŸå®çš„ nsys profile äº†

- TensorRT åœ¨å¯¼å‡ºçš„æ—¶å€™éœ€è¦é…åˆ ONNX è®¾ç½®å¥½å“ªäº›æ˜¯ inputï¼Œè€Œå“ªäº›æ˜¯ outputï¼Œè¿™ä¹Ÿéœ€è¦æˆ‘ä»”ç»†æ•´ç†

- Myelin èåˆèŠ‚ç‚¹ä¼¼ä¹å¹¶ä¸æ˜¯ä¸‡èƒ½é’¥åŒ™ï¼Œæˆ‘æµ‹å‡ºæ¥çš„ rmsnorm æ¯” myelin çš„è¦å¿«å¾ˆå¤š

- è¿˜å¥½è‡ªå·±æœ‰ä¹‹å‰ MLC-LLM çš„ç»å†ï¼Œæ„Ÿè§‰è¿™äº›ä»£ç éƒ½èƒ½æ¯”è¾ƒå¿«é€Ÿåœ°ç†è§£ï¼Œè€Œä¸”è¿™é‡Œçš„ä»£ç ä¼¼ä¹æ¯” MLC-LLM æ›´åŠ è½»é‡ï¼Œç¡®å®é€‚åˆä½œä¸ºç«¯ä¾§çš„æ¨ç†æ¡†æ¶ï¼Œæ„å»ºå¾—ä¹Ÿæ¯”è¾ƒé€šç”¨ï¼Œåº”è¯¥å¯ä»¥æ‰©å±•åˆ°é™¤äº† thor ä¹‹å¤–çš„å…¶ä»–ç«¯ä¾§ GPUã€‚å¹¶ä¸”æœ‰äº† TensorRT çš„æ”¯æŒï¼Œå¯ä»¥ç›´æ¥ä» ONNX Parse å‡ºè®¡ç®—å›¾ï¼Œåœ¨é¡¹ç›®åˆæœŸæœ‰ä¸é”™çš„ä¼˜åŠ¿