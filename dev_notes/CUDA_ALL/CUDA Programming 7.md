# CUDA Programming 7

æœ‰äº†ä¹‹å‰ CUDA Programming 1-6 çš„é“ºå«ï¼Œå¯¹äº CUDA åŸºç¡€åº”è¯¥æœ‰äº†ä¸€å®šçš„äº†è§£ã€‚ç°åœ¨æƒ³è¦å¹²ä¸€äº›æœ‰è¶£çš„äº‹æƒ…

æƒ³å¿…ç»å¤§éƒ¨åˆ†åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸçš„äººéƒ½å¬è¯´è¿‡ [FlashAttention](https://github.com/Dao-AILab/flash-attention)ï¼Œå…¶æ˜¯ä¸€ä¸ª fast & memory efficient çš„ attention å®ç°ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œä¹Ÿæœ‰éå¸¸å¤šçš„äººå¬è¯´è¿‡ [FlashInfer](https://github.com/flashinfer-ai/flashinfer)ï¼Œè¯¥é¡¹ç›®ä¹Ÿæ˜¯ä¸€ä¸ªç”¨äºåŠ é€Ÿ LLM serving çš„ libraryã€‚è€Œä½ å»æŸ¥çœ‹ä»–ä»¬çš„ä»“åº“æ—¶éƒ½ä¼šå‘ç°ä¸€ä¸ªå…±åŒçš„ 3rdparty ä»“åº“ï¼š[CUTLASS](https://github.com/NVIDIA/cutlass)

æˆ‘çš„æœ¬æ„æ˜¯æƒ³è¦å°† FlashInfer ä»“åº“è¿›è¡Œæ·±å…¥çš„å­¦ä¹ ï¼Œä½†æ˜¾ç„¶åœ¨è¿™ä¹‹å‰è¿˜æœ‰å¤§é‡çš„ cutlass çŸ¥è¯†éœ€è¦åšé“ºå«ã€‚æ‰€ä»¥æˆ‘å…ˆè¿›è¡Œ cutlass çš„å­¦ä¹ ï¼Œå†è¿›è¡Œ flashinfer çš„å­¦ä¹ 

ä½†é—®é¢˜åœ¨äºï¼šcutlass ä¼¼ä¹æ²¡æœ‰ç‰¹åˆ«å¥½çš„æ•™ææ¥å¸®åŠ©å…¥é—¨ã€‚ç›®å‰æˆ‘æœ‰ä¸€äº›åˆ‡å…¥å£ï¼š

1. CUDA MODE lecture 15 introduced cutlass a little bit
2. CUTLASS examples
3. CUTE tutorial

é€šè¿‡å­¦ä¹  cutlass çš„ä¾‹å­æ¥æŒæ¡å…¶ç”¨æ³•ï¼Œ**æŒæ¡ç”¨æ³•æ˜¯æœ€ä¸»è¦çš„éœ€æ±‚ï¼Œä¹Ÿæ˜¯æœ€ç›´æ¥çš„åé¦ˆ**

Other zhihu references:

- [Reed's zhihu posts](https://www.zhihu.com/people/reed-84-49/posts), and its gemm code [github](https://github.com/reed-lau/cute-gemm)
- [CUTLASS CuTeå®æˆ˜(ä¸€)-åŸºç¡€](https://zhuanlan.zhihu.com/p/690703999)
- [CUTLASS CuTeå®æˆ˜(äºŒ)-åº”ç”¨](https://zhuanlan.zhihu.com/p/692078624)
  - [github](https://github.com/zeroine/cutlass-cute-sample)
- [cutlass cute 101](https://zhuanlan.zhihu.com/p/660379052)
- A collective repo which gathers a lots of blogs and kenel impl [CUDA-Learn-Notes](https://github.com/DefTruth/CUDA-Learn-Notes), not suitable for system learning, can be used for look-up table if you are trying to seek for some topic

## Install & Hello World

Good news! CUTLASS does not need to be built!!!

> CUTLASS is a header-only template library and does not need to be built to be used by other projects. Client applications should target CUTLASS's `include/` directory in their include paths.

ä½†ä¸ºäº†è¿è¡Œä¸€äº› example codeï¼Œæˆ‘ä»¬å¿…é¡»è¦è¿›è¡Œç¼–è¯‘ï¼Œæ‰èƒ½ run èµ·æ¥

Hello World in CUTE

[sgemm_1.cu](https://github.com/NVIDIA/cutlass/blob/main/examples/cute/tutorial/sgemm_1.cu)

[quick start guide](https://github.com/NVIDIA/cutlass/blob/main/media/docs/quickstart.md)

[quick start guide-cute](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/00_quickstart.md)

æ ¹æ® [quick start guide](https://github.com/NVIDIA/cutlass/blob/main/media/docs/quickstart.md) ä¸­æç¤ºï¼Œæˆ‘å…ˆè¯•ç”¨ cmake å¹¶ä¸”æŒ‡å®šè‡ªå·±çš„ compute capacity

```shell
# see your compute capacity with command:
# nvidia-smi --query-gpu=compute_cap --format=csv
mkdir build && cd build
cmake .. -DCUTLASS_NVCC_ARCHS=86
```

ä¼¼ä¹åº”è¯¥ä½¿ç”¨ `-DCUTLASS_NVCC_ARCHS=80`? å³ä½¿æˆ‘çš„ compute capacity æ˜¯86

åœ¨è¿è¡Œå®Œè¿‡åå¯ä»¥çœ‹åˆ° `build/Makefile` ä¸­ä¼šæœ‰è®¸å¤šæˆ‘ä»¬å¯ build çš„æ–‡ä»¶ï¼Œå…¶ä¸­ `examples` ä¸‹çš„æ‰€æœ‰æ–‡ä»¶éƒ½å¯ä»¥åœ¨é‡Œé¢æœåˆ° (e.g. `00_basic_gemm`)ï¼Œè€Œæˆ‘è¦ä½¿ç”¨çš„å°±æ˜¯ `sgemm_1` 

```shell
make sgemm_1 -j8
```

è¿è¡Œå®Œè¿‡åå°±å¯ä»¥åœ¨ `build/examples/cute/tutorial/sgemm_1` æ‰¾åˆ°ï¼Œè¿è¡Œ

```shell
# under the `build` dir
./examples/cute/tutorial/sgemm_1
```

```shell
âœ  cutlass git:(main) ./build/examples/cute/tutorial/sgemm_1
M = 5120
N = 5120
K = 4096
C = A^N B^T
Using device 0: NVIDIA GeForce RTX 3080  (SM86, 68 SMs)
CUTE_GEMM:     [11452.9]GFlop/s  (18.7506)ms
```

è¯¥ä»£ç çš„æºç å°±åœ¨ `./examples/cute/tutorial/sgemm_1.cu`ã€‚äºæ­¤åŒæ—¶æˆ‘æµ‹è¯•äº†ä¸€ä¸‹åŒç­‰æ¡ä»¶ä¸‹ cuBLAS çš„ latency åªéœ€è¦ 10.89 msï¼Œæ‰€ä»¥ä¸Šé¢çš„ sgemm ä»ç„¶æ˜¯ä¸€ä¸ªéå¸¸æ…¢ gemmï¼Œè¯´æ˜ä¼˜åŒ–ç©ºé—´è¿˜éå¸¸å¤§ğŸ‘€

## A Closer Look

ç°åœ¨æ¥çœ‹ä¸‹ `sgemm_1.cu` åˆ°åº•å¹²äº†ä»€ä¹ˆäº‹æƒ…ä¹Ÿè®¸æ˜¯ä¸ªä¸é”™çš„é€‰æ‹©ï¼Œè¯¥æ–‡ä»¶ä¹Ÿå°±æ˜¯ä¸€ä¸ª 400 å¤šè¡Œçš„ä»£ç ï¼Œæˆ‘æ¥è´´ä¸€éƒ¨åˆ†ä»£ç 

```c++
// to be paste later...
// not so convinient to see the whole picture
```

å¯ä»¥çœ‹åˆ°æ•´ä¸ªä»£ç åˆ†ä¸º4ä¸ªéƒ¨åˆ†ï¼š

1. `gemm_device` æ˜¯ cutlass gpu kernel çš„æ ¸å¿ƒå®ç°
2. `gemm_nt & gemm_tn`ï¼Œè°ƒç”¨ `gemm_devie` å®ŒæˆçŸ©é˜µä¹˜æ³•
3. `gemm` å°±æ˜¯ä¸€ä¸ª wrapperï¼ŒåŒ…è£¹ `gemm_nt & gemm_tn`ï¼Œå…¶åŒºåˆ«äº `cute::gemm`ï¼Œæˆ–è®¸åº”è¯¥æ¢ä¸€ä¸ªåå­—
4. `main` å³ä¸º host ä¸»ç¨‹åºçš„è¿è¡Œï¼ŒåŒ…å«æµ‹é‡ Flops & latency

###  gemm_device

åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»æœªå­¦ä¹ è¿‡å¦‚ä½•ä½¿ç”¨ GPU æ¥è¿›è¡ŒçŸ©é˜µè¿ç®—ï¼Œåªå¯¹ç®€å•çš„ reduce & tranpose åšè¿‡å­¦ä¹ ã€‚é‚£ä¹ˆé—®é¢˜æ¥äº†ï¼šä¸ºä½•çŸ©é˜µä¹˜æ³•é€‚åˆäºå¹¶è¡Œè¿ç®—ï¼Ÿè¿™æ˜¯å› ä¸ºå¯ä»¥ç‹¬ç«‹åœ°è®¡ç®—æ¯ä¸€ä¸ªå…ƒç´ 
$$
C_{ij}=\Sigma_k A_{ik}B_{kj}
$$
è¿™ä¸ªå…¬å¼å¯ä»¥ç”¨å›¾åƒéå¸¸å½¢è±¡åœ°è¡¨è¾¾

<img src="CUDA Programming 7/image-20241128111348874.png" alt="image-20241128111348874" style="zoom: 50%;" />

è“è‰²éƒ¨åˆ†çš„çŸ©é˜µä¹˜ç§¯ç»“æœï¼Œç”±ç»¿è‰²å’Œé»„è‰²éƒ¨åˆ†çš„çŸ©é˜µçš„ç‚¹ç§¯å’Œå¾—åˆ°ã€‚`gemm_device` æ‰€é‡‡ç”¨çš„å°±æ˜¯è¿™æ ·æœ´ç´ çš„æ€ç»´ï¼Œæ¯”ä¸è¿‡ cuBLAS ä¹Ÿæ˜¯æƒ…æœ‰å¯åŸçš„ï¼Œä¸‹é¢å…·ä½“åœ°è®¨è®ºæ•´ä¸ªè¿‡ç¨‹ï¼Œå³ï¼šå¦‚ä½•å°†æ•°æ®åˆ†é…åˆ°å„ä¸ª block/warp/thread å½“ä¸­ï¼Œå¹¶è¿›è¡Œè®¡ç®—ï¼Œæ­¤å¤„å‚è€ƒ [cutlass cute 101](https://zhuanlan.zhihu.com/p/660379052) [0x_gemm_tutorial.md](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/0x_gemm_tutorial.md)

- What does this code trying to do logically?

  è¿™åŸºæœ¬ä¸Šå°±æ˜¯åœ¨æ•™ä½ å¦‚ä½•ç”¨å¹¶è¡Œçš„æ€ç»´å»å¤„ç†çŸ©é˜µä¹˜æ³•ã€‚æˆ‘å…ˆä»é€»è¾‘è§†è§’å®Œæˆè¿™ä»¶äº‹æƒ…ï¼Œç„¶åå†å°†è¿™äº›é€»è¾‘æ˜ å°„åˆ°ä»£ç å½“ä¸­ï¼Œçœ‹ä¸‹ä»£ç çš„å…·ä½“å®ç°

  ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬é¦–å…ˆå®šä¹‰é—®é¢˜ä¸ºæœ€ç®€å•çš„å•ç²¾åº¦çŸ©é˜µä¹˜æ³•ï¼š$C = AB$ï¼Œä»–ä»¬å½¢çŠ¶åˆ†åˆ«ä¸º `A(M,K) & B(K,N), C(M,N)`ï¼Œä¸ºäº†è®©é—®é¢˜æ›´åŠ å…·è±¡åŒ–ï¼Œæˆ‘ä»¬ä»¥ `M=N=5120, K=4096` ä¸ºä¾‹å­ï¼ˆè¿™ä¹Ÿæ˜¯ cutlass ä»£ç ä¾‹å­ä¸­æ‰€ä½¿ç”¨çš„æ•°å€¼ï¼‰

  ä¸€ä¸ªä¸é”™çš„åˆ’åˆ†è§†è§’æ˜¯ä»¥çŸ©é˜µ $C$ ä¸ºæ ¸å¿ƒï¼šæˆ‘ä»¬å°†çŸ©é˜µ C è¿›è¡Œåˆ’åˆ†ï¼Œä»¥ $(128,128)$ çš„æ–¹å—ä½œä¸ºåˆ’åˆ†å•ä½ï¼Œå»å•ç‹¬æ±‚è§£æ¯ä¸€ä¸ªæ–¹å—ä¸­çš„çŸ©é˜µä¹˜æ³•ç»“æœã€‚ä» CUDA ç¼–ç¨‹çš„è§’åº¦æ¥è®²ï¼šæˆ‘ä»¬è®©ä¸€ä¸ª block å»å¤„ç†ä¸€ä¸ª $(128,128)$ çš„çŸ©é˜µä¹˜æ³•ç»“æœ

  <img src="CUDA Programming 7/image-20241203155920569.png" alt="image-20241203155920569" style="zoom:80%;" />

  å¥½ï¼Œç°åœ¨å°±å¯ä»¥é›†ä¸­ç²¾åŠ›æ¥å¤„ç†æ¯ä¸€ä¸ª block åº”å½“å¦‚ä½•è®¡ç®—äº†ï¼Œå³ï¼šè®¡ç®—ä¸€ä¸ª $(128,128)$ çš„çŸ©é˜µ C åº”è¯¥å¦‚ä½•åšåˆ°ï¼Ÿé¦–å…ˆæˆ‘ä»¬éœ€è¦è·å–çŸ©é˜µ A & B ä¸­å¯¹åº”çš„æ•°æ®ï¼Œåˆ†åˆ«è·å¾—å¯¹åº”çš„ $(128, 4096)$â€‹â€‹ å¤§å°çš„æ•°æ®ï¼ˆå³ä¸Šå›¾ä¸­çš„ `gA & gB`ï¼Œ`gC` ä¹Ÿåœ¨å›¾é‡Œï¼Œ`g` ä»£è¡¨çš„æ˜¯ global memoryï¼Œè¯·å¿½ç•¥å›¾ä¸­é”™è¯¯çš„åˆ†å—æ•°é‡ï¼Œå› ä¸ºå®åœ¨ç”»ä¸äº†...ï¼‰

  å…¶å®ç°åœ¨å°±å¯ä»¥åšçŸ©é˜µä¹˜ï¼Œå¾—åˆ°æˆ‘ä»¬æƒ³è¦çš„ç»“æœï¼š
  $$
  gC=gA \times gB
  $$
  ä½†æ˜¯è¿™é‡Œæ˜¯ GPUï¼æˆ‘ä»¬å¦‚æœæ¯æ¬¡éƒ½ä» global memory è¯»å–æ•°æ®çš„è¯ï¼Œæ—¶å»¶ä¼šéå¸¸å¤§ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å…ˆæŠŠæ•°æ®è¯»åˆ° shared memory é‡Œé¢ï¼Œè¿™æ ·è¯»å–æ•°æ®åšè®¡ç®—çš„æ—¶å€™ä¼šæ›´å¿«ã€‚ä½†æ˜¯ shared memory åˆæ²¡æœ‰è¿™ä¹ˆå¤§çš„ç©ºé—´ï¼Œæ¯ä¸€ä¸ª block éœ€è¦åˆ†é…~ 100K*4Byte å¤§å°çš„ç©ºé—´ï¼Œè¿™å¤ªå¤šäº†ï¼å¥½æ¶ˆæ¯æ˜¯ï¼šæˆ‘ä»¬è¿˜å¯ä»¥å°†è¿™ä¸ªé—®é¢˜ç»§ç»­è¿›è¡Œåˆ‡åˆ†

  æˆ‘ä»¬æ²¿ç€ K è½´æ–¹å‘ï¼ŒæŠŠ 4096 åˆ‡åˆ†æˆä¸º $(512,8)$ çš„å½¢çŠ¶ï¼Œæˆ‘ä»¬æ¯æ¬¡åš $(128,8)$ å¤§å°çš„çŸ©é˜µä¹˜æ³•ï¼Œç„¶åè¿›è¡Œäº†ç´¯åŠ ä¹Ÿèƒ½å¤Ÿå¾—åˆ°ç›¸åŒçš„ç»“æœã€‚ç»è¿‡åˆ‡åˆ†è¿‡åæˆ‘ä»¬çš„è®¡ç®—è¿‡ç¨‹å˜ä¸ºäº†
  $$
  gC=\Sigma_{i=1}^{512}sA\times sB
  $$
  <img src="CUDA Programming 7/image-20241203160014235.png" alt="image-20241203160014235" style="zoom:80%;" />

  æœ€åæˆ‘ä»¬å°±éœ€è¦è€ƒè™‘å°†è¿™ä¸ª block level çš„é—®é¢˜ï¼Œåˆ‡åˆ†åˆ° thread level ä¸Šï¼Œè€ƒè™‘æ¯ä¸€ä¸ª thread åº”è¯¥å¹²çš„å·¥ä½œã€‚è€ƒè™‘ä¸€ä¸ª block æœ‰ 256 ä¸ª threadï¼Œæˆ‘ä»¬å°†è¿™ä¸ª thread æ’åˆ—æˆä¸º $(16,16)$ å½¢çŠ¶ï¼Œè¿™æ ·ä¸€æ¬¡å°±å¯ä»¥å¤„ç† $(16,16)$ å¤§å°çš„çŸ©é˜µã€‚

  ä¸ºäº†æ–¹ä¾¿ç”¨å›¾å½¢è¡¨ç¤ºï¼Œæˆ‘æ²¡åŠæ³•ç”»è¿™ä¹ˆå¤§çš„å›¾ï¼Œæˆ‘å°†ç”¨å½¢çŠ¶ `gC.shape=(8,8) & sA.shape=sB.shape=(8,4) & thread.shape=(4,4)` æ¥ç¤ºæ„æ¯ä¸€ä¸ªçº¿ç¨‹æ‰€è¦åˆ†é…çš„æ•°æ®ï¼Œä»¥åŠæœ€ç»ˆè®¡ç®—ç»“æœ [reference: math-partitioning](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/0x_gemm_tutorial.md#math-partitioning)

  <img src="CUDA Programming 7/image-20241203160059324.png" alt="image-20241203160059324" style="zoom: 67%;" />

  æˆ‘åœ¨ threads ä¸­é€‰æ‹©äº†5ä¸ª thread æ¥è¿›è¡Œè¡¨ç¤ºï¼Œæ¯ä¸€ä¸ª thread ç”¨ä¸åŒçš„é¢œè‰²ã€‚è™½ç„¶çœ¼èŠ±ç¼­ä¹±çš„ï¼Œä½†æ˜¯å°±æ˜¯çŸ©é˜µä¹˜æ³•æ‰€éœ€è¦çš„æ•°æ®ï¼Œä¸“æ³¨ä¸€ä¸ªé¢œè‰²çš„å°±OKã€‚ç¬¬ä¸€è¡Œçº¢è‰²å’Œæ©™è‰²çš„æ–¹å—éƒ½éœ€è¦è¯» `sA` ä¸­ç¬¬ä¸€è¡Œçš„æ•°æ®ï¼Œæ‰€ä»¥æˆ‘å¤šç”»äº†ä¸€è¡Œå‡ºæ¥ï¼Œ`sB` ä¸­å¤šå‡ºæ¥çš„ä¸€åˆ—ä¹Ÿæ˜¯è¿™ä¸ªé“ç†ã€‚æ¯ä¸€ä¸ª thread å°†ä¼šå»è¯»å– $sA(2,4)$ ä»¥åŠ $sB(2,4)$ å¤§å°çš„çŸ©é˜µç”¨äºçŸ©é˜µä¹˜æ³•

  æ‰€ä»¥å½“ç†è§£äº†å›¾ä¸­çš„çº¿ç¨‹åˆ†é…æ–¹å¼è¿‡åï¼Œç†è§£æˆ‘ä»¬ä¾‹å­å½“ä¸­çš„ï¼šå°† $(128, 128)$ å¤§å°çš„çŸ©é˜µåˆ†é…åˆ° $(16,16)$ å¤§å°çš„ threads ä¸­ï¼Œå°±éå¸¸å®¹æ˜“äº†ã€‚æ¯ä¸€ä¸ªå•ç‹¬çš„ thread æ€»å…±ä¼šå¤„ç† $gC(8,8)$ å¤§å°çš„çŸ©é˜µï¼Œå¹¶ä¸”æ¯ä¸€æ¬¡éƒ½ä¼šè·å– $sA(8,8)$ ä»¥åŠ $sB(8,8)$â€‹ å¤§å°çš„æ•°æ®ç”¨äºçŸ©é˜µä¹˜æ³•

- What are the cutlass codes?

  ç»è¿‡ä¸Šé¢çš„é€»è¾‘åˆ†ææ•´ä¸ªåˆ‡åˆ†è„‰ç»œå·²ç»å¾ˆæ¸…æ™°äº†ï¼š

  1. å°†æ•´ä¸ªçŸ©é˜µè¿›è¡Œåˆ‡åˆ†ï¼Œåˆ’åˆ†æˆä¸ºå¤šä¸ª blocks
  2. å†å°†çŸ©é˜µä¹˜æ³•æ²¿ç€ K è½´è¿›è¡Œåˆ‡åˆ†ï¼Œä»¥æ»¡è¶³ shared memory è¦æ±‚ï¼Œåˆ©ç”¨å¾ªç¯ç´¯åŠ çš„æ–¹å¼å®ŒæˆçŸ©é˜µä¹˜æ³•
  3. æœ€åå°†çŸ©é˜µä½¿ç”¨ threads é˜Ÿåˆ—è¿›è¡Œåˆ’åˆ†ï¼Œç»™æ¯ä¸€ä¸ª thread åˆ†é…æ•°æ®ï¼Œæ˜¯æœ€ç»ˆçŸ©é˜µä¹˜æ³•çš„æ‰§è¡Œè€…

  æˆ‘ç›´æ¥è´´ä»£ç äº†ï¼Œæ•´ä¸ªè¿‡ç¨‹è·Ÿç€æ³¨é‡Šçœ‹ä¹Ÿéå¸¸çš„æ¸…æ™°æ˜äº†

  ```c++
  // Setup params for an NT GEMM
  // Use m-major smem sA, n-major smem sB, and mn-major threads tA|tB
  template <class TA, class TB, class TC,
            class Alpha, class Beta>
  void
  gemm_nt(int m, int n, int k,
          Alpha alpha,
          TA const* A, int ldA,
          TB const* B, int ldB,
          Beta beta,
          TC      * C, int ldC,
          cudaStream_t stream = 0)
  {
    using namespace cute;
  
    // Define shapes (dynamic)
    auto M = int(m);
    auto N = int(n);
    auto K = int(k);
    auto prob_shape = make_shape(M, N, K);                     // (M, N, K)
  
    // Define NT strides (mixed)
    auto dA = make_stride(Int<1>{}, ldA);                      // (dM, dK)
    auto dB = make_stride(Int<1>{}, ldB);                      // (dN, dK)
    auto dC = make_stride(Int<1>{}, ldC);                      // (dM, dN)
  
    // Define CTA tile sizes (static)
    auto bM = Int<128>{};
    auto bN = Int<128>{};
    auto bK = Int<  8>{};
    auto cta_tiler = make_shape(bM, bN, bK);                   // (BLK_M, BLK_N, BLK_K)
  
    // Define the smem layouts (static)
    auto sA = make_layout(make_shape(bM, bK));                 // (m,k) -> smem_idx; m-major
    auto sB = make_layout(make_shape(bN, bK));                 // (n,k) -> smem_idx; n-major
    auto sC = make_layout(make_shape(bM, bN));                 // (m,n) -> smem_idx; m-major
  
    // Define the thread layouts (static)
    auto tA = make_layout(make_shape(Int<32>{}, Int< 8>{}));   // (m,k) -> thr_idx
    auto tB = make_layout(make_shape(Int<32>{}, Int< 8>{}));   // (n,k) -> thr_idx
    auto tC = make_layout(make_shape(Int<16>{}, Int<16>{}));   // (m,n) -> thr_idx
  
    dim3 dimBlock(size(tC));
    dim3 dimGrid(size(ceil_div(M, bM)),
                 size(ceil_div(N, bN)));
    gemm_device<<<dimGrid, dimBlock, 0, stream>>>
        (prob_shape, cta_tiler,
         A, dA, sA, tA,
         B, dB, sB, tB,
         C, dC, sC, tC,
         alpha, beta);
  }
  
  template <class ProblemShape, class CtaTiler,// CTA means Cooperative Thread Array
            class TA, class AStride, class ASmemLayout, class AThreadLayout,
            class TB, class BStride, class BSmemLayout, class BThreadLayout,
            class TC, class CStride, class CSmemLayout, class CThreadLayout,
            class Alpha, class Beta>
  __global__ static
  __launch_bounds__(decltype(size(CThreadLayout{}))::value)
  void
  gemm_device(ProblemShape shape_MNK, CtaTiler cta_tiler,
              TA const* A, AStride dA, ASmemLayout sA_layout, AThreadLayout tA,
              TB const* B, BStride dB, BSmemLayout sB_layout, BThreadLayout tB,
              TC      * C, CStride dC, CSmemLayout sC_layout, CThreadLayout tC,
              Alpha alpha, Beta beta)
  {
    using namespace cute;
    //
    // Full and Tiled Tensors
    //
  
    // Represent the full tensors
    Tensor mA = make_tensor(make_gmem_ptr(A), select<0,2>(shape_MNK), dA); // (M,K)
    Tensor mB = make_tensor(make_gmem_ptr(B), select<1,2>(shape_MNK), dB); // (N,K)
    Tensor mC = make_tensor(make_gmem_ptr(C), select<0,1>(shape_MNK), dC); // (M,N)
  
    // Get the appropriate blocks for this thread block
    // so when try to tile it with step<_1, X, _1>, it will first tile it in a sub-tensor mA_tiled ((BLK_M, BLK_K), (m, k))
    // then we try to slice it with the coord (blockIdx.x, blockIdx.y, _) in the rest-mode(second-mode)
    // the _ means that for the k dimension, we will take all the elements, similar : in the pytorch
    // so we are actually doing this: gA = mA_tiled[:, :, x, :]
    auto cta_coord = make_coord(blockIdx.x, blockIdx.y, _);              // (x,y,_)
    Tensor gA = local_tile(mA, cta_tiler, cta_coord, Step<_1, X,_1>{});  // (BLK_M,BLK_K,k) i.e. (128, 8, 512)
    Tensor gB = local_tile(mB, cta_tiler, cta_coord, Step< X,_1,_1>{});  // (BLK_N,BLK_K,k) i.e. (128, 8, 512)
    Tensor gC = local_tile(mC, cta_tiler, cta_coord, Step<_1,_1, X>{});  // (BLK_M,BLK_N)   i.e. (128, 128)
  
    // Shared memory buffers
    __shared__ TA smemA[cosize_v<ASmemLayout>];
    __shared__ TB smemB[cosize_v<BSmemLayout>];
    Tensor sA = make_tensor(make_smem_ptr(smemA), sA_layout);            // (BLK_M,BLK_K) i.e. (128, 8)
    Tensor sB = make_tensor(make_smem_ptr(smemB), sB_layout);            // (BLK_N,BLK_K) i.e. (128, 8)
  
    //
    // Partition the copying of A and B tiles across the threads
    //
  
    // local_partition is a lot like local_tile, first we tile the tensor, then we still slice in the first mode
    // gA_tiled = ((THR_M, THR_K), (thr_m, thr_k, k))
    // Note that the threadIdx.x will be converted into a coord (x, y) automatically
    // we slice it at the fist-mode (tile-mode) gA = gA_tiled[x, y, :, :, :]
  
    // gA is the tiled tensor in this block, and tA is this thread
    // so tAgA basically means the single tensor allocated for a single tensor
    Tensor tAgA = local_partition(gA, tA, threadIdx.x);                  // (thr_m,thr_k,k) i.e. (4, 1, 512)
    Tensor tAsA = local_partition(sA, tA, threadIdx.x);                  // (thr_m,thr_k)   i.e. (4, 1)
  
    Tensor tBgB = local_partition(gB, tB, threadIdx.x);                  // (thr_n,thr_k,k) i.e. (4, 1, 512)
    Tensor tBsB = local_partition(sB, tB, threadIdx.x);                  // (THR_N,THR_K)   i.e. (4, 1)
  
    //
    // Define A/B partitioning and C accumulators
    //
  
    // Partition sA(BLK_M,BLK_K) by the rows of tC(THR_M,THR_N)
    // (128, 8) -> ((16), (8, 8)) -> (8, 8)
    Tensor tCsA = local_partition(sA, tC, threadIdx.x, Step<_1, X>{});   // (8, 8)
    Tensor tCsB = local_partition(sB, tC, threadIdx.x, Step< X,_1>{});   // (8, 8)
    // (128, 128) -> ((16, 16), (8, 8)) -> (8, 8)
    Tensor tCgC = local_partition(gC, tC, threadIdx.x, Step<_1,_1>{});   // (8, 8)
  
    // Allocate the accumulators -- same shape/layout as the partitioned data
    Tensor tCrC = make_tensor_like(tCgC);                                // (8, 8)
    // Clear the accumulators
    clear(tCrC);
  
    // TUTORIAL: Example of a simple mainloop that read tiles of data into shared memory,
    //           and then computes on those tiles.
    //   copy(.) operates on the global and shared memory via the tA|tB partitioning
    //   gemm(.) operates on the shared and register memory via the tC partitioning
  
    auto K_TILE_MAX = size<2>(tAgA); // k = K // BLK_K = 4096 // 8 = 512 (in this case)
  
    for (int k_tile = 0; k_tile < K_TILE_MAX; ++k_tile)
    {
      // Copy gmem to smem with tA|tB thread-partitioned tensors
      // tAgA (thr_m, thr_k, k) i.e. (4, 1, 512) in our case
      // tAsA (thr_m, thr_k) i.e. (4, 1) in our case
      // Note that whole block would use the shared memory, (4, 1) actually will multiply (THR_M, THR_K) i.e. (32, 8)
      // and then fill up entire shared memory (128, 8)
      copy(tAgA(_,_,k_tile), tAsA);      // A   (THR_M,THR_K) -> (THR_M,THR_K)
      copy(tBgB(_,_,k_tile), tBsB);      // B   (THR_N,THR_K) -> (THR_N,THR_K)
  
      cp_async_fence();        // Label the end of (potential) cp.async instructions
      cp_async_wait<0>();      // Sync on all (potential) cp.async instructions
      __syncthreads();         // Wait for all threads to write to smem
  
      // Compute gemm on tC thread-partitioned smem
      gemm(tCsA, tCsB, tCrC);            // (THR_M,THR_N) += (THR_M,BLK_K) * (THR_N,BLK_K)
      __syncthreads();         // Wait for all threads to read from smem
    }
  
    //
    // Epilogue
    //
  
    axpby(alpha, tCrC, beta, tCgC);
  }
  ```

## CUTE Tutorials

ä¸»è¦æ€»ç»“ Concepts

### Layout

[layout](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/01_layout.md)

- IntTuple

- Creation & Use

- Shapes & Strides

- Print 

  `print_layout`

  `print_latex` to see layout more visually

- Coordinates & Index Conversion

  Natural coordinate

  1-D coordinate

  

> In CuTe, `Layout` is a first class citizen, is natively hierarchical to naturally represent functions beyond row-major and column-major, and can similarly be indexed with a hierarchy of coordinates.



### Layout Algebra

Only focus on the [Divide](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md#division-tiling) and [Product](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md#product-tiling) section

### Tensor

Basically the application of Layout

### Algorithms

Functions that we can use

- [layout algebra](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md)
- tensor
- algorithms

### MMA Atom

> An "MMA atom" in the CUTLASS library refers to a basic building block or operation unit for performing these matrix multiply-accumulate operations efficiently on NVIDIA GPUs.
>
> The term "atom" in MMA atom is used metaphorically to describe a fundamental unit or building block of computation within this context.



- CTA Cooperative Thread Array

  The simplest of the tutorial examples covers the basics of partitioning the global memory into tiles across the CTAs (also called threadblocks in CUDA), partitioning the data tiles across the threads of each CTA, and writing a mainloop using `cute::copy` and `cute::gemm`.

  - `CtaTiler`. A CuTe [tiler concept](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md#composition-tilers) that determines how to extract a tile of data from the problem shape.
  - At the highest level, the work is distributed across CTAs. In principle, each CTA's tile could come from the input tensors in many different ways. Many [CuTe `Tiler`s](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md#composition-tilers) could be used to tile the data, but for these cases it is sufficient to simply use the shape of the desired CTA tile.

- zipped_divide

  æœ¬æ¥æƒ³å°±çœ‹ä¸‹ zipped_divideï¼Œä½†å¯ä»¥é¡ºæ‰‹æŠŠ logical divide ç»™å­¦äº†ï¼Œéƒ½åœ¨ [layout algebra](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md) é‡Œé¢

- drawing from each level: how gemm is divided from block to thread, and we have different layout to deal with computation (global mem & shared mem)

- From a bigger picture: an intuitive way to think about layout algebra, where is the bottleneck to understanding the process?

Key functions

- `make_tensor`

- `make_coord`

- `make_gemm_ptr`

- `make_smem_ptr`

- `make_shape`

- `make_strid`

- `make_layout`

- `size`

- `local_tile`

  what is `Step` trying to do?

- `local_partition`

  seems like there are reload functions of `local_partition`

- `copy`

- `gemm`

Everything is layout in cutlass: problem layout, block layout, thread layout, memory layout, **use layout algebra to solve them in a unified view**

Other reference

- [CUDA MODE lecture 15](https://www.bilibili.com/video/BV1QZ421N7pT?spm_id_from=333.788.videopod.episodes&p=15) checked, pretty useful
- [Reed's zhihu posts](https://www.zhihu.com/people/reed-84-49/posts), not checked
- [CUTLASS CuTeå®æˆ˜(ä¸€)-åŸºç¡€](https://zhuanlan.zhihu.com/p/690703999), not checked

## CUTLASS in Practice

- improve cutlass gemm  [zhihu](https://zhuanlan.zhihu.com/p/707715989) [Reed's zhihu posts](https://www.zhihu.com/people/reed-84-49/posts)
- pick up cutlass examples: interested in all kinds of gemm and kernel fusion
- [CUTLASS CuTeå®æˆ˜(äºŒ)-åº”ç”¨](https://zhuanlan.zhihu.com/p/692078624) [github](https://github.com/zeroine/cutlass-cute-sample) this gives examples on optimze gemm and fusing kernel, and most importantly, it gives examples on how to use ncu & nsys to analyize the performance
- cutlass in flash attention

## Questions

- How to Debug? I often see colored squares in screen

- cute and cutlass, which should we choose?

- How to fuse kernels?

- when using `cmake .. -DCUTLASS_NVCC_ARCHS=86` does this equal to `cmake .. -DCUTLASS_NVCC_ARCHS=80`

- CUTLASS assert functions

- dynamic & static differenceâœ…

  dynamic is the value you can get at runtime, the static is the value that can be determined at compile time.

- what is nested tensor mode

- sgemm_2 is 3ms faster than sgemm_1 (15ms v.s. 18 ms), still 5ms to go from 10ms (cuBLAS), how to exceed it? [zhihu](https://zhuanlan.zhihu.com/p/707715989)

- what is mode and major

  seems like mode refers to axis...

- Layout compatibility is not well explained: Why Shape 24 is compatible with Shape (24), but Shape (24) is **NOT** compatible with Shape 24.

- There are 2 async functions that I don't quite understand

  ```c++
  cp_async_fence();        // Label the end of (potential) cp.async instructions
  cp_async_wait<0>();      // Sync on all (potential) cp.async instructions
  __syncthreads();         // Wait for all threads to write to smem
  ```

  
