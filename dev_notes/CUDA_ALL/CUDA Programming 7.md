# CUDA Programming 7

æœ‰äº†ä¹‹å‰ CUDA Programming 1-6 çš„é“ºå«ï¼Œå¯¹äº CUDA åŸºç¡€åº”è¯¥æœ‰äº†ä¸€å®šçš„äº†è§£ã€‚ç°åœ¨æƒ³è¦å¹²ä¸€äº›æœ‰è¶£çš„äº‹æƒ…

æƒ³å¿…ç»å¤§éƒ¨åˆ†åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸçš„äººéƒ½å¬è¯´è¿‡ [FlashAttention](https://github.com/Dao-AILab/flash-attention)ï¼Œå…¶æ˜¯ä¸€ä¸ª fast & memory efficient çš„ attention å®ç°ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œä¹Ÿæœ‰éå¸¸å¤šçš„äººå¬è¯´è¿‡ [FlashInfer](https://github.com/flashinfer-ai/flashinfer)ï¼Œè¯¥é¡¹ç›®ä¹Ÿæ˜¯ä¸€ä¸ªç”¨äºåŠ é€Ÿ LLM serving çš„ libraryã€‚è€Œä½ å»æŸ¥çœ‹ä»–ä»¬çš„ä»“åº“æ—¶éƒ½ä¼šå‘ç°ä¸€ä¸ªå…±åŒçš„ 3rdparty ä»“åº“ï¼š[CUTLASS](https://github.com/NVIDIA/cutlass)

æˆ‘çš„æœ¬æ„æ˜¯æƒ³è¦å°† FlashInfer ä»“åº“è¿›è¡Œæ·±å…¥çš„å­¦ä¹ ï¼Œä½†æ˜¾ç„¶åœ¨è¿™ä¹‹å‰è¿˜æœ‰å¤§é‡çš„ cutlass çŸ¥è¯†éœ€è¦åšé“ºå«ã€‚æ‰€ä»¥æˆ‘å…ˆè¿›è¡Œ cutlass çš„å­¦ä¹ ï¼Œå†è¿›è¡Œ flashinfer çš„å­¦ä¹ 

ä½†é—®é¢˜åœ¨äºï¼šcutlass ä¼¼ä¹æ²¡æœ‰ç‰¹åˆ«å¥½çš„æ•™ææ¥å¸®åŠ©å…¥é—¨ã€‚ç›®å‰æˆ‘æœ‰ä¸€äº›åˆ‡å…¥å£ï¼š

1. CUDA MODE lecture 15 introduced cutlass a little bit
2. CUTLASS examples
3. CUTE tutorial

é€šè¿‡å­¦ä¹  cutlass çš„ä¾‹å­æ¥æŒæ¡å…¶ç”¨æ³•ï¼Œ**æŒæ¡ç”¨æ³•æ˜¯æœ€ä¸»è¦çš„éœ€æ±‚ï¼Œä¹Ÿæ˜¯æœ€ç›´æ¥çš„åé¦ˆ**

Other zhihu references:

- [Reed's zhihu posts](https://www.zhihu.com/people/reed-84-49/posts), and its gemm code [github](https://github.com/reed-lau/cute-gemm)
- [CUTLASS CuTeå®æˆ˜(ä¸€)-åŸºç¡€](https://zhuanlan.zhihu.com/p/690703999)
- [CUTLASS CuTeå®æˆ˜(äºŒ)-åº”ç”¨](https://zhuanlan.zhihu.com/p/692078624)
  - [github](https://github.com/zeroine/cutlass-cute-sample)
- [cutlass cute 101](https://zhuanlan.zhihu.com/p/660379052)
- A collective repo which gathers a lots of blogs and kenel impl [CUDA-Learn-Notes](https://github.com/DefTruth/CUDA-Learn-Notes), not suitable for system learning, can be used for look-up table if you are trying to seek for some topic

## Install & Hello World

Good news! CUTLASS does not need to be built!!!

> CUTLASS is a header-only template library and does not need to be built to be used by other projects. Client applications should target CUTLASS's `include/` directory in their include paths.

ä½†ä¸ºäº†è¿è¡Œä¸€äº› example codeï¼Œæˆ‘ä»¬å¿…é¡»è¦è¿›è¡Œç¼–è¯‘ï¼Œæ‰èƒ½ run èµ·æ¥

Hello World in CUTE

[sgemm_1.cu](https://github.com/NVIDIA/cutlass/blob/main/examples/cute/tutorial/sgemm_1.cu)

[quick start guide](https://github.com/NVIDIA/cutlass/blob/main/media/docs/quickstart.md)

[quick start guide-cute](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/00_quickstart.md)

æ ¹æ® [quick start guide](https://github.com/NVIDIA/cutlass/blob/main/media/docs/quickstart.md) ä¸­æç¤ºï¼Œæˆ‘å…ˆè¯•ç”¨ cmake å¹¶ä¸”æŒ‡å®šè‡ªå·±çš„ compute capacity

```shell
# see your compute capacity with command:
# nvidia-smi --query-gpu=compute_cap --format=csv
mkdir build && cd build
cmake .. -DCUTLASS_NVCC_ARCHS=86
```

ä¼¼ä¹åº”è¯¥ä½¿ç”¨ `-DCUTLASS_NVCC_ARCHS=80`? å³ä½¿æˆ‘çš„ compute capacity æ˜¯86

åœ¨è¿è¡Œå®Œè¿‡åå¯ä»¥çœ‹åˆ° `build/Makefile` ä¸­ä¼šæœ‰è®¸å¤šæˆ‘ä»¬å¯ build çš„æ–‡ä»¶ï¼Œå…¶ä¸­ `examples` ä¸‹çš„æ‰€æœ‰æ–‡ä»¶éƒ½å¯ä»¥åœ¨é‡Œé¢æœåˆ° (e.g. `00_basic_gemm`)ï¼Œè€Œæˆ‘è¦ä½¿ç”¨çš„å°±æ˜¯ `sgemm_1` 

```shell
make sgemm_1 -j8
```

è¿è¡Œå®Œè¿‡åå°±å¯ä»¥åœ¨ `build/examples/cute/tutorial/sgemm_1` æ‰¾åˆ°ï¼Œè¿è¡Œ

```shell
# under the `build` dir
./examples/cute/tutorial/sgemm_1
```

```shell
âœ  cutlass git:(main) ./build/examples/cute/tutorial/sgemm_1
M = 5120
N = 5120
K = 4096
C = A^N B^T
Using device 0: NVIDIA GeForce RTX 3080  (SM86, 68 SMs)
CUTE_GEMM:     [11452.9]GFlop/s  (18.7506)ms
```

è¯¥ä»£ç çš„æºç å°±åœ¨ `./examples/cute/tutorial/sgemm_1.cu`ã€‚äºæ­¤åŒæ—¶æˆ‘æµ‹è¯•äº†ä¸€ä¸‹åŒç­‰æ¡ä»¶ä¸‹ cuBLAS çš„ latency åªéœ€è¦ 10.89 msï¼Œæ‰€ä»¥ä¸Šé¢çš„ sgemm ä»ç„¶æ˜¯ä¸€ä¸ªéå¸¸æ…¢ gemmï¼Œè¯´æ˜ä¼˜åŒ–ç©ºé—´è¿˜éå¸¸å¤§ğŸ‘€

## A Closer Look

ç°åœ¨æ¥çœ‹ä¸‹ `sgemm_1.cu` åˆ°åº•å¹²äº†ä»€ä¹ˆäº‹æƒ…ä¹Ÿè®¸æ˜¯ä¸ªä¸é”™çš„é€‰æ‹©ï¼Œè¯¥æ–‡ä»¶ä¹Ÿå°±æ˜¯ä¸€ä¸ª 400 å¤šè¡Œçš„ä»£ç ï¼Œæˆ‘æ¥è´´ä¸€éƒ¨åˆ†ä»£ç 

```c++
// to be paste later...
// not so convinient to see the whole picture
```

å¯ä»¥çœ‹åˆ°æ•´ä¸ªä»£ç åˆ†ä¸º4ä¸ªéƒ¨åˆ†ï¼š

1. `gemm_device` æ˜¯ cutlass gpu kernel çš„æ ¸å¿ƒå®ç°
2. `gemm_nt & gemm_tn`ï¼Œè°ƒç”¨ `gemm_devie` å®ŒæˆçŸ©é˜µä¹˜æ³•
3. `gemm` å°±æ˜¯ä¸€ä¸ª wrapperï¼ŒåŒ…è£¹ `gemm_nt & gemm_tn`ï¼Œå…¶åŒºåˆ«äº `cute::gemm`ï¼Œæˆ–è®¸åº”è¯¥æ¢ä¸€ä¸ªåå­—
4. `main` å³ä¸º host ä¸»ç¨‹åºçš„è¿è¡Œï¼ŒåŒ…å«æµ‹é‡ Flops & latency

###  gemm_device

åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»æœªå­¦ä¹ è¿‡å¦‚ä½•ä½¿ç”¨ GPU æ¥è¿›è¡ŒçŸ©é˜µè¿ç®—ï¼Œåªå¯¹ç®€å•çš„ reduce & tranpose åšè¿‡å­¦ä¹ ã€‚é‚£ä¹ˆé—®é¢˜æ¥äº†ï¼šä¸ºä½•çŸ©é˜µä¹˜æ³•é€‚åˆäºå¹¶è¡Œè¿ç®—ï¼Ÿè¿™æ˜¯å› ä¸ºå¯ä»¥ç‹¬ç«‹åœ°è®¡ç®—æ¯ä¸€ä¸ªå…ƒç´ 
$$
C_{ij}=\Sigma_k A_{ik}B_{kj}
$$
è¿™ä¸ªå…¬å¼å¯ä»¥ç”¨å›¾åƒéå¸¸å½¢è±¡åœ°è¡¨è¾¾

<img src="CUDA Programming 7/image-20241128111348874.png" alt="image-20241128111348874" style="zoom: 50%;" />

è“è‰²éƒ¨åˆ†çš„çŸ©é˜µä¹˜ç§¯ç»“æœï¼Œç”±ç»¿è‰²å’Œé»„è‰²éƒ¨åˆ†çš„çŸ©é˜µçš„ç‚¹ç§¯å’Œå¾—åˆ°ã€‚`gemm_device` æ‰€é‡‡ç”¨çš„å°±æ˜¯è¿™æ ·æœ´ç´ çš„æ€ç»´ï¼Œæ¯”ä¸è¿‡ cuBLAS ä¹Ÿæ˜¯æƒ…æœ‰å¯åŸçš„ï¼Œä¸‹é¢å…·ä½“åœ°è®¨è®ºæ•´ä¸ªè¿‡ç¨‹ï¼Œå³ï¼šå¦‚ä½•å°†æ•°æ®åˆ†é…åˆ°å„ä¸ª block/warp/thread å½“ä¸­ï¼Œå¹¶è¿›è¡Œè®¡ç®—ï¼Œæ­¤å¤„å‚è€ƒ [cutlass cute 101](https://zhuanlan.zhihu.com/p/660379052) [0x_gemm_tutorial.md](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/0x_gemm_tutorial.md)

- What does this code trying to do?

  è¿™åŸºæœ¬ä¸Šå°±æ˜¯åœ¨æ•™ä½ å¦‚ä½•ç”¨å¹¶è¡Œçš„æ€ç»´å»å¤„ç†çŸ©é˜µä¹˜æ³•ã€‚æˆ‘å…ˆä»é€»è¾‘è§†è§’å®Œæˆè¿™ä»¶äº‹æƒ…ï¼Œç„¶åå†å°†è¿™äº›é€»è¾‘æ˜ å°„åˆ°ä»£ç å½“ä¸­ï¼Œçœ‹ä¸‹ä»£ç çš„å…·ä½“å®ç°

  ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬é¦–å…ˆå®šä¹‰é—®é¢˜ä¸ºæœ€ç®€å•çš„çŸ©é˜µä¹˜æ³•ï¼š$C = AB$ï¼Œä»–ä»¬å½¢çŠ¶åˆ†åˆ«ä¸º `A(M,K) & B(K,N), C(M,N)`ï¼Œä¸ºäº†è®©é—®é¢˜æ›´åŠ å…·è±¡åŒ–ï¼Œæˆ‘ä»¬ä»¥ `M=N=5120, K=4096` ä¸ºä¾‹å­ï¼ˆè¿™ä¹Ÿæ˜¯ cutlass ä»£ç ä¾‹å­ä¸­æ‰€ä½¿ç”¨çš„æ•°å€¼ï¼‰

  ä¸€ä¸ªä¸é”™çš„åˆ’åˆ†è§†è§’æ˜¯ä»¥çŸ©é˜µ $C$ ä¸ºæ ¸å¿ƒï¼šæˆ‘ä»¬å°†çŸ©é˜µ C è¿›è¡Œåˆ’åˆ†ï¼Œä»¥ $(128,128)$ çš„æ–¹å—ä½œä¸ºåˆ’åˆ†å•ä½ï¼Œå»å•ç‹¬æ±‚è§£æ¯ä¸€ä¸ªæ–¹å—ä¸­çš„çŸ©é˜µä¹˜æ³•ç»“æœã€‚ä» CUDA ç¼–ç¨‹çš„è§’åº¦æ¥è®²ï¼šæˆ‘ä»¬è®©ä¸€ä¸ª block å»å¤„ç†ä¸€ä¸ª $(128,128)$ çš„çŸ©é˜µä¹˜æ³•ç»“æœ

  TODO: æ’å›¾

  å¥½ï¼Œç°åœ¨å°±å¯ä»¥é›†ä¸­ç²¾åŠ›æ¥å¤„ç†æ¯ä¸€ä¸ª block åº”å½“å¦‚ä½•è®¡ç®—äº†

- CTA Cooperative Thread Array

  The simplest of the tutorial examples covers the basics of partitioning the global memory into tiles across the CTAs (also called threadblocks in CUDA), partitioning the data tiles across the threads of each CTA, and writing a mainloop using `cute::copy` and `cute::gemm`.

  - `CtaTiler`. A CuTe [tiler concept](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md#composition-tilers) that determines how to extract a tile of data from the problem shape.
  - At the highest level, the work is distributed across CTAs. In principle, each CTA's tile could come from the input tensors in many different ways. Many [CuTe `Tiler`s](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md#composition-tilers) could be used to tile the data, but for these cases it is sufficient to simply use the shape of the desired CTA tile.

- zipped_divide

  æœ¬æ¥æƒ³å°±çœ‹ä¸‹ zipped_divideï¼Œä½†å¯ä»¥é¡ºæ‰‹æŠŠ logical divide ç»™å­¦äº†ï¼Œéƒ½åœ¨ [layout algebra](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md) é‡Œé¢

- drawing from each level: how gemm is divided from block to thread, and we have different layout to deal with computation (global mem & shared mem)

- From a bigger picture: an intuitive way to think about layout algebra, where is the bottleneck to understanding the process?

Key functions

- `make_tensor`

- `make_coord`

- `make_gemm_ptr`

- `make_smem_ptr`

- `make_shape`

- `make_strid`

- `make_layout`

- `size`

- `local_tile`

  what is `Step` trying to do?

- `local_partition`

  seems like there are reload functions of `local_partition`

- `copy`

- `gemm`

Everything is layout in cutlass: problem layout, block layout, thread layout, memory layout, **use layout algebra to solve them in a unified view**

## CUTE Tutorials

- [CUDA MODE lecture 15](https://www.bilibili.com/video/BV1QZ421N7pT?spm_id_from=333.788.videopod.episodes&p=15)
- [Reed's zhihu posts](https://www.zhihu.com/people/reed-84-49/posts)
- [CUTLASS CuTeå®æˆ˜(ä¸€)-åŸºç¡€](https://zhuanlan.zhihu.com/p/690703999)

ä¸»è¦æ€»ç»“ Concepts

- [layout algebra](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md)
- tensor
- algorithms

## CUTLASS in Practice

- improve cutlass gemm  [zhihu](https://zhuanlan.zhihu.com/p/707715989) [Reed's zhihu posts](https://www.zhihu.com/people/reed-84-49/posts)
- pick up cutlass examples: interested in all kinds of gemm and kernel fusion
- [CUTLASS CuTeå®æˆ˜(äºŒ)-åº”ç”¨](https://zhuanlan.zhihu.com/p/692078624) [github](https://github.com/zeroine/cutlass-cute-sample) this gives examples on optimze gemm and fusing kernel, and most importantly, it gives examples on how to use ncu & nsys to analyize the performance
- cutlass in flash attention

## Questions

- How to Debug? I often see colored squares in screen
- cute and cutlass, which should we choose?
- How to fuse kernels?
- What are the basic tools? and maybe basic types?
- when using `cmake .. -DCUTLASS_NVCC_ARCHS=86` does this equal to `cmake .. -DCUTLASS_NVCC_ARCHS=80`
- CUTLASS assert functions
- dynamic & static difference
- what is nested tensor mode
- why do we try to divide the K dimension into smaller k?
- sgemm_2 is 3ms faster than sgemm_1 (15ms v.s. 18 ms), still 5ms to go from 10ms (cuBLAS), how to exceed it? [zhihu](https://zhuanlan.zhihu.com/p/707715989)
- what is mode and major

  seems like mode refers to axis...
