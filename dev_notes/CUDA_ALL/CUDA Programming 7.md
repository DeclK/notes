# CUDA Programming 7

æœ‰äº†ä¹‹å‰ CUDA Programming 1-6 çš„é“ºå«ï¼Œå¯¹äº CUDA åŸºç¡€åº”è¯¥æœ‰äº†ä¸€å®šçš„äº†è§£ã€‚ç°åœ¨æƒ³è¦å¹²ä¸€äº›æœ‰è¶£çš„äº‹æƒ…

æƒ³å¿…ç»å¤§éƒ¨åˆ†åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸçš„äººéƒ½å¬è¯´è¿‡ [FlashAttention](https://github.com/Dao-AILab/flash-attention)ï¼Œå…¶æ˜¯ä¸€ä¸ª fast & memory efficient çš„ attention å®ç°ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œä¹Ÿæœ‰éå¸¸å¤šçš„äººå¬è¯´è¿‡ [FlashInfer](https://github.com/flashinfer-ai/flashinfer)ï¼Œè¯¥é¡¹ç›®ä¹Ÿæ˜¯ä¸€ä¸ªç”¨äºåŠ é€Ÿ LLM serving çš„ libraryã€‚è€Œä½ å»æŸ¥çœ‹ä»–ä»¬çš„ä»“åº“æ—¶éƒ½ä¼šå‘ç°ä¸€ä¸ªå…±åŒçš„ 3rdparty ä»“åº“ï¼š[CUTLASS](https://github.com/NVIDIA/cutlass)

æˆ‘çš„æœ¬æ„æ˜¯æƒ³è¦å°† FlashInfer ä»“åº“è¿›è¡Œæ·±å…¥çš„å­¦ä¹ ï¼Œä½†æ˜¾ç„¶åœ¨è¿™ä¹‹å‰è¿˜æœ‰å¤§é‡çš„ cutlass çŸ¥è¯†éœ€è¦åšé“ºå«ã€‚æ‰€ä»¥æˆ‘å…ˆè¿›è¡Œ cutlass çš„å­¦ä¹ ï¼Œå†è¿›è¡Œ flashinfer çš„å­¦ä¹ 

ä½†é—®é¢˜åœ¨äºï¼šcutlass ä¼¼ä¹æ²¡æœ‰ç‰¹åˆ«å¥½çš„æ•™ææ¥å¸®åŠ©å…¥é—¨ã€‚ç›®å‰æˆ‘æœ‰ä¸€äº›åˆ‡å…¥å£ï¼š

1. CUDA MODE lecture 15 introduced cutlass a little bit
2. CUTLASS examples
3. CUTE tutorial

é€šè¿‡å­¦ä¹  cutlass çš„ä¾‹å­æ¥æŒæ¡å…¶ç”¨æ³•ï¼Œ**æŒæ¡ç”¨æ³•æ˜¯æœ€ä¸»è¦çš„éœ€æ±‚ï¼Œä¹Ÿæ˜¯æœ€ç›´æ¥çš„åé¦ˆ**

Other zhihu references:

- [Reed's zhihu posts](https://www.zhihu.com/people/reed-84-49/posts), and its gemm code [github](https://github.com/reed-lau/cute-gemm)
- [CUTLASS CuTeå®æˆ˜(ä¸€)-åŸºç¡€](https://zhuanlan.zhihu.com/p/690703999)
- [CUTLASS CuTeå®æˆ˜(äºŒ)-åº”ç”¨](https://zhuanlan.zhihu.com/p/692078624)
  - [github](https://github.com/zeroine/cutlass-cute-sample)
- [cutlass cute 101](https://zhuanlan.zhihu.com/p/660379052)
- A collective repo which gathers a lots of blogs and kenel impl [CUDA-Learn-Notes](https://github.com/DefTruth/CUDA-Learn-Notes), not suitable for system learning, can be used for look-up table if you are trying to seek for some topic

## Install & Hello World

Good news! CUTLASS does not need to be built!!!

> CUTLASS is a header-only template library and does not need to be built to be used by other projects. Client applications should target CUTLASS's `include/` directory in their include paths.

ä½†ä¸ºäº†è¿è¡Œä¸€äº› example codeï¼Œæˆ‘ä»¬å¿…é¡»è¦è¿›è¡Œç¼–è¯‘ï¼Œæ‰èƒ½ run èµ·æ¥

Hello World in CUTE

[sgemm_1.cu](https://github.com/NVIDIA/cutlass/blob/main/examples/cute/tutorial/sgemm_1.cu)

[quick start guide](https://github.com/NVIDIA/cutlass/blob/main/media/docs/quickstart.md)

[quick start guide-cute](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/00_quickstart.md)

æ ¹æ® [quick start guide](https://github.com/NVIDIA/cutlass/blob/main/media/docs/quickstart.md) ä¸­æç¤ºï¼Œæˆ‘å…ˆè¯•ç”¨ cmake å¹¶ä¸”æŒ‡å®šè‡ªå·±çš„ compute capacity

```shell
# see your compute capacity with command:
# nvidia-smi --query-gpu=compute_cap --format=csv
mkdir build && cd build
cmake .. -DCUTLASS_NVCC_ARCHS=86
```

ä¼¼ä¹åº”è¯¥ä½¿ç”¨ `-DCUTLASS_NVCC_ARCHS=80`? å³ä½¿æˆ‘çš„ compute capacity æ˜¯86ã€‚Another tip: æˆ‘å°è¯•åœ¨ WSL ä¸Šè¿›è¡Œ cmakeï¼Œç»“æœå¡ä½äº† `(pid, sts) = os.waitpid(self.pid, wait_flags)`ï¼Œåº”è¯¥æ˜¯å¤šçº¿ç¨‹åœ¨ WSL ä¸Šä¸å¤ªç®¡ç”¨ï¼ŒæŠŠ unit test å–æ¶ˆç¼–è¯‘å°±å¥½ ` cmake .. -DCUTLASS_NVCC_ARCHS=86 -DCUTLASS_ENABLE_TESTS=OFF`

åœ¨è¿è¡Œå®Œè¿‡åå¯ä»¥çœ‹åˆ° `build/Makefile` ä¸­ä¼šæœ‰è®¸å¤šæˆ‘ä»¬å¯ build çš„æ–‡ä»¶ï¼Œå…¶ä¸­ `examples` ä¸‹çš„æ‰€æœ‰æ–‡ä»¶éƒ½å¯ä»¥åœ¨é‡Œé¢æœåˆ° (e.g. `00_basic_gemm`)ï¼Œè€Œæˆ‘è¦ä½¿ç”¨çš„å°±æ˜¯ `sgemm_1` 

```shell
make sgemm_1 -j8
```

è¿è¡Œå®Œè¿‡åå°±å¯ä»¥åœ¨ `build/examples/cute/tutorial/sgemm_1` æ‰¾åˆ°ï¼Œè¿è¡Œ

```shell
# under the `build` dir
./examples/cute/tutorial/sgemm_1
```

```shell
âœ  cutlass git:(main) ./build/examples/cute/tutorial/sgemm_1
M = 5120
N = 5120
K = 4096
C = A^N B^T
Using device 0: NVIDIA GeForce RTX 3080  (SM86, 68 SMs)
CUTE_GEMM:     [11452.9]GFlop/s  (18.7506)ms
```

è¯¥ä»£ç çš„æºç å°±åœ¨ `./examples/cute/tutorial/sgemm_1.cu`ã€‚äºæ­¤åŒæ—¶æˆ‘æµ‹è¯•äº†ä¸€ä¸‹åŒç­‰æ¡ä»¶ä¸‹ cuBLAS çš„ latency åªéœ€è¦ 10.89 msï¼Œæ‰€ä»¥ä¸Šé¢çš„ sgemm ä»ç„¶æ˜¯ä¸€ä¸ªéå¸¸æ…¢ gemmï¼Œè¯´æ˜ä¼˜åŒ–ç©ºé—´è¿˜éå¸¸å¤§ğŸ‘€

## A Closer Look

ç°åœ¨æ¥çœ‹ä¸‹ `sgemm_1.cu` åˆ°åº•å¹²äº†ä»€ä¹ˆäº‹æƒ…ä¹Ÿè®¸æ˜¯ä¸ªä¸é”™çš„é€‰æ‹©ï¼Œè¯¥æ–‡ä»¶ä¹Ÿå°±æ˜¯ä¸€ä¸ª 400 å¤šè¡Œçš„ä»£ç ï¼Œæˆ‘æ¥è´´ä¸€éƒ¨åˆ†ä»£ç 

```c++
// to be paste later...
// not so convinient to see the whole picture
```

å¯ä»¥çœ‹åˆ°æ•´ä¸ªä»£ç åˆ†ä¸º4ä¸ªéƒ¨åˆ†ï¼š

1. `gemm_device` æ˜¯ cutlass gpu kernel çš„æ ¸å¿ƒå®ç°
2. `gemm_nt & gemm_tn`ï¼Œè°ƒç”¨ `gemm_devie` å®ŒæˆçŸ©é˜µä¹˜æ³•
3. `gemm` å°±æ˜¯ä¸€ä¸ª wrapperï¼ŒåŒ…è£¹ `gemm_nt & gemm_tn`ï¼Œå…¶åŒºåˆ«äº `cute::gemm`ï¼Œæˆ–è®¸åº”è¯¥æ¢ä¸€ä¸ªåå­—
4. `main` å³ä¸º host ä¸»ç¨‹åºçš„è¿è¡Œï¼ŒåŒ…å«æµ‹é‡ Flops & latency

###  gemm_device

åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘ä»æœªå­¦ä¹ è¿‡å¦‚ä½•ä½¿ç”¨ GPU æ¥è¿›è¡ŒçŸ©é˜µè¿ç®—ï¼Œåªå¯¹ç®€å•çš„ reduce & tranpose åšè¿‡å­¦ä¹ ã€‚é‚£ä¹ˆé—®é¢˜æ¥äº†ï¼šä¸ºä½•çŸ©é˜µä¹˜æ³•é€‚åˆäºå¹¶è¡Œè¿ç®—ï¼Ÿè¿™æ˜¯å› ä¸ºå¯ä»¥ç‹¬ç«‹åœ°è®¡ç®—æ¯ä¸€ä¸ªå…ƒç´ 
$$
C_{ij}=\Sigma_k A_{ik}B_{kj}
$$
è¿™ä¸ªå…¬å¼å¯ä»¥ç”¨å›¾åƒéå¸¸å½¢è±¡åœ°è¡¨è¾¾

<img src="CUDA Programming 7/image-20241128111348874.png" alt="image-20241128111348874" style="zoom: 50%;" />

è“è‰²éƒ¨åˆ†çš„çŸ©é˜µä¹˜ç§¯ç»“æœï¼Œç”±ç»¿è‰²å’Œé»„è‰²éƒ¨åˆ†çš„çŸ©é˜µçš„ç‚¹ç§¯å’Œå¾—åˆ°ã€‚`gemm_device` æ‰€é‡‡ç”¨çš„å°±æ˜¯è¿™æ ·æœ´ç´ çš„æ€ç»´ï¼Œæ¯”ä¸è¿‡ cuBLAS ä¹Ÿæ˜¯æƒ…æœ‰å¯åŸçš„ï¼Œä¸‹é¢å…·ä½“åœ°è®¨è®ºæ•´ä¸ªè¿‡ç¨‹ï¼Œå³ï¼šå¦‚ä½•å°†æ•°æ®åˆ†é…åˆ°å„ä¸ª block/warp/thread å½“ä¸­ï¼Œå¹¶è¿›è¡Œè®¡ç®—ï¼Œæ­¤å¤„å‚è€ƒ [cutlass cute 101](https://zhuanlan.zhihu.com/p/660379052) [0x_gemm_tutorial.md](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/0x_gemm_tutorial.md)

- What does this code trying to do logically?

  è¿™åŸºæœ¬ä¸Šå°±æ˜¯åœ¨æ•™ä½ å¦‚ä½•ç”¨å¹¶è¡Œçš„æ€ç»´å»å¤„ç†çŸ©é˜µä¹˜æ³•ã€‚æˆ‘å…ˆä»é€»è¾‘è§†è§’å®Œæˆè¿™ä»¶äº‹æƒ…ï¼Œç„¶åå†å°†è¿™äº›é€»è¾‘æ˜ å°„åˆ°ä»£ç å½“ä¸­ï¼Œçœ‹ä¸‹ä»£ç çš„å…·ä½“å®ç°

  ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬é¦–å…ˆå®šä¹‰é—®é¢˜ä¸ºæœ€ç®€å•çš„å•ç²¾åº¦çŸ©é˜µä¹˜æ³•ï¼š$C = AB$ï¼Œä»–ä»¬å½¢çŠ¶åˆ†åˆ«ä¸º `A(M,K) & B(K,N), C(M,N)`ï¼Œä¸ºäº†è®©é—®é¢˜æ›´åŠ å…·è±¡åŒ–ï¼Œæˆ‘ä»¬ä»¥ `M=N=5120, K=4096` ä¸ºä¾‹å­ï¼ˆè¿™ä¹Ÿæ˜¯ cutlass ä»£ç ä¾‹å­ä¸­æ‰€ä½¿ç”¨çš„æ•°å€¼ï¼‰

  ä¸€ä¸ªä¸é”™çš„åˆ’åˆ†è§†è§’æ˜¯ä»¥çŸ©é˜µ $C$ ä¸ºæ ¸å¿ƒï¼šæˆ‘ä»¬å°†çŸ©é˜µ C è¿›è¡Œåˆ’åˆ†ï¼Œä»¥ $(128,128)$ çš„æ–¹å—ä½œä¸ºåˆ’åˆ†å•ä½ï¼Œå»å•ç‹¬æ±‚è§£æ¯ä¸€ä¸ªæ–¹å—ä¸­çš„çŸ©é˜µä¹˜æ³•ç»“æœã€‚ä» CUDA ç¼–ç¨‹çš„è§’åº¦æ¥è®²ï¼šæˆ‘ä»¬è®©ä¸€ä¸ª block å»å¤„ç†ä¸€ä¸ª $(128,128)$ çš„çŸ©é˜µä¹˜æ³•ç»“æœ

  <img src="CUDA Programming 7/image-20241203155920569.png" alt="image-20241203155920569" style="zoom:80%;" />

  å¥½ï¼Œç°åœ¨å°±å¯ä»¥é›†ä¸­ç²¾åŠ›æ¥å¤„ç†æ¯ä¸€ä¸ª block åº”å½“å¦‚ä½•è®¡ç®—äº†ï¼Œå³ï¼šè®¡ç®—ä¸€ä¸ª $(128,128)$ çš„çŸ©é˜µ C åº”è¯¥å¦‚ä½•åšåˆ°ï¼Ÿé¦–å…ˆæˆ‘ä»¬éœ€è¦è·å–çŸ©é˜µ A & B ä¸­å¯¹åº”çš„æ•°æ®ï¼Œåˆ†åˆ«è·å¾—å¯¹åº”çš„ $(128, 4096)$â€‹â€‹ å¤§å°çš„æ•°æ®ï¼ˆå³ä¸Šå›¾ä¸­çš„ `gA & gB`ï¼Œ`gC` ä¹Ÿåœ¨å›¾é‡Œï¼Œ`g` ä»£è¡¨çš„æ˜¯ global memoryï¼Œè¯·å¿½ç•¥å›¾ä¸­é”™è¯¯çš„åˆ†å—æ•°é‡ï¼Œå› ä¸ºå®åœ¨ç”»ä¸äº†...ï¼‰

  å…¶å®ç°åœ¨å°±å¯ä»¥åšçŸ©é˜µä¹˜ï¼Œå¾—åˆ°æˆ‘ä»¬æƒ³è¦çš„ç»“æœï¼š
  $$
  gC=gA \times gB
  $$
  ä½†æ˜¯è¿™é‡Œæ˜¯ GPUï¼æˆ‘ä»¬å¦‚æœæ¯æ¬¡éƒ½ä» global memory è¯»å–æ•°æ®çš„è¯ï¼Œæ—¶å»¶ä¼šéå¸¸å¤§ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å…ˆæŠŠæ•°æ®è¯»åˆ° shared memory é‡Œé¢ï¼Œè¿™æ ·è¯»å–æ•°æ®åšè®¡ç®—çš„æ—¶å€™ä¼šæ›´å¿«ã€‚ä½†æ˜¯ shared memory åˆæ²¡æœ‰è¿™ä¹ˆå¤§çš„ç©ºé—´ï¼Œæ¯ä¸€ä¸ª block éœ€è¦åˆ†é…~ 100K*4Byte å¤§å°çš„ç©ºé—´ï¼Œè¿™å¤ªå¤šäº†ï¼å¥½æ¶ˆæ¯æ˜¯ï¼šæˆ‘ä»¬è¿˜å¯ä»¥å°†è¿™ä¸ªé—®é¢˜ç»§ç»­è¿›è¡Œåˆ‡åˆ†

  æˆ‘ä»¬æ²¿ç€ K è½´æ–¹å‘ï¼ŒæŠŠ 4096 åˆ‡åˆ†æˆä¸º $(512,8)$ çš„å½¢çŠ¶ï¼Œæˆ‘ä»¬æ¯æ¬¡åš $(128,8)$ å¤§å°çš„çŸ©é˜µä¹˜æ³•ï¼Œç„¶åè¿›è¡Œäº†ç´¯åŠ ä¹Ÿèƒ½å¤Ÿå¾—åˆ°ç›¸åŒçš„ç»“æœã€‚ç»è¿‡åˆ‡åˆ†è¿‡åæˆ‘ä»¬çš„è®¡ç®—è¿‡ç¨‹å˜ä¸ºäº†
  $$
  gC=\Sigma_{i=1}^{512}sA\times sB
  $$
  <img src="CUDA Programming 7/image-20241203160014235.png" alt="image-20241203160014235" style="zoom:80%;" />

  æœ€åæˆ‘ä»¬å°±éœ€è¦è€ƒè™‘å°†è¿™ä¸ª block level çš„é—®é¢˜ï¼Œåˆ‡åˆ†åˆ° thread level ä¸Šï¼Œè€ƒè™‘æ¯ä¸€ä¸ª thread åº”è¯¥å¹²çš„å·¥ä½œã€‚è€ƒè™‘ä¸€ä¸ª block æœ‰ 256 ä¸ª threadï¼Œæˆ‘ä»¬å°†è¿™ä¸ª thread æ’åˆ—æˆä¸º $(16,16)$ å½¢çŠ¶ï¼Œè¿™æ ·ä¸€æ¬¡å°±å¯ä»¥å¤„ç† $(16,16)$ å¤§å°çš„çŸ©é˜µã€‚

  ä¸ºäº†æ–¹ä¾¿ç”¨å›¾å½¢è¡¨ç¤ºï¼Œæˆ‘æ²¡åŠæ³•ç”»è¿™ä¹ˆå¤§çš„å›¾ï¼Œæˆ‘å°†ç”¨å½¢çŠ¶ `gC.shape=(8,8) & sA.shape=sB.shape=(8,4) & thread.shape=(4,4)` æ¥ç¤ºæ„æ¯ä¸€ä¸ªçº¿ç¨‹æ‰€è¦åˆ†é…çš„æ•°æ®ï¼Œä»¥åŠæœ€ç»ˆè®¡ç®—ç»“æœ [reference: math-partitioning](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/0x_gemm_tutorial.md#math-partitioning)

  <img src="CUDA Programming 7/image-20241203160059324.png" alt="image-20241203160059324" style="zoom: 67%;" />

  æˆ‘åœ¨ threads ä¸­é€‰æ‹©äº†5ä¸ª thread æ¥è¿›è¡Œè¡¨ç¤ºï¼Œæ¯ä¸€ä¸ª thread ç”¨ä¸åŒçš„é¢œè‰²ã€‚è™½ç„¶çœ¼èŠ±ç¼­ä¹±çš„ï¼Œä½†æ˜¯å°±æ˜¯çŸ©é˜µä¹˜æ³•æ‰€éœ€è¦çš„æ•°æ®ï¼Œä¸“æ³¨ä¸€ä¸ªé¢œè‰²çš„å°±OKã€‚ç¬¬ä¸€è¡Œçº¢è‰²å’Œæ©™è‰²çš„æ–¹å—éƒ½éœ€è¦è¯» `sA` ä¸­ç¬¬ä¸€è¡Œçš„æ•°æ®ï¼Œæ‰€ä»¥æˆ‘å¤šç”»äº†ä¸€è¡Œå‡ºæ¥ï¼Œ`sB` ä¸­å¤šå‡ºæ¥çš„ä¸€åˆ—ä¹Ÿæ˜¯è¿™ä¸ªé“ç†ã€‚æ¯ä¸€ä¸ª thread å°†ä¼šå»è¯»å– $sA(2,4)$ ä»¥åŠ $sB(2,4)$ å¤§å°çš„çŸ©é˜µç”¨äºçŸ©é˜µä¹˜æ³•

  æ‰€ä»¥å½“ç†è§£äº†å›¾ä¸­çš„çº¿ç¨‹åˆ†é…æ–¹å¼è¿‡åï¼Œç†è§£æˆ‘ä»¬ä¾‹å­å½“ä¸­çš„ï¼šå°† $(128, 128)$ å¤§å°çš„çŸ©é˜µåˆ†é…åˆ° $(16,16)$ å¤§å°çš„ threads ä¸­ï¼Œå°±éå¸¸å®¹æ˜“äº†ã€‚æ¯ä¸€ä¸ªå•ç‹¬çš„ thread æ€»å…±ä¼šå¤„ç† $gC(8,8)$ å¤§å°çš„çŸ©é˜µï¼Œå¹¶ä¸”æ¯ä¸€æ¬¡éƒ½ä¼šè·å– $sA(8,8)$ ä»¥åŠ $sB(8,8)$â€‹ å¤§å°çš„æ•°æ®ç”¨äºçŸ©é˜µä¹˜æ³•

- What are the cutlass codes?

  ç»è¿‡ä¸Šé¢çš„é€»è¾‘åˆ†ææ•´ä¸ªåˆ‡åˆ†è„‰ç»œå·²ç»å¾ˆæ¸…æ™°äº†ï¼š

  1. å°†æ•´ä¸ªçŸ©é˜µè¿›è¡Œåˆ‡åˆ†ï¼Œåˆ’åˆ†æˆä¸ºå¤šä¸ª blocks
  2. å†å°†çŸ©é˜µä¹˜æ³•æ²¿ç€ K è½´è¿›è¡Œåˆ‡åˆ†ï¼Œä»¥æ»¡è¶³ shared memory è¦æ±‚ï¼Œåˆ©ç”¨å¾ªç¯ç´¯åŠ çš„æ–¹å¼å®ŒæˆçŸ©é˜µä¹˜æ³•
  3. æœ€åå°†çŸ©é˜µä½¿ç”¨ threads é˜Ÿåˆ—è¿›è¡Œåˆ’åˆ†ï¼Œç»™æ¯ä¸€ä¸ª thread åˆ†é…æ•°æ®ï¼Œæ˜¯æœ€ç»ˆçŸ©é˜µä¹˜æ³•çš„æ‰§è¡Œè€…

  æˆ‘ç›´æ¥è´´ä»£ç äº†ï¼Œæ•´ä¸ªè¿‡ç¨‹è·Ÿç€æ³¨é‡Šçœ‹ä¹Ÿéå¸¸çš„æ¸…æ™°æ˜äº†

  ```c++
  // Setup params for an NT GEMM
  // Use m-major smem sA, n-major smem sB, and mn-major threads tA|tB
  template <class TA, class TB, class TC,
            class Alpha, class Beta>
  void
  gemm_nt(int m, int n, int k,
          Alpha alpha,
          TA const* A, int ldA,
          TB const* B, int ldB,
          Beta beta,
          TC      * C, int ldC,
          cudaStream_t stream = 0)
  {
    using namespace cute;
  
    // Define shapes (dynamic)
    auto M = int(m);
    auto N = int(n);
    auto K = int(k);
    auto prob_shape = make_shape(M, N, K);                     // (M, N, K)
  
    // Define NT strides (mixed)
    auto dA = make_stride(Int<1>{}, ldA);                      // (dM, dK)
    auto dB = make_stride(Int<1>{}, ldB);                      // (dN, dK)
    auto dC = make_stride(Int<1>{}, ldC);                      // (dM, dN)
  
    // Define CTA tile sizes (static)
    auto bM = Int<128>{};
    auto bN = Int<128>{};
    auto bK = Int<  8>{};
    auto cta_tiler = make_shape(bM, bN, bK);                   // (BLK_M, BLK_N, BLK_K)
  
    // Define the smem layouts (static)
    auto sA = make_layout(make_shape(bM, bK));                 // (m,k) -> smem_idx; m-major
    auto sB = make_layout(make_shape(bN, bK));                 // (n,k) -> smem_idx; n-major
    auto sC = make_layout(make_shape(bM, bN));                 // (m,n) -> smem_idx; m-major
  
    // Define the thread layouts (static)
    auto tA = make_layout(make_shape(Int<32>{}, Int< 8>{}));   // (m,k) -> thr_idx
    auto tB = make_layout(make_shape(Int<32>{}, Int< 8>{}));   // (n,k) -> thr_idx
    auto tC = make_layout(make_shape(Int<16>{}, Int<16>{}));   // (m,n) -> thr_idx
  
    dim3 dimBlock(size(tC));
    dim3 dimGrid(size(ceil_div(M, bM)),
                 size(ceil_div(N, bN)));
    gemm_device<<<dimGrid, dimBlock, 0, stream>>>
        (prob_shape, cta_tiler,
         A, dA, sA, tA,
         B, dB, sB, tB,
         C, dC, sC, tC,
         alpha, beta);
  }
  
  template <class ProblemShape, class CtaTiler,// CTA means Cooperative Thread Array
            class TA, class AStride, class ASmemLayout, class AThreadLayout,
            class TB, class BStride, class BSmemLayout, class BThreadLayout,
            class TC, class CStride, class CSmemLayout, class CThreadLayout,
            class Alpha, class Beta>
  __global__ static
  __launch_bounds__(decltype(size(CThreadLayout{}))::value)
  void
  gemm_device(ProblemShape shape_MNK, CtaTiler cta_tiler,
              TA const* A, AStride dA, ASmemLayout sA_layout, AThreadLayout tA,
              TB const* B, BStride dB, BSmemLayout sB_layout, BThreadLayout tB,
              TC      * C, CStride dC, CSmemLayout sC_layout, CThreadLayout tC,
              Alpha alpha, Beta beta)
  {
    using namespace cute;
    //
    // Full and Tiled Tensors
    //
  
    // Represent the full tensors
    Tensor mA = make_tensor(make_gmem_ptr(A), select<0,2>(shape_MNK), dA); // (M,K)
    Tensor mB = make_tensor(make_gmem_ptr(B), select<1,2>(shape_MNK), dB); // (N,K)
    Tensor mC = make_tensor(make_gmem_ptr(C), select<0,1>(shape_MNK), dC); // (M,N)
  
    // Get the appropriate blocks for this thread block
    // so when try to tile it with step<_1, X, _1>, it will first tile it in a sub-tensor mA_tiled ((BLK_M, BLK_K), (m, k))
    // then we try to slice it with the coord (blockIdx.x, blockIdx.y, _) in the rest-mode(second-mode)
    // the _ means that for the k dimension, we will take all the elements, similar : in the pytorch
    // so we are actually doing this: gA = mA_tiled[:, :, x, :]
    auto cta_coord = make_coord(blockIdx.x, blockIdx.y, _);              // (x,y,_)
    Tensor gA = local_tile(mA, cta_tiler, cta_coord, Step<_1, X,_1>{});  // (BLK_M,BLK_K,k) i.e. (128, 8, 512)
    Tensor gB = local_tile(mB, cta_tiler, cta_coord, Step< X,_1,_1>{});  // (BLK_N,BLK_K,k) i.e. (128, 8, 512)
    Tensor gC = local_tile(mC, cta_tiler, cta_coord, Step<_1,_1, X>{});  // (BLK_M,BLK_N)   i.e. (128, 128)
  
    // Shared memory buffers
    __shared__ TA smemA[cosize_v<ASmemLayout>];
    __shared__ TB smemB[cosize_v<BSmemLayout>];
    Tensor sA = make_tensor(make_smem_ptr(smemA), sA_layout);            // (BLK_M,BLK_K) i.e. (128, 8)
    Tensor sB = make_tensor(make_smem_ptr(smemB), sB_layout);            // (BLK_N,BLK_K) i.e. (128, 8)
  
    //
    // Partition the copying of A and B tiles across the threads
    //
  
    // local_partition is a lot like local_tile, first we tile the tensor, then we still slice in the first mode
    // gA_tiled = ((THR_M, THR_K), (thr_m, thr_k, k))
    // Note that the threadIdx.x will be converted into a coord (x, y) automatically
    // we slice it at the fist-mode (tile-mode) gA = gA_tiled[x, y, :, :, :]
  
    // gA is the tiled tensor in this block, and tA is this thread
    // so tAgA basically means the single tensor allocated for a single tensor
    Tensor tAgA = local_partition(gA, tA, threadIdx.x);                  // (thr_m,thr_k,k) i.e. (4, 1, 512)
    Tensor tAsA = local_partition(sA, tA, threadIdx.x);                  // (thr_m,thr_k)   i.e. (4, 1)
  
    Tensor tBgB = local_partition(gB, tB, threadIdx.x);                  // (thr_n,thr_k,k) i.e. (4, 1, 512)
    Tensor tBsB = local_partition(sB, tB, threadIdx.x);                  // (THR_N,THR_K)   i.e. (4, 1)
  
    //
    // Define A/B partitioning and C accumulators
    //
  
    // Partition sA(BLK_M,BLK_K) by the rows of tC(THR_M,THR_N)
    // (128, 8) -> ((16), (8, 8)) -> (8, 8)
    Tensor tCsA = local_partition(sA, tC, threadIdx.x, Step<_1, X>{});   // (8, 8)
    Tensor tCsB = local_partition(sB, tC, threadIdx.x, Step< X,_1>{});   // (8, 8)
    // (128, 128) -> ((16, 16), (8, 8)) -> (8, 8)
    Tensor tCgC = local_partition(gC, tC, threadIdx.x, Step<_1,_1>{});   // (8, 8)
  
    // Allocate the accumulators -- same shape/layout as the partitioned data
    Tensor tCrC = make_tensor_like(tCgC);                                // (8, 8)
    // Clear the accumulators
    clear(tCrC);
  
    // TUTORIAL: Example of a simple mainloop that read tiles of data into shared memory,
    //           and then computes on those tiles.
    //   copy(.) operates on the global and shared memory via the tA|tB partitioning
    //   gemm(.) operates on the shared and register memory via the tC partitioning
  
    auto K_TILE_MAX = size<2>(tAgA); // k = K // BLK_K = 4096 // 8 = 512 (in this case)
  
    for (int k_tile = 0; k_tile < K_TILE_MAX; ++k_tile)
    {
      // Copy gmem to smem with tA|tB thread-partitioned tensors
      // tAgA (thr_m, thr_k, k) i.e. (4, 1, 512) in our case
      // tAsA (thr_m, thr_k) i.e. (4, 1) in our case
      // Note that whole block would use the shared memory, (4, 1) actually will multiply (THR_M, THR_K) i.e. (32, 8)
      // and then fill up entire shared memory (128, 8)
      copy(tAgA(_,_,k_tile), tAsA);      // A   (THR_M,THR_K) -> (THR_M,THR_K)
      copy(tBgB(_,_,k_tile), tBsB);      // B   (THR_N,THR_K) -> (THR_N,THR_K)
  
      cp_async_fence();        // Label the end of (potential) cp.async instructions
      cp_async_wait<0>();      // Sync on all (potential) cp.async instructions
      __syncthreads();         // Wait for all threads to write to smem
  
      // Compute gemm on tC thread-partitioned smem
      gemm(tCsA, tCsB, tCrC);            // (THR_M,THR_N) += (THR_M,BLK_K) * (THR_N,BLK_K)
      __syncthreads();         // Wait for all threads to read from smem
    }
  
    //
    // Epilogue
    //
  
    axpby(alpha, tCrC, beta, tCgC);
  }
  ```

## CUTE Tutorials

ä¸»è¦æ€»ç»“ Concepts

### Layout

[layout](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/01_layout.md)

- Dynamic and static int

  åœ¨ä»‹ç» layout ä¹‹å‰å…ˆç®€è¦ä»‹ç»ä¸‹ cutlass ä¸­è¡¨ç¤º int çš„æ–¹å¼ã€‚æœ‰ä¸¤ç§ï¼Œä¸€ç§å°±æ˜¯ dynamic intï¼Œå¦ä¸€ç§æ˜¯ static int

  ```c++
  int{2};   // dynamic
  2;		  // also dynamic
  Int<2>{}; // static
  _2{};	  // equal with Int<2>{}, using cute::_2 = cute::Int<2> 
  ```

- What is Layout?

  > A `Layout` is a pair of (`Shape`, `Stride`). Semantically, it implements a mapping from any coordinate within the Shape to an index via the Stride. 
  >
  > In CuTe, `Layout` is a first class citizen.

  æ­£å¦‚ä¸Šé¢æ‰€è¯´ï¼Œlayout æ˜¯ç”± shape å’Œ stride ç»„æˆçš„ pairï¼Œè€Œ shape å’Œ stride æœ¬èº«æ˜¯ tuple (IntTuple to be exact)ï¼Œlayout æä¾›äº†ä¸€å¥—æŠ½è±¡ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°æè¿° multi-dimensional array çš„æ’å¸ƒï¼Œå¹¶ä¸”æ–¹ä¾¿æˆ‘ä»¬å¯¹æ’å¸ƒè¿›è¡Œå˜æ¢å’Œæ“ä½œã€‚å½“ä½ å» print ä¸€ä¸ª cutlass layout æ—¶ï¼Œå°±ä¼šè·å¾—  `shape:stride` è¿™æ ·çš„è¡¨ç¤º

  é‚£ä¹ˆè¿™å°±å¼•å‡ºäº†æ¥ä¸‹æ¥çš„é—®é¢˜ï¼šwhat is shape and stride?

- Shape & Stride

  Shape å…¶å®æ²¡ä»€ä¹ˆå¥½ä»‹ç»çš„ï¼Œå°±æ˜¯æè¿° layout çš„å½¢çŠ¶çš„ã€‚å’Œ pytorch ä¸­çš„ tensor.shape ç±»ä¼¼ï¼Œä¸åŒçš„æ˜¯ Shape ä¹Ÿå¯ä»¥åµŒå¥—: `(make_shape(2, make_shape (2,2))`ï¼Œè¿™æ ·æˆ‘ä»¬å°±æœ‰ä¸€ä¸ªå½¢çŠ¶ `(2, (2, 2))` çš„ shapeã€‚å…¶å®åµŒå¥—çš„ shapr or stride æ²¡ä»€ä¹ˆ fancy çš„ä¸œè¥¿ï¼Œé€šå¸¸ç”¨æ¥è¡¨ç¤º sub layout/tensorï¼Œåœ¨æˆ‘ä»¬åœ¨è¿›è¡Œ tiling or dividing çš„æ—¶å€™ä¼šæ¯”è¾ƒæœ‰ç”¨ï¼Œä½ å…¶å®å°±æŠŠåµŒå¥—çš„æ‹¬å·æ‰“å¼€ï¼ŒæŠŠè¿™ä¸ªçœ‹ä½œä¸€ä¸ªå¤šç»´ tensor ä¹Ÿ OK çš„

  Stride å¯ä»¥è¯´æ˜¯æè¿° layout æ’å¸ƒçš„å…³é”®æ‰€åœ¨ï¼Œå®ƒå‘Šè¯‰äº†æˆ‘ä»¬å…ƒç´ ä¹‹é—´çš„é—´éš”åˆ°åº•æ˜¯å¤šå°‘ã€‚æˆ‘ä»¬å¯ä»¥è¡¨ç¤ºä¸€ä¸ª column-major çš„çŸ©é˜µå¦‚ä¸‹

  ```c++
  // Shape :  (4,2)
  // Stride:  (1,4)
    0   4
    1   5
    2   6
    3   7
  ```

  è€Œè¡¨ç¤ºä¸€ä¸ª row-major çš„çŸ©é˜µåªéœ€è¦å°† stride åè¿‡æ¥å³å¯

  ```c++
  // Shape :  (4,2)
  // Stride:  (2,1)
    0   1
    2   3
    4   5
    6   7
  ```

  åœ¨ cutlass ä¸­ä¹Ÿç§° column-major ä¸º LayoutLeftï¼Œè€Œ row-major ä¸º LayoutRightã€‚å¦‚æœä½ åœ¨æ„å»º layout çš„æ—¶å€™ä¸æŒ‡å®š strideï¼Œå°†é»˜è®¤ä½¿ç”¨ LayoutLeftã€‚ç”¨ä¸€ä¸ªæ›´åŠ é€šç”¨çš„è¡¨ç¤º LayoutLeft

  ```c++
  // Shape : (x1, x2, x3, ..., xn)
  // LayoutLeft: (1, x1, x1Â·x2, ..., x1Â·x2Â·x3Â·...Â·xn-1)
  ```

- Creation & Use

  å¯ä»¥ä½¿ç”¨ `make_layout & make_shape & make_tuple` æ¥åˆ›å»º layout

  ```c++
  Layout s8 = make_layout(Int<8>{});
  Layout s2xs4 = make_layout(make_shape(Int<2>{},Int<4>{}));
  Layout s2xd4_a = make_layout(make_shape (2,4), make_stride(12, 1}));
  ```

  åˆ›å»ºå®Œ layout è¿‡åå¯ä»¥é€šè¿‡ cutlass å†…å»ºæ–¹æ³•æ¥æŸ¥çœ‹ layout çš„ä¸€äº›æ€§è´¨ï¼šrank, depth, shape, stride, size

- **Coordinates & Index Conversion/Mapping**

  Coordinates & Index Conversion å¯ä»¥è¯´å°±æ˜¯ layout æœ€é‡è¦çš„æ ¸å¿ƒé€»è¾‘ï¼ä¸€å…±æœ‰ä¸¤ç§ç±»å‹çš„ Mapping:

  1. the map from an input coordinate to the corresponding natural coordinate via the `Shape`
  2. the map from a natural coordinate to the index via the `Stride`

  ä¸Šé¢å‡ºç°äº†ä¸¤ä¸ªæ–°çš„æ¦‚å¿µï¼š

  1. input coordinate
  2. natural coordinate

  coordinate å¯¹æˆ‘æ¥è¯´æ˜¯ç†Ÿæ‚‰çš„ï¼Œå…¶å®å°±æ˜¯ tensor çš„ç´¢å¼•ï¼Œæˆ–è€…è¯´ç©ºé—´ä¸­çš„åæ ‡ï¼ŒåŠ ä¸€ä¸ª natural å°±å˜å¾—æ¯”è¾ƒè¿·æƒ‘äº†ã€‚ä¸ªäººç†è§£ï¼šä¸å…¶è¯´æ˜¯ natural coordinate ä¸å¦‚è¯´æ˜¯ natural layoutï¼Œå› ä¸º coordinate æœ¬èº«æ²¡æœ‰ä»€ä¹ˆå¥½å˜åŒ–çš„ï¼Œåªæœ‰å½“ coordinate é…åˆ stride æ—¶æ‰æœ‰æ›´å¤šçš„åŒºåˆ†ã€‚**è€Œ natural layout å…¶å®å°±æ˜¯ LayoutLeft**ï¼

  é‚£ä¹ˆä»€ä¹ˆæ˜¯ input coordinate? ä¸ªäººç†è§£ï¼šå…¶å®æ˜¯ cutlass å¯¹ natural coordinate çš„æ‹“å±•ï¼Œæˆ–è€…è¯´æ³›åŒ–ã€‚å¯¹äºä¸€ä¸ª n-D shape çš„ coordinateï¼Œå¯ä»¥è½¬æ¢ä¸ºå…¶ä»–ç»´åº¦çš„ coordinateã€‚åœ¨ tutorial ä¸­ä¸¾äº†ä¸€ä¸ª 3-D shape `(3, (2, 3))` çš„ coordinate åœ¨ 2-D å’Œ 1-D coordinate ä¹‹é—´çš„è½¬æ¢

  | 1-D  | 2-D     | Natural     |      | 1-D  | 2-D     | Natural     |
  | ---- | ------- | ----------- | ---- | ---- | ------- | ----------- |
  | `0`  | `(0,0)` | `(0,(0,0))` |      | `9`  | `(0,3)` | `(0,(1,1))` |
  | `1`  | `(1,0)` | `(1,(0,0))` |      | `10` | `(1,3)` | `(1,(1,1))` |
  | `2`  | `(2,0)` | `(2,(0,0))` |      | `11` | `(2,3)` | `(2,(1,1))` |
  | `3`  | `(0,1)` | `(0,(1,0))` |      | `12` | `(0,4)` | `(0,(0,2))` |
  | `4`  | `(1,1)` | `(1,(1,0))` |      | `13` | `(1,4)` | `(1,(0,2))` |
  | `5`  | `(2,1)` | `(2,(1,0))` |      | `14` | `(2,4)` | `(2,(0,2))` |
  | `6`  | `(0,2)` | `(0,(0,1))` |      | `15` | `(0,5)` | `(0,(1,2))` |
  | `7`  | `(1,2)` | `(1,(0,1))` |      | `16` | `(1,5)` | `(1,(1,2))` |
  | `8`  | `(2,2)` | `(2,(0,1))` |      | `17` | `(2,5)` | `(2,(1,2))` |

  é‚£ä¹ˆè¿™äº›ç»´åº¦çš„ coordinate ä¹‹é—´æ˜¯æ€ä¹ˆè½¬åŒ–çš„å‘¢ï¼Ÿå¾ˆæ˜¾ç„¶ä»–ä»¬åº”è¯¥éƒ½æœ‰ä¸€ä¸ªç›¸åŒçš„ idï¼Œä½¿å¾—ä»–ä»¬æœ¬è´¨æ˜¯ç­‰ä»·çš„ï¼Œè¿™ä¸ª id å°±æ˜¯ **index**

  è®¡ç®— index çš„æ–¹å¼å¾ˆç®€å•ï¼šå°±æ˜¯ coordinate å’Œ stride çš„å†…ç§¯ (inner product)

  ```c++
  // Shape: (M, N, K)
  // Stride: (1, M, MN), LayoutLeft/natural layout
  // Coord: (i, j, k)
  index = i*1 + j*M + k*MN
  ```

  è¿™ä¸ªå…¬å¼å°±èƒ½å¤Ÿå®Œæˆä» coordinate åˆ° index çš„æ˜ å°„ï¼Œå¹¶ä¸”é€šè¿‡è¿™ä¸ªå…³ç³»ï¼Œæˆ‘ä»¬ä¹Ÿèƒ½å¤Ÿåœ¨å¤šä¸ªç»´åº¦ä¹‹é—´è½¬æ¢å¾—åˆ°ç­‰ä»·çš„åæ ‡

  NOTE: å…¶å® stride çš„å®šä¹‰å¯ä»¥éå¸¸çµæ´»ï¼Œä¾‹å¦‚ä¹‹å‰å®šä¹‰è¿‡ layout

  ```c++
  Layout s2xd4_a = make_layout(make_shape (2,4), make_stride(12, 1}));
  // (2,4):(12,1)
  ```

  æ˜¾ç„¶è¿™ä¸ª stride éå¸¸åœ°ä¸ naturalï¼Œæ‰€è®¡ç®—å¾—åˆ°çš„ index è¿œè¿œè¶…è¿‡äº† natural layout ä¸­æœ€å¤§çš„ index

- Congruent

  æ‰€è°“çš„ congruent å°±æ˜¯æŒ‡çš„ shape å’Œ stride ä»–ä»¬ tuple profiles æ˜¯ä¸€æ ·çš„ã€‚æ¢å¥è¯è¯´ï¼šshape æœ‰å‡ ä¸ªç»´åº¦ï¼Œstride å°±æœ‰å‡ ä¸ªç»´åº¦

  ```c++
  (2, 2):(1, 2) // congruent
  (2, 2):(3)	// not congruent
  ```

- Print

  cutlass cute æä¾›äº†ä¸€äº› print å‡½æ•°å¸®åŠ©æˆ‘ä»¬æŸ¥çœ‹ layout ä¿¡æ¯ï¼Œä¾‹å¦‚ `print_layout & print_latex`

### Layout Algebra

Only focus on the [Divide](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md#division-tiling) and [Product](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md#product-tiling) section

### Tensor

Basically the application of Layout

### Algorithms

Functions that we can use

- [layout algebra](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md)
- tensor
- algorithms

### MMA Atom

> An "MMA atom" in the CUTLASS library refers to a basic building block or operation unit for performing these matrix multiply-accumulate operations efficiently on NVIDIA GPUs.
>
> The term "atom" in MMA atom is used metaphorically to describe a fundamental unit or building block of computation within this context.



- CTA Cooperative Thread Array

  The simplest of the tutorial examples covers the basics of partitioning the global memory into tiles across the CTAs (also called threadblocks in CUDA), partitioning the data tiles across the threads of each CTA, and writing a mainloop using `cute::copy` and `cute::gemm`.

  - `CtaTiler`. A CuTe [tiler concept](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md#composition-tilers) that determines how to extract a tile of data from the problem shape.
  - At the highest level, the work is distributed across CTAs. In principle, each CTA's tile could come from the input tensors in many different ways. Many [CuTe `Tiler`s](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md#composition-tilers) could be used to tile the data, but for these cases it is sufficient to simply use the shape of the desired CTA tile.

- zipped_divide

  æœ¬æ¥æƒ³å°±çœ‹ä¸‹ zipped_divideï¼Œä½†å¯ä»¥é¡ºæ‰‹æŠŠ logical divide ç»™å­¦äº†ï¼Œéƒ½åœ¨ [layout algebra](https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/02_layout_algebra.md) é‡Œé¢

- drawing from each level: how gemm is divided from block to thread, and we have different layout to deal with computation (global mem & shared mem)

- From a bigger picture: an intuitive way to think about layout algebra, where is the bottleneck to understanding the process?

Key functions

- `make_tensor`

- `make_coord`

- `make_gemm_ptr`

- `make_smem_ptr`

- `make_shape`

- `make_strid`

- `make_layout`

- `size`

- `local_tile`

  what is `Step` trying to do?

- `local_partition`

  seems like there are reload functions of `local_partition`

- `copy`

- `gemm`

Everything is layout in cutlass: problem layout, block layout, thread layout, memory layout, **use layout algebra to solve them in a unified view**

Other reference

- [CUDA MODE lecture 15](https://www.bilibili.com/video/BV1QZ421N7pT?spm_id_from=333.788.videopod.episodes&p=15) checked, pretty useful
- [Reed's zhihu posts](https://www.zhihu.com/people/reed-84-49/posts), not checked
- [CUTLASS CuTeå®æˆ˜(ä¸€)-åŸºç¡€](https://zhuanlan.zhihu.com/p/690703999), not checked

## Learn Cutlass with DeepSeek-R1

æˆ‘ä¹‹å‰çš„å­¦ä¹ è·¯çº¿æ˜¯ç›´æ¥åˆ‡å…¥åˆ°äº† cute å½“ä¸­ï¼Œåœ¨ cute çš„å­¦ä¹ ä¸­æˆ‘å¯¹çŸ©é˜µä¹˜æ³•çš„åˆ‡åˆ†æœ‰äº†ä¸é”™çš„è®¤è¯†ã€‚ä½†æˆ‘åˆå‘ç°ä¸€ä¸ªäº‹å®ï¼Œé‚£å°±æ˜¯ cute çš„ example å®é™…ä¸Šå¹¶ä¸å¤šï¼Œè¿™è¯´æ˜å¤§é‡çš„ä½¿ç”¨èŒƒå›´å¯èƒ½å¹¶ä¸åœ¨ cuteã€‚å¯¹äºè¿™ä¸ªè§‚ç‚¹ï¼ŒDeepSeek ç»™å‡ºäº† cutlass å’Œ cute ä¹‹é—´çš„å·®å¼‚

1. **CUTLASS ä¸ CUTE çš„å®šä½å·®å¼‚**

| **ç»´åº¦**     | **CUTLASS**                      | **CUTE**                                 |
| ------------ | -------------------------------- | ---------------------------------------- |
| **æŠ½è±¡å±‚çº§** | é«˜å±‚æ¨¡æ¿åº“ï¼ˆGEMM/Conv ç®—å­çº§ï¼‰   | åº•å±‚ç¼–ç¨‹æ¨¡å‹ï¼ˆçº¿ç¨‹çº§æ•°æ®æµæŠ½è±¡ï¼‰         |
| **æ ¸å¿ƒç›®æ ‡** | æä¾›é«˜æ€§èƒ½çº¿æ€§ä»£æ•°ç®—å­çš„é€šç”¨å®ç° | è§£è€¦æ•°æ®å¸ƒå±€ä¸è®¡ç®—é€»è¾‘ï¼Œå®ç°æè‡´çµæ´»æ€§   |
| **å…¸å‹åº”ç”¨** | æ ‡å‡† GEMM/Conv2d/æ‰¹å¤„ç†åœºæ™¯      | éè§„åˆ™ç®—å­ï¼ˆå¦‚ Attentionï¼‰ã€ç¡¬ä»¶ç‰¹æ€§æ¦¨å– |
| **ä»£ç å…³ç³»** | CUTLASS 3.0+ åŸºäº CUTE é‡æ„      | ä½œä¸º CUTLASS çš„åº•å±‚å¼•æ“ï¼Œå¯ç‹¬ç«‹ä½¿ç”¨      |

2. **ä½•æ—¶åº”è¯¥æ·±å…¥ CUTEï¼Ÿ**

   - **æ¨èä½¿ç”¨ CUTE çš„åœºæ™¯**

     - **éæ ‡å‡†æ•°æ®å¸ƒå±€**  

       éœ€è¦å¤„ç†äº¤é”™å­˜å‚¨ï¼ˆinterleavedï¼‰ã€å—ç¨€ç–ï¼ˆblock-sparseï¼‰ç­‰å¤æ‚å†…å­˜æ¨¡å¼æ—¶ï¼ŒCUTE çš„ `Layout` å’Œ `Copy` åŸå­æ“ä½œèƒ½ç®€åŒ–è·¨æ­¥è®¿é—®é€»è¾‘ã€‚

     - **è‡ªå®šä¹‰è®¡ç®—æ¨¡å¼**  

       ä¾‹å¦‚å®ç° Flash Attention ä¸­çš„åˆ†å— Softmax æˆ– Mask å¤„ç†ï¼ŒCUTE çš„ `TiledMMA` å’Œ `Pipeline` èƒ½ç²¾ç¡®æ§åˆ¶æµæ°´çº¿ã€‚

     - **ç¡¬ä»¶ç‰¹æ€§æ¦¨å–**  

       åœ¨ Hopper æ¶æ„ä¸Šåˆ©ç”¨ TMAï¼ˆTensor Memory Acceleratorï¼‰å¼‚æ­¥æ‹·è´æ—¶ï¼Œéœ€é€šè¿‡ CUTE çš„ `Copy_Atom` æ˜¾å¼ç®¡ç†æ•°æ®ä¼ è¾“ã€‚


   - **CUTLASS é«˜å±‚ API æ›´ä¼˜çš„åœºæ™¯**

     - **æ ‡å‡† GEMM/Conv**  

       ç›´æ¥ä½¿ç”¨ `cutlass::gemm::device::Gemm` ç­‰é¢„å®šä¹‰æ¨¡æ¿ï¼Œé€šè¿‡å‚æ•°è°ƒæ•´å³å¯è·å¾— 90%+ çš„ç¡¬ä»¶åˆ©ç”¨ç‡ã€‚

     - **å¿«é€ŸåŸå‹å¼€å‘**  

       CUTLASS çš„ `Epilogue` æœºåˆ¶èƒ½å¿«é€Ÿå®ç°èåˆæ“ä½œï¼ˆå¦‚ ReLU + GEMMï¼‰ï¼Œæ— éœ€æ·±å…¥çº¿ç¨‹çº§ç¼–æ’ã€‚

è¿™æ ·çœ‹æ¥ cute åº”è¯¥æ˜¯ä¸€ç§æ›´åŠ çµæ´»ï¼ˆåº•å±‚ï¼‰çš„ CUDA ç¼–ç¨‹æŠ€æœ¯ï¼Œæ‰€ä»¥è¯´å…ˆå¯¹ cutlass æ¨¡æ¿è¿›è¡Œå­¦ä¹ åº”è¯¥æ˜¯æ›´å¥½çš„é€‰æ‹©ï¼Œæ›´å¥½ä¸Šæ‰‹ã€‚æˆ‘ç»™äº† DeepSeek ä¸€ä¸ª promptï¼Œæˆ‘æœ‰ä¸€å®šçš„ CUDA åŸºç¡€ï¼Œæˆ‘æƒ³è¦åœ¨ä¸¤å¤©ä¹‹å†…å­¦ä¹  cutlassï¼Œåº”è¯¥å¦‚ä½•å®‰æ’æˆ‘çš„å­¦ä¹ è®¡åˆ’ğŸ˜ä¸‹é¢å°±è·Ÿç€ DeepSeek çš„æ€»ç»“å¼€å§‹å­¦ä¹ ï¼Œåé¢ç®€ç§° DeepSeek-R1 ä¸º DS

### æ¨¡æ¿å…ƒç¼–ç¨‹

æˆ‘é¦–å…ˆè®© DS ä»‹ç»ä¸‹ cutlass ä¸­çš„æ¨¡ç‰ˆå…ƒç¼–ç¨‹ï¼Œä»–ç»™å‡ºäº†å¦‚ä¸‹çš„ä¼˜åŠ¿

1. ç¼–è¯‘æ—¶å¤šæ€ï¼šé€šè¿‡æ¨¡æ¿å‚æ•°åœ¨ç¼–è¯‘æ—¶ç¡®å®šç±»å‹å’Œç®—æ³•ï¼Œé¿å…è¿è¡Œæ—¶åˆ†æ”¯ã€‚
2. é›¶æˆæœ¬æŠ½è±¡ï¼šç”Ÿæˆçš„ä»£ç ç›´æ¥é’ˆå¯¹ç¡¬ä»¶ç‰¹æ€§ä¼˜åŒ–ï¼ˆå¦‚Tensor Coreï¼‰ï¼Œæ— è¿è¡Œæ—¶å¼€é”€ã€‚
3. ç»„åˆå¼è®¾è®¡ï¼šé€šè¿‡åµŒå¥—æ¨¡æ¿å‚æ•°ç»„åˆçº¿ç¨‹å—ã€Warpã€æŒ‡ä»¤é›†ç­‰å±‚çº§çš„åˆ†å—ç­–ç•¥ã€‚

è¿™ä¸‰ä¸ªä¼˜ç‚¹éƒ½éå¸¸çš„æŠ½è±¡ï¼Œæ ¹æœ¬æ²¡åŠæ³•ç†è§£ã€‚ä¸ºä»€ä¹ˆè¦é‡‡ç”¨æ¨¡æ¿å…ƒç¼–ç¨‹çš„å½¢å¼ï¼Œè€Œä¸ç›´æ¥é‡‡ç”¨ CUDA C++ è¿›è¡Œç¼–ç¨‹ï¼Ÿæ¥ä¸‹æ¥ DS ç»™å‡ºäº†éå¸¸å…·ä½“çš„å›ç­”ï¼ŒåŸºæœ¬è§£å†³ç–‘æƒ‘ã€‚

Cutlass æƒ³è¦è§£å†³çš„ä¸€ä¸ªé‡è¦é—®é¢˜ï¼šå®ç°ä¸€ä¸ªé«˜æ•ˆçš„ GEMMã€‚åœ¨è¿™ä¸ªé—®é¢˜èƒŒåæœ‰å¾ˆå¤šå¤æ‚éœ€æ±‚ï¼š

1. å¤šæ•°æ®ç±»å‹ï¼šfloat & half & int
2. å¤šç¡¬ä»¶æ¶æ„ï¼šPascalï¼ˆsm60ï¼‰ã€Voltaï¼ˆsm70ï¼‰ã€Ampereï¼ˆsm80ï¼‰
3. å¤šåˆ†å—ç­–ç•¥ï¼šçº¿ç¨‹å—åˆ†å—å¤§å°ï¼ˆ128x128 vs 256x64ï¼‰

**å¦‚æœè¦ä½¿ç”¨ä¼ ç»Ÿçš„ CUDA C++ ç¼–ç¨‹ï¼ŒåŠ¿å¿…è¦ä½¿ç”¨å¤§é‡çš„ if else åˆ¤æ–­æˆ–è€…è¿è¡Œæ—¶å¤šæ€ï¼ˆè™šå‡½æ•°æˆ–æŒ‡é’ˆå‡½æ•°ï¼‰æ¥å®Œæˆå¦‚æ­¤å¤šç§ç±»çš„ GEMMã€‚**ä½†æ˜¯è¿™æ ·å¿…ç„¶æ‰å…¥åˆ°äº†ä½æ•ˆé™·é˜±ä¸­ï¼Œè¿è¡Œæ—¶æ¯æ¬¡ if else åˆ†æ”¯æˆ–è€…è™šå‡½æ•°è°ƒç”¨éƒ½ä¼šå¯¼è‡´é¢å¤–çš„å¼€é”€ï¼Œè€Œä¸”ä¼šå¯¼è‡´ä»£ç è†¨èƒ€é—®é¢˜ï¼Œå³æ‰€æœ‰çš„ä»£ç é€»è¾‘éƒ½è¢«ç¼–è¯‘åˆ°äº†åŒä¸€ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶å½“ä¸­ï¼Œå¹¶ä¸”å¾ˆå¤§éƒ¨åˆ†çš„äºŒè¿›åˆ¶æ–‡ä»¶éƒ½ä¸ä¼šè¢«æ‰§è¡Œ

```c++
// è¿è¡Œæ—¶é€šè¿‡åˆ†æ”¯é€‰æ‹©é€»è¾‘
__global__ void gemm_kernel(
    int M, int N, int K,
    void* A, void* B, void* C,
    DataType dtype,     // æ•°æ®ç±»å‹ï¼šfloat æˆ– half
    Arch arch,          // ç¡¬ä»¶æ¶æ„ï¼šsm60/sm70/sm80
    TileShape tile      // åˆ†å—ç­–ç•¥ï¼š128x128 æˆ– 256x64
) {
    // è¿è¡Œæ—¶åˆ†æ”¯åˆ¤æ–­æ•°æ®ç±»å‹
    if (dtype == FLOAT) {
        float* A_f = static_cast<float*>(A);
        float* B_f = static_cast<float*>(B);
        // è¿›ä¸€æ­¥åˆ¤æ–­ç¡¬ä»¶æ¶æ„
        if (arch == SM80) {
            // ä½¿ç”¨ Tensor Core çš„ä»£ç 
            if (tile == TILE_128x128) { ... }
            else { ... }
        } else if (arch == SM60) { ... }
    } else if (dtype == HALF) {
        half* A_h = static_cast<half*>(A);
        half* B_h = static_cast<half*>(B);
        // åŒæ ·åµŒå¥—åˆ†æ”¯...
    }
}
```

**æ­¤æ—¶ï¼Œæ¨¡ç‰ˆå…ƒç¼–ç¨‹å°±æ´¾ä¸Šç”¨åœºï¼Œå°†å„ä¸ªé€‰é¡¹ä½œä¸ºæ¨¡æ¿çš„å‚æ•°ä¼ å…¥ç»™æ¨¡æ¿ï¼Œè¿™æ ·å°±èƒ½å°†æ¨¡ç‰ˆè¿›è¡Œå®ä¾‹åŒ–**

```c++
// å®šä¹‰æ¨¡æ¿å‚æ•°
using Gemm = cutlass::gemm::device::Gemm<
    half_t,                     // ElementA
    cutlass::layout::RowMajor,  // LayoutA
    half_t,                     // ElementB
    cutlass::layout::ColumnMajor, // LayoutB
    half_t,                     // ElementC
    cutlass::layout::RowMajor,
    cutlass::arch::OpClassTensorOp, // ä½¿ç”¨ Tensor Core
    cutlass::arch::Sm80,           // Ampere æ¶æ„
    cutlass::gemm::GemmShape<128, 128, 32>, // çº¿ç¨‹å—åˆ†å—
    cutlass::gemm::GemmShape<64, 64, 32>    // Warp åˆ†å—
>;

// ç¼–è¯‘å™¨ä¼šç”Ÿæˆä¸€ä¸ªä¸“ç”¨çš„æ ¸å‡½æ•°ï¼š
// - ä½¿ç”¨ half æ•°æ®ç±»å‹
// - å¯ç”¨ Tensor Core (mma.sync æŒ‡ä»¤)
// - åˆ†å—ç­–ç•¥ 128x128x32
// - ä»…é€‚ç”¨äº sm80 æ¶æ„
```

è¿™æ ·åšå°±è§£å†³äº†ä¹‹å‰çš„èŠ±é”€é—®é¢˜ï¼Œå› ä¸ºæ‰€æœ‰çš„åˆ†æ”¯åœ¨ç¼–è¯‘æ—¶ä¼šè¿›è¡Œé™æ€å±•å¼€ï¼Œæ²¡æœ‰ä»»ä½•çš„åˆ¤æ–­é€»è¾‘ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œä»£ç çš„å¯ç»´æŠ¤æ€§ä¹Ÿæå¤§å¢åŠ ï¼Œæˆ‘ä»¬å¯ä»¥ä¸“æ³¨äºç¼–å†™æ¯ä¸€ä¸ªä¸åŒç‰¹æ€§çš„ gemm

æ­¤æ—¶å¦ä¸€ä¸ªé—®é¢˜äº§ç”Ÿäº†ï¼šæ¨¡ç‰ˆå…ƒç¼–ç¨‹å’Œæ¨¡ç‰ˆç¼–ç¨‹æœ‰ä»€ä¹ˆæœ¬è´¨ä¸Šçš„åŒºåˆ«å—ï¼Ÿè¿™é‡Œ DS åˆ†æˆäº†ä¸‰ä¸ªæ­¥éª¤æ¥è§£é‡Šï¼š

1. ä»€ä¹ˆæ˜¯å…ƒç¼–ç¨‹

   **å…ƒç¼–ç¨‹ï¼ˆMetaprogrammingï¼‰** çš„æœ¬è´¨æ˜¯ **â€œç¼–å†™èƒ½å¤Ÿç”Ÿæˆä»£ç çš„ä»£ç â€**ã€‚å®ƒé€šè¿‡ç¨‹åºé€»è¾‘æ“çºµå¦ä¸€æ®µç¨‹åºçš„ç»“æ„ï¼Œç”šè‡³æ”¹å˜ç¼–è¯‘å™¨çš„è¡Œä¸ºã€‚åœ¨ C++ ä¸­ï¼Œæ¨¡æ¿å…ƒç¼–ç¨‹åˆ©ç”¨æ¨¡æ¿ç³»ç»Ÿåœ¨ **ç¼–è¯‘æ—¶** å®Œæˆè¿™äº›æ“ä½œï¼Œè€Œä¸æ˜¯è¿è¡Œæ—¶

   ç±»æ¯”è§£é‡Šï¼š

   - **æ™®é€šç¼–ç¨‹**ï¼šä½ ç›´æ¥ç¼–å†™å¤„ç†æ•°æ®çš„ä»£ç ï¼ˆä¾‹å¦‚æ’åºç®—æ³•ï¼‰ã€‚
   - **å…ƒç¼–ç¨‹**ï¼šä½ ç¼–å†™ä¸€æ®µä»£ç ï¼Œè¿™æ®µä»£ç çš„ **è¾“å‡ºæ˜¯å¦ä¸€æ®µä»£ç **ï¼ˆä¾‹å¦‚ç”Ÿæˆç‰¹å®šç¡¬ä»¶çš„ä¼˜åŒ–æ’åºç®—æ³•ï¼‰ã€‚

2. æ¨¡ç‰ˆç¼–ç¨‹ vs æ¨¡ç‰ˆå…ƒç¼–ç¨‹

   DS é’ˆå¯¹è¿™ä¸¤ä¸ªç¼–ç¨‹æ–¹å¼ä¸¾äº†ä¾‹å­

   **æ™®é€šæ¨¡æ¿ç¼–ç¨‹ï¼ˆGeneric Programmingï¼‰**

   - ç›®æ ‡ï¼šå®ç°ç±»å‹æ— å…³çš„é€šç”¨ä»£ç 

   - Example

     ```c++
     template <typename T>
     T add(T a, T b) { return a + b; }
     ```

     ç¼–è¯‘å™¨ä¸º `add<int>` å’Œ `add<float>` ç”Ÿæˆä¸åŒçš„å‡½æ•°ã€‚å…¶æœ¬è´¨æ˜¯ç±»å‹å‚æ•°åŒ–ï¼Œé¿å…é‡å¤ä»£ç 

   **æ¨¡æ¿å…ƒç¼–ç¨‹ï¼ˆTMPï¼‰**

   - ç›®æ ‡ï¼šåœ¨ç¼–è¯‘æ—¶è®¡ç®—å€¼ã€ç”Ÿæˆä»£ç æˆ–åšå†³ç­–

   - Example ç¼–è¯‘æ—¶çš„é˜¶ä¹˜è®¡ç®—

     ```c++
     template <int N>
     struct Factorial {
         static const int value = N * Factorial<N-1>::value;
     };
     template <>
     struct Factorial<0> { static const int value = 1; };
     
     int main() {
         constexpr int fact_5 = Factorial<5>::value; // ç¼–è¯‘æ—¶è®¡ç®—å‡º120
     }
     ```

     **å…³é”®**ï¼šç¼–è¯‘å™¨é€šè¿‡æ¨¡æ¿é€’å½’å±•å¼€å®Œæˆè®¡ç®—ï¼Œç”Ÿæˆçš„ç»“æœç›´æ¥å†™å…¥äºŒè¿›åˆ¶ã€‚

   é€šè¿‡ä¸Šé¢ä¸¤ä¸ªä¾‹å­èƒ½å¤Ÿå¾ˆæ¸…æ¥šåœ°æ„ŸçŸ¥åˆ°äºŒè€…çš„ç›®æ ‡æœ‰æ˜¾è‘—åŒºåˆ«ã€‚å¦ä¸€ä¸ªæ˜¾è‘—åŒºåˆ«åˆ™æ˜¯ï¼šæ™®é€šæ¨¡æ¿ç¼–ç¨‹ï¼Œæ¨¡ç‰ˆæ‰€æ¥å—çš„æ˜¯ä¸€ä¸ªç±» (like int, float...)ï¼›è€Œæ¨¡ç‰ˆå…ƒç¼–ç¨‹åˆ™æ¥æ”¶çš„æ˜¯ä¸€ä¸ªå…·ä½“ç±»çš„å…·ä½“å€¼ (like 5, 6...)

3. ä¸ºä»€ä¹ˆ cutlass ä½¿ç”¨æ¨¡ç‰ˆå…ƒç¼–ç¨‹

   è¿™ä¸‹å°±å½»åº•ç†è§£äº†ä¸ºä»€ä¹ˆ cutlass ä½¿ç”¨æ¨¡æ¿å…ƒç¼–ç¨‹äº†ï¼š**Cutlass å°†ç¡¬ä»¶ç‰¹æ€§ã€ç®—æ³•ç­–ç•¥å’Œæ•°æ®ç±»å‹è½¬åŒ–ä¸ºç¼–è¯‘æ—¶çš„â€œå…ƒå‚æ•°â€ï¼Œç”Ÿæˆé«˜åº¦å®šåˆ¶çš„å†…æ ¸ã€‚**

   DS ä¸¾äº†ä¸€ä¸ªæ™®é€š CUDA C++ å®ç° vs æ¨¡ç‰ˆå…ƒç¼–ç¨‹å®ç°çš„ä¾‹å­

   ```c++
   __device__ void multiply(float* A, float* B, float* C, int K) {
       for (int k = 0; k < K; ++k) {  // åŠ¨æ€å¾ªç¯ï¼Œå¯èƒ½æœ‰åˆ†æ”¯å¼€é”€
           C[threadIdx.x] += A[k] * B[k];
       }
   }
   
   template <int K>
   __device__ void multiply(float* A, float* B, float* C) {
       if constexpr (K > 0) {
           C[threadIdx.x] += A[K-1] * B[K-1];
           multiply<K-1>(A, B, C); // ç¼–è¯‘æ—¶é€’å½’å±•å¼€
       }
   }
   
   // æ˜¾å¼å®ä¾‹åŒ–æ¨¡æ¿ï¼ˆK=64ï¼‰
   template __device__ void multiply<64>(float*, float*, float*);
   
   ```

   **æ™®é€š CUDA C++ å®ç°çš„é—®é¢˜**ï¼šå¾ªç¯æ¬¡æ•° `K` åœ¨è¿è¡Œæ—¶ç¡®å®šï¼Œç¼–è¯‘å™¨æ— æ³•è‡ªåŠ¨å±•å¼€ã€‚è€Œæ¨¡ç‰ˆå…ƒç¼–ç¨‹å°±å¯ä»¥åœ¨ç¼–è¯‘æ—¶ç›´æ¥å±•å¼€å¾ªç¯ï¼Œä»è€Œé™ä½å¼€é”€

### ThreadBlock/Grid å±‚æ¬¡ç»“æ„

è¿™ä¸ªåœ¨ä¹‹å‰çš„å­¦ä¹ ä¸­å·²ç»æ¯”è¾ƒç†Ÿæ‚‰ï¼Œä¸å†è¿›è¡Œæ•´ç†

### SplitK & SlicedK



`kSplitKSerial` vs `kParallel`

### Cuda Core & Tensor Core (FMA vs MMA)





## Basic gemm

ä½¿ç”¨ cutlass ä¸­çš„ gemmï¼Œè½»æ¾è¾¾åˆ° cublas ä¹æˆåŠŸåŠ›

### include/cutlass/gemm/device/gemm.h

ç†è§£è¿™ä¸€ä¸ªå‡½æ•°çš„åŠŸèƒ½ï¼Œå…¶å®ç°é€»è¾‘

è¿™ä¸ª gemm.h ä¸­å°±æ˜¯ cutlass æš´éœ²ç»™æˆ‘ä»¬çš„ device-level apiï¼Œå¯¹æ ‡çš„å°±æ˜¯ cublas apiï¼Œç›¸å½“äºæ˜¯ä¸€ä¸ª `__global__` kernel functionï¼Œè¿˜ä¸éœ€è¦ä½ æŒ‡å®š grid & block & shared memory

```cpp
  The intent is to provide a convenient mechanism for interacting with most plausible GEMM
  configurations for each supported architecture. Consequently, not all parameters are exposed
  to the top-level interface. Rather, sensible defaults at each level of the CUTLASS hierarchy
  are selected to tradeoff simplicity of the interface with flexibility. We expect 
  most configurations to be specified at this level. Applications with more exotic requirements 
  may construct their kernels of interest using CUTLASS components at the threadblock, warp, 
  and thread levels of abstraction.
```

ç¿»è¯‘

> **ç›®çš„æ˜¯ä¸ºæ¯ä¸ªæ”¯æŒçš„æ¶æ„æä¾›ä¸€ç§ä¾¿æ·çš„äº¤äº’æœºåˆ¶ï¼Œä»¥ä¾¿ä½¿ç”¨æœ€åˆç†çš„GEMMï¼ˆé€šç”¨çŸ©é˜µä¹˜æ³•ï¼‰é…ç½®**ã€‚å› æ­¤ï¼Œå¹¶éæ‰€æœ‰å‚æ•°éƒ½å¼€æ”¾ç»™é¡¶å±‚æ¥å£ï¼Œè€Œæ˜¯é€šè¿‡é€‰æ‹©CUTLASSå±‚æ¬¡ç»“æ„ä¸­æ¯ä¸€çº§çš„åˆç†é»˜è®¤å€¼ï¼Œåœ¨æ¥å£ç®€æ´æ€§å’Œçµæ´»æ€§ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚æˆ‘ä»¬é¢„è®¡å¤§å¤šæ•°é…ç½®å°†åœ¨æ­¤å±‚çº§æŒ‡å®šã€‚å¯¹äºæœ‰ç‰¹æ®Šéœ€æ±‚çš„åº”ç”¨ï¼Œå¼€å‘è€…å¯ä»¥åˆ©ç”¨CUTLASSåœ¨æŠ½è±¡å±‚çº§ï¼ˆçº¿ç¨‹å—çº§ã€warpçº§å’Œçº¿ç¨‹çº§ï¼‰æä¾›çš„ç»„ä»¶ï¼Œè‡ªè¡Œæ„å»ºæ‰€éœ€çš„æ ¸å¿ƒè®¡ç®—æ¨¡å—ã€‚
>
> ï¼ˆæ³¨ï¼šwarpæ˜¯NVIDIA GPUæ¶æ„ä¸­çš„åŸºæœ¬æ‰§è¡Œå•å…ƒï¼Œé€šå¸¸åŒ…å«32ä¸ªå¹¶è¡Œçº¿ç¨‹ï¼Œåœ¨æ­¤ä¿ç•™è‹±æ–‡æœ¯è¯­ä»¥å‡†ç¡®åæ˜ CUDAç¼–ç¨‹æ¨¡å‹çš„æ¦‚å¿µï¼‰

å¯ä»¥çœ‹åˆ°æ¨¡ç‰ˆå½“ä¸­å¾ˆå¤šéƒ½ç»™å®šäº†é»˜è®¤å€¼ï¼Œæ‰€ä»¥åœ¨ä½¿ç”¨æ—¶åªéœ€è¦ç»™å®šæœ€åŸºç¡€çš„5ä¸ªå€¼ï¼Œå°±èƒ½å®ä¾‹åŒ–ä¸€ä¸ª gemm class å‡ºæ¥

```cpp
  using CutlassGemm = cutlass::gemm::device::Gemm<float,        // Data-type of A matrix
                                                  ColumnMajor,  // Layout of A matrix
                                                  float,        // Data-type of B matrix
                                                  ColumnMajor,  // Layout of B matrix
                                                  float,        // Data-type of C matrix
                                                  ColumnMajor>; // Layout of C matrix

```

è€Œä¸” cutlass è¿˜æœ‰æ›´ç»†ç²’åº¦çš„ç»„ä»¶ï¼Œå¯ä»¥åœ¨ä¸‹é¢çš„è·¯å¾„ä¸­æ‰¾åˆ°ï¼š

1. `include/cutlass/gemm/thread`
2. `include/cutlass/gemm/warp`
3. `include/cutlass/gemm/threadblock`

### æ¨¡æ¿éƒ¨åˆ†ç‰¹åŒ–



### column major & row major & layout transpose

ä½¿ç”¨å·§å¦™çš„æ’å¸ƒæ–¹å¼ï¼Œæ¥è§„é¿å¯¹çŸ©é˜µè¿›è¡Œ transposeï¼Œè¿™å°±æ˜¯ cute çš„æ ¸å¿ƒæ€æƒ³ï¼Œlayout algebra

### Class Gemm

### How to debug cutlass

### Components in cutlass gemm

OKï¼Œç›®å‰åˆé‡åˆ°ç“¶é¢ˆäº†ï¼Œæˆ‘ç°åœ¨çŸ¥é“äº† cutlass çš„è°ƒç”¨æµç¨‹ï¼Œä¸»è¦çœ‹äº† `include/cutlass/gemm/kernel/gemm.h`

æ•´ä¸ª gemm è®¡ç®—è¢«æŠ½è±¡åœ¨äº†è¿™ä¸ª `gemm.h` å½“ä¸­ï¼Œä½¿ç”¨ `Mma_ & Epilogue_ & ThreadblockSwizzle & SplitKSerial` å››ä¸ªæ¨¡æ¿æ¥å†³å®šã€‚åŸºæœ¬ä¸Šå°±æ˜¯åˆ©ç”¨è¿™å››ä¸ªæ¨¡æ¿æ¥è¿›è¡Œç»„åˆï¼Œå°±èƒ½å¤Ÿå®Œæˆä¸€ä¸ªæ¯”è¾ƒé«˜æ•ˆ gemm è®¡ç®—

é‚£ä¹ˆé—®é¢˜å°±æ¥äº†ï¼š

1. è¿™å››ä¸ª component åˆ°åº•æœ‰ä»€ä¹ˆç”¨å‘¢ï¼Ÿ

2. è¿™å››ä¸ª component çš„ä»£ç åˆ°åº•åœ¨å“ªé‡Œï¼Œå„ä¸ªéƒ¨åˆ†ä»£è¡¨ç€ä»€ä¹ˆï¼Ÿ**mapping the code to the gemm process is my next step**

3. é™¤äº†è¿™å››ä¸ª component ä¹‹å¤– cutlass ä¼¼ä¹è¿˜æœ‰å…¶ä»–çš„ç»„ä»¶ï¼Œå¦‚ä½•ä½¿ç”¨è¿™äº›ç»„ä»¶æ¥ä¼˜åŒ–æˆ‘ä»¬çš„ gemm or universal kernel

   è¿™ä¸ªé—®é¢˜è¿˜æ˜¯è¿‡äºå®å¤§äº†ï¼Œä¸å¤ªå¥½å›ç­”ï¼Œéœ€è¦æœ‰å…·ä½“é—®é¢˜ï¼ˆexamplesï¼‰çš„å¸¦å…¥ï¼Œå¦åˆ™æ— æ³•è·å¾—åé¦ˆ

4. cutlass 3.x å’Œ cutlass 2.x ä¹‹é—´çš„åŒºåˆ«åˆ°åº•åœ¨å“ªäº›åœ°æ–¹ï¼Œè¿™äº› example åˆ°åº•å±äº cutlass3.x è¿˜æ˜¯ cutlass2.x?

   è¿™ä¸ªé—®é¢˜çš„æ ¹æœ¬åŸå› åœ¨äºå¼•å…¥äº†æ›´é«˜çº§çš„ç¡¬ä»¶ç‰¹æ€§ï¼Œæ‰€ä»¥æ›´æ ¸å¿ƒçš„çŸ¥è¯†ç‚¹åœ¨äºå­¦ä¹ ç¡¬ä»¶ç‰¹æ€§

åœ¨å›ç­”é—®é¢˜ä¹‹å‰ï¼Œæˆ‘è¿˜éœ€è¦äº†è§£ GPU æµæ°´çº¿åˆ°åº•æ˜¯ä»€ä¹ˆæ„æ€ï¼Œå› ä¸ºè¿™ä¸€å¼ å›¾ç»å¸¸å‡ºç°

[cutlass/media/docs/efficient_gemm.md at main Â· NVIDIA/cutlass](https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md) å¿…é¡»ç†è§£ efficient gemm çš„å…³é”®åŸç†

<img src="CUDA Programming 7/software-pipeline.png" alt="ALT" style="zoom:80%;" />

using cutlass gemm api v.s. using cute component

the key is to understand how the gpu computing model is working, and how to make this gpu work efficiently.

TODOï¼šæ•´ç† kernel/gemm.h operator() æµç¨‹ã€‚ç”±äº cutlass æ˜¯æ¨¡æ¿å‡½æ•°ï¼Œå…¶æœ¬æ„å¹¶ä¸æ˜¯æƒ³è®©ä½  debug å¥½ç”¨ï¼Œæ‰€ä»¥æœ€å¥½å½“åš API æ¥ä½¿ç”¨ã€‚è€Œå­¦ä¹  cute æ‰æ˜¯å®Œå…¨ step by step æŒæ¡ cutlass ç¼–ç¨‹ç²¾é«“çš„æ ¸å¿ƒ

[cutlass/media/docs/ide_setup.md at main Â· NVIDIA/cutlass](https://github.com/NVIDIA/cutlass/blob/main/media/docs/ide_setup.md)

[cutlass/media/docs/fundamental_types.md at main Â· NVIDIA/cutlass](https://github.com/NVIDIA/cutlass/blob/main/media/docs/fundamental_types.md)

[cutlass/media/docs/gemm_api.md at main Â· NVIDIA/cutlass](https://github.com/NVIDIA/cutlass/blob/main/media/docs/gemm_api.md)

[cutlass/media/docs/gemm_api_3x.md at main Â· NVIDIA/cutlass](https://github.com/NVIDIA/cutlass/blob/main/media/docs/gemm_api_3x.md)

## Partition

## MMA

## Epilogue

- ç›®å‰æ‰€æœ‰çš„æ•°æ®éƒ½è¿˜åœ¨ accumulator é‡Œé¢ï¼Œè¿˜æ²¡æœ‰ä¿å­˜åˆ°å…¨å±€å†…å­˜å½“ä¸­ï¼Œéœ€è¦é€šè¿‡ epliogue æ¥æŠŠæ•°æ®å­˜æ”¾åˆ°å…¨å±€å†…å­˜ã€‚åœ¨æŠŠæ•°æ®å­˜åˆ°å…¨å±€å†…å­˜ä¹‹å‰ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥åˆ©ç”¨è¿™äº›æ•°æ®åšä¸€äº›é¢å¤–çš„ç®€å•æ“ä½œï¼Œæ“ä½œå®Œè¿‡åå†å­˜ã€‚è¿™é€šå¸¸ä¹Ÿèƒ½èŠ‚çœä¸å°‘çš„æ•°æ®æ¬è¿æ—¶é—´ï¼Œå¦åˆ™è¿˜å¾—å†ä»å…¨å±€å†…å­˜ä¸­è¯»å‡ºæ¥ï¼Œå®Œæˆè¿™äº›ç®€å•æ“ä½œ

TODO: here is the next focus!!!!!!!

# Efficient Gemm

[cutlass efficient gemm doc](https://github.com/NVIDIA/cutlass/blob/main/media/docs/efficient_gemm.md)

ç†è§£è¿™ä¸ªæ–‡æ¡£å¯¹ç†è§£ gemm é«˜æ•ˆå®ç°éå¸¸é‡è¦ã€‚åŒæ—¶èƒ½å¤Ÿå»ºç«‹è‰¯å¥½çš„ GPU è¿ç®—æ¨¡å‹

## Pipelining

æµæ°´çº¿å¹¶è¡Œ

1. GEMMçš„å—çŠ¶è®¡ç®—éœ€è¦æ¯ä¸ªçº¿ç¨‹ç»´æŠ¤å¤šä¸ªç´¯åŠ å™¨ï¼Œè¿™éƒ¨åˆ†å æ®è‡³å°‘ä¸€åŠå¯„å­˜å™¨

   æŒ‰ç…§ cutlass çš„å®ä¾‹ï¼Œæ˜¯ä¸æ˜¯æ¯ä¸€ä¸ªçº¿ç¨‹è‡³å°‘éœ€è¦ç»´æŠ¤ 8x8 ä¸ª register ä½œä¸ºç´¯åŠ å™¨ï¼Ÿå‰©ä¸‹çš„å¯„å­˜å™¨ç”¨äºä»€ä¹ˆï¼Ÿéœ€è¦ç¡®è®¤è¿™ä¸ªå‡è®¾æ˜¯å¦æ˜¯æˆç«‹çš„ã€‚çœ‹æ¥å¦ä¸€åŠå¯„å­˜å™¨ç”¨äºä» shared memory è·å–æ•°æ®

   å¦‚æœä¸€ä¸ªçº¿ç¨‹éœ€è¦å¤§é‡çš„ registerï¼Œé‚£ä¹ˆ SM åˆ©ç”¨ç‡å¾ˆå®¹æ˜“å°±è¢« register æ•°é‡ bound ä½ã€‚å¦‚æœæŒ‰ç…§ cutlass å›¾ä¸­æ‰€ç¤ºï¼Œæˆ‘ä»¬æŒ‰ç…§ 128 ä¸ª register è¿›è¡Œè®¡ç®—

   åœ¨ 3080 ä¸Šä¸€ä¸ª block æœ€å¤šçš„ register æ•°é‡ä¸º 65536ï¼Œå¦‚æœä¸€ä¸ª register ä½¿ç”¨ 128 ä¸ª registerï¼Œé‚£æœ€å¤šä¹Ÿå°±é©»ç•™ 512 ä¸ª threadï¼Œè€Œ 3080   Maximum number of threads per block ä¸º 1024ï¼Œå ç”¨ç‡éå¸¸ä½

   è€Œæ­¤æ—¶å¯ä»¥é€šè¿‡æµæ°´çº¿ï¼Œè®©è®¡ç®—æµç¨‹æŒç»­å‘ç”Ÿï¼šä¸€åŠå¯„å­˜å™¨ç”¨äºç´¯åŠ å™¨ï¼Œè¿™éƒ¨åˆ†æ˜¯ç”¨ä½œè®¡ç®—çš„ registerï¼›è€Œå¦ä¸€åŠå¯„å­˜å™¨å»è·å–æ•°æ®ï¼Œè¿™éƒ¨åˆ†å¯„å­˜å™¨ä¸å‚ä¸è®¡ç®—ã€‚

2. å¦‚ä½•ç†è§£æµæ°´çº¿å»¶æ—¶æ©è—ï¼Ÿ

   è¿™éå¸¸å½¢è±¡ï¼Œå‡è®¾æœ‰ä¸€ä¸ªåœ°æ–¹ç€ç«äº†ï¼Œä½†æ˜¯æ°´æºå¾ˆè¿œï¼Œä¸è¿‡ä¸€ç¾¤äººæ‹¿ç€ç›†å­å½¢æˆäº†ä¸€æ¡æµæ°´çº¿ï¼Œç”±ç¬¬ä¸€ä¸ªäººä»æ°´æºå¤„å¼€å§‹æ¥æ°´ï¼Œç„¶åä¸æ–­åœ°ä¼ é€’åˆ°ç€ç«ç‚¹ã€‚æœ‰æ„æ€çš„æ˜¯ï¼Œå½“æ•´ä¸ªæµæ°´çº¿å®Œå…¨å¼€å§‹è¿è½¬æ—¶ï¼Œä»æ°´æºå¤„è¾“å‡ºäº†å¤šå°‘æ°´ï¼Œåœ¨åŒä¸€æ—¶åˆ»å°±ä¼šåœ¨ç€ç«ç‚¹è¾“å‡ºå¤šå°‘æ°´ï¼ˆä¹Ÿå¯ä»¥æŠŠè¿™ä¸ªæµæ°´çº¿æƒ³è±¡æˆä¸€æ¡æ°´ç®¡ï¼‰ã€‚æ­¤æ—¶ä»æ°´æºå¤„è¿è¾“åˆ°ç€ç«ç‚¹çš„æ—¶é—´ä¼¼ä¹æ²¡æœ‰äº†ï¼Œæˆ‘ä»¬ä¹Ÿå°±è¯´è¿™æ®µæ—¶é—´è¢«æ©è—äº†èµ·æ¥ã€‚

3. ä¸ºä»€ä¹ˆåŒç¼“å­˜å°±å¤Ÿäº†ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ä¸‰çº§ç¼“å­˜æˆ–è€…æ›´å¤šçº§ï¼Ÿ

   æˆ‘æœ‰ä¸€ä¸ªæ¯”è¾ƒå½¢è±¡çš„ç†è§£ï¼šå¦‚ä½•ç†è§£æµæ°´çº¿å»¶æ—¶æ©è—ï¼Ÿè¿™éå¸¸å½¢è±¡ï¼Œå‡è®¾æœ‰ä¸€ä¸ªåœ°æ–¹ç€ç«äº†ï¼Œä½†æ˜¯æ°´æºå¾ˆè¿œï¼Œä¸è¿‡ä¸€ç¾¤äººæ‹¿ç€ç›†å­å½¢æˆäº†ä¸€æ¡æµæ°´çº¿ï¼Œç”±ç¬¬ä¸€ä¸ªäººä»æ°´æºå¤„å¼€å§‹æ¥æ°´ï¼Œç„¶åä¸æ–­åœ°ä¼ é€’åˆ°ç€ç«ç‚¹ã€‚æœ‰æ„æ€çš„æ˜¯ï¼Œå½“æ•´ä¸ªæµæ°´çº¿å®Œå…¨å¼€å§‹è¿è½¬æ—¶ï¼Œä»æ°´æºå¤„è¾“å‡ºäº†å¤šå°‘æ°´ï¼Œåœ¨åŒä¸€æ—¶åˆ»å°±ä¼šåœ¨ç€ç«ç‚¹è¾“å‡ºå¤šå°‘æ°´ï¼ˆä¹Ÿå¯ä»¥æŠŠè¿™ä¸ªæµæ°´çº¿æƒ³è±¡æˆä¸€æ¡æ°´ç®¡ï¼‰ã€‚æ­¤æ—¶ä»æ°´æºå¤„è¿è¾“åˆ°ç€ç«ç‚¹çš„æ—¶é—´ä¼¼ä¹æ²¡æœ‰äº†ï¼Œæˆ‘ä»¬ä¹Ÿå°±è¯´è¿™æ®µæ—¶é—´è¢«æ©è—äº†èµ·æ¥ã€‚åœ¨è¿™ä¸ªæ¨¡å‹ä¹‹ä¸‹ï¼Œè¦éšè—è¿è¾“æ°´çš„æ¡ä»¶æœ‰ä¸¤ä¸ªï¼š1. æ°´è¶³å¤Ÿå¤šï¼›2. è¦æœ‰è¶³å¤Ÿå¤šçš„äººå’Œç›†å­ã€‚å†ç±»æ¯”åˆ° GPU å½“ä¸­ï¼Œè¦æ©è—æ—¶é—´çš„æ¡ä»¶ä¹Ÿæ˜¯ä¸¤ä¸ªï¼š1. è¶³å¤Ÿå¤šçš„æ•°æ®é‡ï¼›2. è¶³å¤Ÿå¤šçš„å­˜å‚¨ï¼ˆä¸æ˜¯å¾ˆç¡®å®šç¬¬äºŒæ¡ï¼‰ï¼Ÿæ˜¾ç„¶å¦‚æœåœ¨ shared memory å’Œ register å¾ˆå°‘çš„æ—¶å€™ï¼Œä¸è¶³ä»¥æ”¯æ’‘æµæ°´çº¿çš„å»ºç«‹ï¼Œå¦‚ä½•è®¡ç®—è¿™ä¸ªæœ€å° shared memory æˆ–è€… register å‘¢ï¼Ÿ
   
   æˆ‘åº”è¯¥æ˜¯æŠŠå¤šçº§ç¼“å­˜å’Œå¤šçº§æµæ°´çº¿ææ··äº†ï¼åŒç¼“å­˜å°±èƒ½å¤Ÿæ»¡è¶³æµæ°´çº¿ä¸å—é˜»ï¼Œåªæ˜¯éœ€è¦è°ƒæ•´æ¯ä¸€æ¬¡ç”³è¯·æ•°æ®é‡çš„å¤§å°ï¼Œå¦‚æœä¸€æ¬¡ç”³è¯·çš„æ•°æ®é‡å¤ªå¤§ï¼Œç¼“å­˜å°±å¤±å»æ„ä¹‰ï¼Œæ‰€ä»¥åˆ‡åˆ† tile æ˜¯ä¸€ä¸ªé‡è¦å‚æ•°
   
   æˆ‘é‡æ–°æ„å»ºäº†ä¸€ä¸ªå½¢è±¡æ¨¡å‹ï¼ŒæŠŠ global memory æ¯”ä½œä¸€ä¸ªå¤§ houseï¼Œè€Œ shared memory å’Œ register æ˜¯å° houseï¼Œä»£è¡¨äº†ä»–ä»¬èƒ½å­˜å‚¨æ•°æ®é‡çš„å¤šå°‘ã€‚æ•°æ®ä» Global Memory å‘å‡ºï¼Œæ”¾åˆ°äº† transfer highway ä¸Šï¼Œæ•°æ®åœ¨è¿™æ¡é«˜é€Ÿè·¯ä¸Šä¼šä»¥æ’å®šçš„é€Ÿåº¦å‘å‰ç§»åŠ¨ï¼ˆå…‰é€Ÿï¼‰ï¼Œæ‰€ä»¥æˆ‘æŠŠè¿™ä¸ª transfer highway çœ‹åšä¸€ä¸ªæ’å®šè½¬é€Ÿçš„å±¥å¸¦ï¼ŒæŠŠæ•°æ®æ”¾ä¸Šå»æ•°æ®å°±å¼€å§‹è¿è¾“ï¼Œä¸­é€”æ— æ³•è‡ªå·±åœæ­¢ï¼Œé™¤éè¿è¾“åˆ°äº†å­˜å‚¨ç»“æ„å½“ä¸­ã€‚å¦å¤–æˆ‘æŠŠ bandwidth æ ‡è®°ä¸ºäº†è¿™ä¸ª highway çš„å®½åº¦ï¼Œè¿™ä»£è¡¨äº†è¿™æ¡ highway å•ä½æ—¶é—´èƒ½å¤Ÿè¿è¾“çš„æœ€å¤§æ•°æ®é‡ï¼Œè¿™ä¸ªå®½åº¦è¶Šå¤§ï¼Œä»£è¡¨å¸¦å®½è¶Šå¤§ã€‚æˆ‘æ·»åŠ äº†ä¸€ä¸ª ish åœ¨å…¶åé¢ï¼Œæ˜¯å› ä¸º Bandwidth æ˜¯ Bytes per secondsï¼Œè¿˜éœ€è¦è€ƒè™‘å•ä½æ—¶é—´ï¼Œä»…æ ‡è®°ä¸ºå®½åº¦ä¸å¤ªä¸¥è°¨ã€‚è®¡ç®—æ ¸å¿ƒ compute cores åªä¼šä» register ä¸­è·å–æ•°æ®ï¼Œå¹¶å¿«é€Ÿåœ°è¿›è¡Œæ¶ˆè€—ï¼Œå½“è®¡ç®—æ•°æ®æ˜¯ä» global memory å‘å‡ºæ—¶ï¼Œè·ç¦»éå¸¸è¿œï¼Œæœ‰å¾ˆé•¿çš„å»¶æ—¶ latencyï¼Œä½†å¦‚æœæ˜¯ä» shared memory å‘å‡ºåˆ™æ›´ä¸ºæ–¹ä¾¿ï¼Œè€Œä¸€ä¸ªæœ€ä¼˜æƒ…å†µæ˜¯ï¼Œglobal memory ä¸æ–­åœ°ä»¥æœ€å¤§å¸¦å®½å‘é€æ•°æ®ï¼Œç›´æ¥ä¼ è¾“åˆ° compute cores è¿›è¡Œæ¶ˆè€—ï¼Œè¿™æ ·å°±èƒ½å¤Ÿæ©è—æ‰ä¸­é—´çš„è¿è¾“æ—¶é—´ï¼Œä¸è¿‡è¿™ç§æƒ…å†µå¾ˆéš¾å‘ç”Ÿï¼Œå¦‚æœä¸€æ—¦æ•°æ®è¿è¾“åˆ° compute core ä½†æ²¡æœ‰è¢«ä½¿ç”¨ï¼Œé‚£ä¹ˆæ•°æ®å°±ä¼šè¢«æµªè´¹æ‰ï¼Œå› ä¸º compute cores æ— æ³•å¯¹æ•°æ®è¿›è¡Œå­˜å‚¨ï¼Œæ­¤æ—¶å°±å¾—é‡æ–°è®© global memory å‘é€æ•°æ®ï¼Œæ‰€ä»¥éœ€è¦ shared memory å’Œ register è¿›è¡Œæ•°æ®å­˜æ”¾ï¼Œä¸ compute cores è¿›è¡Œé…åˆï¼Œä¿è¯è®¡ç®—æ‰€éœ€æ•°æ®æŒç»­è¢«æ»¡è¶³ï¼Œå¹¶å°½å¯èƒ½ä¿æŒè¿™æ¡é«˜é€ŸæŒç»­åœ¨è¿è¾“æ•°æ®ã€‚è‡³æ­¤å°±èƒ½æ¯”è¾ƒå®Œæ•´ä¸”å½¢è±¡åœ°æè¿°æ•´ä¸ª GPU å­˜å‚¨/è®¡ç®—æ¨¡å‹
   
   ![image-20250226104844732](CUDA Programming 7/image-20250226104844732.png)



Bandwidth & Flops & Roofline æ¨¡å‹

[zhihu](https://zhuanlan.zhihu.com/p/34204282)

æ‰€è°“â€œRoof-lineâ€ï¼ŒæŒ‡çš„å°±æ˜¯ç”±è®¡ç®—å¹³å°çš„ç®—åŠ›å’Œå¸¦å®½ä¸Šé™è¿™ä¸¤ä¸ªå‚æ•°æ‰€å†³å®šçš„â€œå±‹é¡¶â€å½¢æ€ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

- **ç®—åŠ›**å†³å®šâ€œå±‹é¡¶â€çš„é«˜åº¦ï¼ˆç»¿è‰²çº¿æ®µï¼‰
- **å¸¦å®½**å†³å®šâ€œæˆ¿æªâ€çš„æ–œç‡ï¼ˆçº¢è‰²çº¿æ®µï¼‰

<img src="CUDA Programming 7/v2-cafb93b9a31fca2d7c84951555762e59_1440w.jpg" alt="img" style="zoom:80%;" />

1. **æ¨ªåæ ‡**ï¼š**ç®—æœ¯å¼ºåº¦ï¼ˆArithmetic Intensityï¼‰**

   - å®šä¹‰ï¼šå•ä½å­—èŠ‚æ•°æ®èƒ½å®Œæˆçš„è®¡ç®—é‡ï¼ˆFLOPs/Byteï¼‰ï¼Œå³ï¼š
     $$
     \frac{\text{total FLOPs}}{\text{total Bytes}}
     $$

2. **çºµåæ ‡**ï¼š**å¯è¾¾åˆ°çš„æ€§èƒ½ï¼ˆFLOPSï¼‰**

   - è¡¨ç¤ºç¡¬ä»¶åœ¨ç‰¹å®šç®—æœ¯å¼ºåº¦ä¸‹èƒ½è¾¾åˆ°çš„æœ€å¤§è®¡ç®—æ€§èƒ½ï¼ˆå•ä½å¦‚ TFLOPsï¼‰

- çŸ©é˜µä¹˜æ³•ï¼ˆé«˜ç®—æœ¯å¼ºåº¦ï¼‰ï¼šæ¯åŠ è½½ä¸€ä¸ªå…ƒç´ ï¼Œå‚ä¸å¤šæ¬¡è®¡ç®—ã€‚
- å‘é‡åŠ æ³•ï¼ˆä½ç®—æœ¯å¼ºåº¦ï¼‰ï¼šæ¯åŠ è½½ä¸€ä¸ªå…ƒç´ ï¼Œä»…å‚ä¸ä¸€æ¬¡è®¡ç®—ã€‚

1. **å¸¦å®½å—é™åŒºï¼ˆMemory-Boundï¼‰**
   - **æ¡ä»¶**ï¼šç®—æœ¯å¼ºåº¦ < ç¡¬ä»¶å¹³è¡¡ç‚¹ã€‚
   - **æ€§èƒ½å…¬å¼**ï¼šæ€§èƒ½=ç®—æœ¯å¼ºåº¦Ã—æ˜¾å­˜å¸¦å®½æ€§èƒ½=ç®—æœ¯å¼ºåº¦Ã—æ˜¾å­˜å¸¦å®½ã€‚
   - **ä¼˜åŒ–æ–¹å‘**ï¼šå‡å°‘æ•°æ®è®¿é—®é‡ï¼ˆå¦‚æ•°æ®å¤ç”¨ã€åˆ†å—è®¡ç®—ã€ä½¿ç”¨å…±äº«å†…å­˜ï¼‰ã€‚
2. **è®¡ç®—å—é™åŒºï¼ˆCompute-Boundï¼‰**
   - **æ¡ä»¶**ï¼šç®—æœ¯å¼ºåº¦ > ç¡¬ä»¶å¹³è¡¡ç‚¹ã€‚
   - **æ€§èƒ½å…¬å¼**ï¼šæ€§èƒ½=å³°å€¼ç®—åŠ›æ€§èƒ½=å³°å€¼ç®—åŠ›ã€‚
   - **ä¼˜åŒ–æ–¹å‘**ï¼šæé«˜è®¡ç®—æ•ˆç‡ï¼ˆå¦‚ä½¿ç”¨Tensor Coreã€å‡å°‘çº¿ç¨‹åŒæ­¥å¼€é”€ï¼‰ã€‚

### Exercise: is attention memory bound or compute bound

TODO: take A100 as example

## Swizzle

æœ‰ä¸¤ä¸ªä¸åŒçš„ swizzle

### cute swizzle

[reed swizzle](https://zhuanlan.zhihu.com/p/671419093) [killua swizzle](https://zhuanlan.zhihu.com/p/684250988)

1. ä¸ºä»€ä¹ˆè¯»ä¸€æ¬¡ bank å¿…é¡»è¦è¯» 16 byte

   ä¸€ä¸ª phase = 8 bank * 16 byte = 128 byteï¼Œè¿™ä¼¼ä¹å°±æ˜¯ä¸€æ¬¡å†…å­˜è®¿é—®äº‹åŠ¡çš„ç²’åº¦ï¼Œéœ€è¦ç¡®è®¤

ä¸ºäº†è®©è¯»å†™çš„æ‰€æœ‰æ•°æ®éƒ½åˆ†å¸ƒåœ¨ä¸åŒçš„ bank å½“ä¸­

### cutlass threadblock swizzle

å‰è€…æ›´ä¸ºå¤æ‚ï¼Œåè€…æ›´ä¸ºç›´è§‚

å±€éƒ¨æ€§ï¼šæˆ‘ä»¬å¸Œæœ›**ç›¸é‚»çº¿ç¨‹å—å¤„ç†çš„çŸ©é˜µå­å—åœ¨å…¨å±€å†…å­˜ä¸­ç‰©ç†ç›¸é‚»**ï¼Œè¿™æ ·å°±èƒ½æé«˜ L2 ç¼“å­˜çš„å‘½ä¸­ç‡

<img src="CUDA Programming 7/v2-98fbbda7966f798a1fed54be30a79477_1440w.jpg" alt="img" style="zoom: 50%;" />

- **åŸå§‹é¡ºåº**ï¼šçº¿ç¨‹å—æŒ‰è¡Œä¼˜å…ˆé¡ºåºæ‰§è¡Œ `(0,0) â†’ (0,1) â†’ (0,2)...`
- **Swizzleåé¡ºåº**ï¼šçº¿ç¨‹å—æŒ‰Zå­—å‹é¡ºåºæ‰§è¡Œ `(0,0) â†’ (1,0) â†’ (0,1) â†’ (1,1)...`

è¿™ç§è°ƒæ•´è®©ç›¸é‚»çº¿ç¨‹å—è®¿é—®çš„Aå’ŒBå­å—åœ¨å…¨å±€å†…å­˜ä¸­æ›´æ¥è¿‘ï¼Œä»è€Œæé«˜L2ç¼“å­˜å‘½ä¸­ç‡ã€‚æ›´å…·ä½“æ¥è¯´ï¼Œå½“æˆ‘ä»¬åœ¨æ‰§è¡Œ `(1,1)` å—æ—¶ï¼Œå…¶ä¸­æ‰€éœ€è¦çš„æ•°æ®ï¼Œå…¶å®è¢«å‰é¢çš„çº¿ç¨‹å—å·²ç»ä½¿ç”¨è¿‡ï¼Œæ‰€ä»¥æ•°æ®å¯èƒ½éƒ½è¿˜åœ¨ç¼“å­˜ä¸­ï¼Œä»è€Œå‘½ä¸­ã€‚è€ŒæŒ‰ç…§åŸå§‹é¡ºåºï¼Œåœ¨æ‰§è¡Œ `(0,3)` çš„æ—¶å€™ï¼Œå…¶æ‰€éœ€çš„ B çŸ©é˜µæ•°æ®ä¸€å®šæ˜¯æ²¡åŠæ³•åœ¨ç¼“å­˜ä¸­æ‰¾åˆ°çš„ï¼Œå› ä¸ºä¹‹å‰éƒ½æ²¡æœ‰ä½¿ç”¨åˆ°

## SplitK & SlicedK

[reed's blog](https://zhuanlan.zhihu.com/p/667521327) å¯¹æ­¤æœ‰ä¸€äº›ä»‹ç»ï¼Œä¸€èˆ¬ç”¨ sliced-kï¼Œsplit-k åªåœ¨ç‰¹æ®Šåœºæ™¯æœ‰æ•ˆ (å° m & nï¼Œå¤§ k)

# CUTLASS in Practice

- improve cutlass gemm  [zhihu](https://zhuanlan.zhihu.com/p/707715989) [Reed's zhihu posts](https://www.zhihu.com/people/reed-84-49/posts)
- pick up cutlass examples: interested in all kinds of gemm and kernel fusion
- [CUTLASS CuTeå®æˆ˜(äºŒ)-åº”ç”¨](https://zhuanlan.zhihu.com/p/692078624) [github](https://github.com/zeroine/cutlass-cute-sample) [zhihu](https://zhuanlan.zhihu.com/p/690703999)this gives examples on optimze gemm and fusing kernel, and most importantly, it gives examples on how to use ncu & nsys to analyize the performance
- cutlass in flash attention
- understand cutlass scale mm in vllm
- sage attention implementation (not much cutlass involved, but have a lot to do with flashinfer and vllm)

## Learning Stages

Stage1:

learn gpu model with cutlass gemm (improved gemm kernel with cute), **here is the next next focus**

learn important layer implementation: **quantization**, **flash attention 2**, layer norm

compare your implementation with sota project integration (vllm, sage attention) focusing on quantization gemm

Stage2:

dive into sota projects

learn improved gpu hardware (Hopper) features ï¼ˆLaterï¼‰

we are going to explore all the tricks that these inference engine used (vllm, sglang, flashinfer)

## Questions

- How to Debug? I often see colored squares in screen

- cute and cutlass, which should we choose?

- How to fuse kernels?

- when using `cmake .. -DCUTLASS_NVCC_ARCHS=86` does this equal to `cmake .. -DCUTLASS_NVCC_ARCHS=80`

- CUTLASS assert functions

- dynamic & static differenceâœ…

  dynamic is the value you can get at runtime, the static is the value that can be determined at compile time.

- what is nested tensor mode

- sgemm_2 is 3ms faster than sgemm_1 (15ms v.s. 18 ms), still 5ms to go from 10ms (cuBLAS), how to exceed it? [zhihu](https://zhuanlan.zhihu.com/p/707715989)

- what is mode and major

  seems like mode refers to axis...

- Layout compatibility is not well explained: Why Shape 24 is compatible with Shape (24), but Shape (24) is **NOT** compatible with Shape 24.

- There are 2 async functions that I don't quite understand

  ```c++
  cp_async_fence();        // Label the end of (potential) cp.async instructions
  cp_async_wait<0>();      // Sync on all (potential) cp.async instructions
  __syncthreads();         // Wait for all threads to write to smem
  ```
