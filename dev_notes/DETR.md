# DETR

[zhihu](https://zhuanlan.zhihu.com/p/348060767)	[bilibili](https://www.bilibili.com/video/BV1GB4y1X72R)

DETR çš„ç‰¹ç‚¹ï¼šç®€å•ï¼ç«¯åˆ°ç«¯ï¼no anchorï¼åœ¨æ€§èƒ½ï¼ˆè¡¨ç°/é€Ÿåº¦ï¼‰ä¸Šå’Œ Faster RCNN ç›¸ä¼¼ã€‚è™½ç„¶å’Œå½“æ—¶æœ€å¥½çš„æ–¹æ³•ç›¸å·®10ä¸ªç‚¹ï¼Œä½†æ˜¯è¿™ä¸ªæ¡†æ¶å¤ªå¥½äº†ï¼Œæ˜¯ä¸€ä¸ªæŒ–å‘æ€§è´¨çš„æ–‡ç« ï¼Œæ‰€ä»¥è¿™ä¹Ÿæ˜¯ä»¶å¥½äº‹å„¿

## Intro

ç°é˜¶æ®µç›®æ ‡æ£€æµ‹å™¨å¾ˆå¤§ç¨‹åº¦ä¸Šéƒ½å—é™äºåå¤„ç† **NMS** çš„æ–¹æ³•ï¼Œä¸ç®¡æ˜¯ anchor-based or anchor-freeï¼ŒRCNN or SSD or YOLOï¼Œéƒ½æœ‰è¿™ä¸ªé—®é¢˜ï¼Œè¿™ä¹Ÿè®©ç›®æ ‡æ£€æµ‹åœ¨ç›®å‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•é‡Œéƒ½æ˜¯æ¯”è¾ƒå¤æ‚çš„ï¼Œåœ¨ä¼˜åŒ–å’Œè°ƒå‚çš„éš¾åº¦ä¸Šéƒ½æ¯”è¾ƒå¤§

DETR çš„ç½‘ç»œæµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤º

<img src="DETR/image-20220919001758670.png" alt="image-20220919001758670" style="zoom:67%;" />

ç”¨è¯­è¨€æ€»ç»“å¦‚ä¸‹ï¼š

1. ä½¿ç”¨ CNN æŠ½å–å›¾åƒç‰¹å¾
2. ä½¿ç”¨ Transformer encoder è·å¾—å…¨å±€ç‰¹å¾
3. ä½¿ç”¨ Transformer decoder è·å¾—é¢„æµ‹æ¡†
4. å°†é¢„æµ‹æ¡†å’Œ ground truth boxes åšåŒ¹é…å¹¶è®¡ç®— loss

Transformer å¤§è‡´ç»“æ„å¦‚ä¸‹

<img src="C:\Data\Projects\notes\dev_notes\DETR\image-20221215140623383.png" alt="image-20221215140623383" style="zoom:50%;" />

å›¾ä¸­ Decoder è¿™è¾¹çš„ç»“æ„æ˜¯ä¸å¤Ÿå®Œæ•´çš„ï¼Œæˆ–è€…è¯´ä¸å¤Ÿæ­£ç¡®çš„ã€‚å®é™…ä¸Š Decoder å…³äº query çš„è¾“å…¥æœ‰ä¸¤ä¸ªï¼š1

1. Query itself
2. Query positional encoding, **i.e. Object Query**

å›¾ä¸­æ²¡æœ‰æŠŠ Query æœ¬èº«ç»™ç”»å‡ºæ¥ï¼Œè®ºæ–‡é‡Œæ˜¯**åˆå§‹åŒ–ä¸º0**ï¼Œæ‰€ä»¥å›¾ä¸­ç›´æ¥çœç•¥ä¸ç”»äº†ï¼Œå¯¼è‡´ Decoder ä¸‹ä¾§çš„ `+` å·æ„ä¹‰ä¸æ¸…æ™°

### ä¸€äº›ç»“è®º

å—ç›Šäº transformer çš„å…¨å±€å»ºæ¨¡èƒ½åŠ›ï¼ŒDETR å¯¹äºå¤§ç‰©ä½“çš„æ£€æµ‹èƒ½åŠ›éå¸¸å¼ºï¼Œä½†æ˜¯å¯¹å°ç‰©ä½“çš„æ¯”è¾ƒå·®ï¼Œå¹¶ä¸” DETR æ”¶æ•›çš„é€Ÿåº¦éå¸¸æ…¢ã€‚æ”¹è¿›æ–¹æ³•åœ¨ Deformable DETR ä¸­æå‡ºï¼Œä¾ç„¶æ˜¯ä½¿ç”¨å¤šå°ºåº¦çš„ç‰¹å¾å›¾è°± + Deformable attention

å‰äººä¹Ÿæœ‰ä½¿ç”¨äºŒåˆ†å›¾åŒ¹é…çš„æ–¹æ³•ï¼Œæˆ–è€…ä½¿ç”¨ RNN åš encoder-decoder æ¥è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œä½†æ˜¯éƒ½æ²¡æœ‰ç”¨ transformer æ‰€ä»¥æ€§èƒ½ä¸Šä¸å»ã€‚æ‰€ä»¥è¯´ DETR çš„æˆåŠŸï¼Œä¹Ÿæ˜¯ transformer çš„æˆåŠŸ

##  Model

### Bipartite Matching Loss

è®ºæ–‡è®¤ä¸ºç»“æ„éƒ½æ˜¯æ¯”è¾ƒç®€å•å¥½ç†è§£çš„ï¼Œæ‰€ä»¥å…ˆè®²äº†æŸå¤±å‡½æ•°è¿™ä¸€å—ï¼šå¦‚ä½•ä½¿ç”¨äºŒåˆ†å›¾åŒ¹é…æ¥è®¡ç®—æŸå¤±

DETR é¢„æµ‹è¾“å‡ºæ˜¯ä¸€ä¸ªå›ºå®šå€¼ï¼Œå³é¢„æµ‹å›ºå®šçš„ N(=100) ä¸ªé¢„æµ‹

å…³äºäºŒåˆ†å›¾åŒ¹é…ç®—æ³•ï¼ˆåŒˆç‰™åˆ©ç®—æ³•ï¼‰ï¼Œæˆ‘åœ¨ä¹‹å‰çš„åšå®¢ **å›¾è®ºç®—æ³•** é‡Œæœ‰ä¸€äº›æ€»ç»“å¯ä»¥å‚è€ƒï¼Œåœ¨ DETR çš„åœºæ™¯ä¸‹è¯¥åŒ¹é…ç®—æ³•çš„ä½œç”¨ä¸ºï¼šå°† N ä¸ª prediction ä¸ N ä¸ª gt è¿›è¡Œé…å¯¹ï¼ˆæ²¡æœ‰ N ä¸ª gt åˆ™éœ€è¦ paddingï¼‰ã€‚é¢„æµ‹æœ‰äº† gt è¿‡åå°±å¯ä»¥è®¡ç®—æŸå¤±å‡½æ•°äº†

é…å¯¹ä½¿ç”¨çš„ cost matrix è®¡ç®—å…¬å¼å¦‚ä¸‹
$$
\hat{\sigma}=\underset{\sigma \in \mathfrak{S}_{N}}{\arg \min } \sum_{i}^{N} \mathcal{L}_{\operatorname{match}}\left(y_{i}, \hat{y}_{\sigma(i)}\right) \\
\mathcal{L}_{\operatorname{match}}\left(y_{i}, \hat{y}_{\sigma(i)}\right)=
-\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \hat{p}_{\sigma(i)}\left(c_{i}\right)+\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \mathcal{L}_{\mathrm{box}}\left(b_{i}, \hat{b}_{\sigma(i)}\right)\\

\mathcal{L}_{\mathrm{box}}=\lambda_{\text {iou }} \mathcal{L}_{\text {iou }}\left(b_{i}, \hat{b}_{\sigma(i)}\right)+\lambda_{\mathrm{L} 1}\left\|b_{i}-\hat{b}_{\sigma(i)}\right\|_{1}
$$
å…¶ä¸­ $\sigma$ å¯ä»¥çœ‹ä½œä¸€ä¸ªæ’åˆ—æˆ–è€…æ˜ å°„ï¼Œ$\sigma(i)$ ä»£è¡¨ç¬¬ i ä¸ª gt æ‰€åŒ¹é…çš„é¢„æµ‹çš„ indexï¼Œbox æŸå¤±ä½¿ç”¨çš„æ˜¯ GIoU æŸå¤±å’Œ L1 æŸå¤±çš„åŠ æƒã€‚æ³¨æ„åˆ°ç©º gt å’Œä»»ä½• prediction çš„ cost éƒ½æ˜¯ 0ï¼Œæ‰€ä»¥æœ¬è´¨ä¸Šå°±æ˜¯ N ä¸ª prediction å’Œ M ä¸ª gt ä¹‹é—´çš„åŒ¹é…ï¼Œç”¨äºç¡®å®š M ä¸ªæ­£æ ·æœ¬ prediction å’Œ N - M ä¸ªè´Ÿæ ·æœ¬ prediction

### Detection Loss

åŒ¹é…å®Œæˆåï¼Œå°±å¯ä»¥è®¡ç®—æŸå¤±å‡½æ•°
$$
\mathcal{L}_{\text {Hungarian }}(y, \hat{y})=\sum_{i=1}^{N}\left[-\log \hat{p}_{\hat{\sigma}(i)}\left(c_{i}\right)+\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \mathcal{L}_{\text {box }}\left(b_{i}, \hat{b}_{\hat{\sigma}}(i)\right)\right]
$$
è®ºæ–‡æåˆ°åœ¨è®¡ç®—åˆ†ç±»æŸå¤±æ—¶ï¼Œå¯¹äºç©ºç±» gt $\varnothing$ çš„åˆ†ç±»æŸå¤±è¦é™¤ä»¥ 10 ç”¨äºå¹³è¡¡æ­£è´Ÿæ ·æœ¬ï¼Œåœ¨å®ç°ä¸Šæ˜¯ç›´æ¥æŒ‡å®š `F.cross_entropy(..., weight=)` å®Œæˆ

å¦å¤–è¿˜æ˜¯ä½¿ç”¨äº†ä¸­é—´ç›‘ç£ï¼Œæˆ–è€…ç§°ä¸ºè¾…åŠ©æŸå¤±ã€‚å³æŠŠ decoder çš„ä¸­é—´ block çš„è¾“å‡ºä¹Ÿä½œä¸ºé¢„æµ‹ç»“æœï¼Œå¹¶è®¡ç®—æ£€æµ‹æŸå¤±

swin v2

DC5ï¼Œæ˜¯ dilated convolution at resnet stage 5 çš„ä¸€ä¸ªç®€ç§°ï¼Œåœ¨ DETR ä¸­ä½¿ç”¨çš„å…·ä½“æè¿°ä¸º

> Following [21, FCN], we also increase the feature resolution by adding a dilation to the last stage of the backbone and removing a stride from the first convolution of this stage.

ä½†æ˜¯åœ¨åé¢çš„ DETR-based ç›®æ ‡æ£€æµ‹å™¨ä¸­æ²¡æœ‰å¸¸ç”¨

### DETR é—®é¢˜

å¤§ç‰©ä½“å’Œå°ç‰©ä½“æ•ˆæœç›¸å·®å¤§

object query çš„é¢„æµ‹ç»“æœå¯è§†åŒ–

## Deformable DETR & Deformable Attention

### Multi-Scaele Deformable Attention

ä¸€å¼€å§‹çœ‹ Deformable DETR çš„æ—¶å€™ï¼Œæ³¨æ„åŠ›å¾ˆå®¹æ˜“é›†ä¸­åˆ° Deformable ä¸Šï¼Œä½†æˆ‘è§‰å¾— Multi-Scale åŒæ ·éå¸¸é‡è¦ã€‚ä¸ºäº†å½»åº•å¼„æ¸… Multi-Scale Deformable Attentionï¼Œæˆ‘æƒ³ä»å¦‚ä¸‹å‡ ä¸ªé—®é¢˜å…¥æ‰‹

1. å¦‚ä½•è¡¨ç¤º multi-scale feature
2. å¦‚ä½•è¡¨ç¤º multi-scale feature's positional embedding 
3. å¦‚ä½•è¡¨ç¤º reference points
4. å¦‚ä½•å®Œæˆ multi-scale deformable attention

#### Multi-scale Feature

è¿™éå¸¸å¥½ç†è§£ï¼Œå°±æ˜¯ä» backbone **ResNet 50** ä¸­è¾“å‡ºçš„ä¸­é—´ç‰¹å¾å±‚ï¼Œè¶Šæ·±çš„ç‰¹å¾å±‚ï¼Œåˆ†è¾¨ç‡è¶Šå°

```python
# output feature dict
features = self.backbone(images.tensor)

# project backbone features to the reuired dimension of transformer
multi_level_feats = self.neck(features)
```

æœ€ç»ˆ neck æŠŠè¿™äº›ç‰¹å¾å›¾è°±æ˜ å°„åˆ°åŒä¸€ä¸ªç»´åº¦ï¼Œä»¥è¾“å…¥åˆ° transformer ä¸­åšç‚¹ç§¯

<img src="C:\Data\Projects\notes\dev_notes\DETR\image-20221216190202156.png" alt="image-20221216190202156" style="zoom:50%;" />

#### Multi-scale Positional Embeddings

è®ºæ–‡é‡Œè¿˜æåˆ°äº† scale-level embeddingï¼Œä½†åœ¨ two stage ä¸­å¹¶æ²¡æœ‰ä½¿ç”¨ï¼Œæ„Ÿè§‰ç”¨å¤„ä¸å¤§ï¼Œè¿™é‡Œç›´æ¥å¿½ç•¥ã€‚è€Œå¤„ç† multi-scale çš„ positional embeddingsï¼Œä¹Ÿæ²¡æœ‰å¤ªå¤šæ”¹åŠ¨ï¼Œå°±æ˜¯é€ä¸ª level è·å¾—

```python
for feat in multi_level_feats:
    multi_level_masks.append(
        F.interpolate(img_masks[None], size=feat.shape[-2:]).to(torch.bool).squeeze(0)
    )
    multi_level_position_embeddings.append(self.position_embedding(multi_level_masks[-1]))
```

è§£é‡Šä¸€ä¸‹ï¼špositional embedding æ˜¯æ ¹æ®ä¸€ä¸ª 2D mask ç›´æ¥ç”Ÿæˆçš„ï¼Œå¯¹äºä¸åŒ scale çš„ mask æ˜¯ç›´æ¥æ ¹æ® img mask æ’å€¼è·å¾—ï¼ˆimg mask ä¸­çš„éé›¶å€¼å³è¡¨ç¤ºè¯¥åƒç´ ç‚¹è¢«å¿½ç•¥ï¼‰ 

#### Reference Points

reference points å°±æ˜¯æ¯ä¸ªåƒç´ ç‚¹ä¸­å¿ƒçš„**å½’ä¸€åŒ–åæ ‡**ã€‚æ¯ä¸€ä¸ª scale çš„ reference points ä¸ºä¸€ä¸ªå¼ é‡ï¼Œå½¢çŠ¶ä¸º (H, W, 2)ï¼Œé‚£ä¹ˆå¤šä¸ª scale çš„ reference points åˆèµ·æ¥åº”è¯¥æ˜¯ `(B, h1w1 + h2w2 + ..., 2)` æ‰å¯¹ï¼Œä½†å®é™…ä¸Šçš„ä»£ç å¹¶ä¸æ˜¯è¿™ä¹ˆåšçš„

```python
    def get_reference_points(spatial_shapes, valid_ratios, device):
        """
        Args:
            spatial_shapes (Tensor): The shape of all feature maps, has shape (num_level, 2).
            valid_ratios (Tensor): The ratios of valid points on the feature map, has shape (bs, num_levels, 2)
        Returns:
            Tensor: reference points used in decoder, has shape (bs, num_keys, num_levels, 2).
        """
        reference_points_list = []
        for lvl, (H, W) in enumerate(spatial_shapes):
            ref_y, ref_x = torch.meshgrid(
                torch.linspace(0.5, H - 0.5, H, dtype=torch.float32, device=device),
                torch.linspace(0.5, W - 0.5, W, dtype=torch.float32, device=device),
            )
            ref_y = ref_y.reshape(-1)[None] / (valid_ratios[:, None, lvl, 1] * H)   # (1, HW) / (B, 1) -> (B, HW)
            ref_x = ref_x.reshape(-1)[None] / (valid_ratios[:, None, lvl, 0] * W)
            ref = torch.stack((ref_x, ref_y), -1)   # (B, HW, 2)
            reference_points_list.append(ref)
        reference_points = torch.cat(reference_points_list, 1)  # (B, N1+N2+..., 2)
        reference_points = reference_points[:, :, None] * valid_ratios[:, None]
        return reference_points
```

å¯ä»¥çœ‹åˆ°æœ€ç»ˆçš„è¾“å‡ºå½¢çŠ¶æ˜¯ `(bs, num_keys, num_levels, 2)`ï¼Œ**å®é™…ä¸Šè¿™æ˜¯ä¸ºäº†å„ä¸ª scale ä¹‹é—´çš„äº¤äº’**ï¼Œå³æŸä¸ª scale çš„ reference point å¯ä»¥å»å¦ä¸€ä¸ª scale è¿›è¡Œé‡‡æ · 

#### MSDeformAttention

ä¸‹é¢æ­£å¼ä»‹ç» multi-scale deformable attention æ¨¡å—ï¼Œå…ˆç®€å•æè¿°ä¸‹ä»£ç å¹²äº†ä»€ä¹ˆäº‹æƒ…

1. åˆ¤æ–­æ˜¯å¦ä¸ºè‡ªæ³¨æ„åŠ›ï¼Œå¦‚æœæ²¡æœ‰ä¼ å…¥ value åˆ™ä½¿ç”¨ query æœ¬èº«

2. ç»™ query åŠ å…¥ positional embeddingï¼Œ**æ³¨æ„ï¼Œè¿™é‡Œæ²¡æœ‰ç»™ key åŠ å…¥ positional embeddingï¼Œæ›´ç¡®åˆ‡åœ°è¯´ï¼Œåœ¨ deformable attetion é‡Œæ²¡æœ‰ key çš„æ¦‚å¿µï¼Œkey æå…¶ attention æ˜¯é€šè¿‡å…¶ä»–æ–¹å¼è·å¾—**

3. value ç»è¿‡ä¸€ä¸ªçº¿æ€§å±‚ï¼Œç»´åº¦ä¸å˜ï¼Œå¹¶ä¸” mask æ‰ä¸éœ€è¦è¿›è¡Œæ³¨æ„åŠ›çš„ç‚¹ã€‚ç„¶åå† view ä¸ºå¤šå¤´çš„å½¢å¼

4. å°† query é€åˆ° `self.sampling_points` çº¿æ€§å±‚ï¼Œè¿›è¡Œåç§»é‡é¢„æµ‹ï¼Œä¹Ÿå¯¹åç§»é‡è¿›è¡Œå½’ä¸€åŒ–

   ```python
   self.sampling_offsets = nn.Linear(embed_dim, num_heads * num_levels * num_points * 2)
   ```

5. å°† query é€åˆ° `self.attention_weight` çº¿æ€§å±‚ï¼Œå¹¶ç”¨ softmax è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°

   ```python
   self.attention_weights = nn.Linear(embed_dim, num_heads * num_levels * num_points)
   ```

6. æœ‰äº†å‰é¢çš„å‡†å¤‡å·¥ä½œï¼Œå°±èƒ½å¤Ÿæ„‰å¿«è®¡ç®—å¯å˜æ³¨æ„åŠ›äº†ï¼Œpure pytorch å®ç°ä¸º `multi_scale_deformable_attn_pytorch`ï¼Œæ³¨æ„è¿™é‡Œçš„è¾“å‡ºå·²ç»åˆå¹¶äº†å¤šå¤´

7. è·å¾—çš„ç»“æœå†è¿‡ä¸€ä¸ªçº¿æ€§å±‚ï¼Œç»´åº¦ä¸å˜

ç”±äºä»£ç è¿‡äº mmlab çš„é£æ ¼ğŸ¤£ï¼Œæˆ‘åˆ é™¤äº†ä¸€äº›ï¼Œä»¥æ–¹ä¾¿ç†è§£

```python
    def forward(
        self,
        query: torch.Tensor,	# (B, N, C)
        key = None,
        value = None,
        identity = None,
        query_pos = None,
        key_padding_mask = None,
        reference_points = None,
        spatial_shapes = None,
        level_start_index = None,
        **kwargs) -> torch.Tensor:
        
        if value is None:	# True, when self-attetion
            value = query
        if identity is None:	# True
            identity = query
        if query_pos is not None:	# True
            query = query + query_pos
        bs, num_query, _ = query.shape
        bs, num_value, _ = value.shape

        value = self.value_proj(value)
        
        if key_padding_mask is not None:	# True
            value = value.masked_fill(key_padding_mask[..., None], float(0))
        value = value.view(bs, num_value, self.num_heads, -1)
        
        sampling_offsets = self.sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2
        )
        # bs, num_query, num_heads, num_levels, num_points, 2
        offset_normalizer = torch.stack([spatial_shapes[..., 1], spatial_shapes[..., 0]], -1)
        sampling_locations = (
            reference_points[:, :, None, :, None, :]
            + sampling_offsets / offset_normalizer[None, None, None, :, None, :]
        )
        
        attention_weights = self.attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        attention_weights = attention_weights.softmax(-1)
        attention_weights = attention_weights.view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points)
        
        output = multi_scale_deformable_attn_pytorch(
                value, spatial_shapes, sampling_locations, attention_weights)
        output = self.output_proj(output)

        return self.dropout(output) + identity
```

pure pytorch ä»£ç å—å¦‚ä¸‹ï¼Œæ¯”è¾ƒéš¾çœ‹çš„æ˜¯å¼ é‡çš„å½¢çŠ¶ï¼Œæˆ‘éƒ½ä»¥æ³¨é‡Šç»™å‡ºï¼Œåº”è¯¥æ¯”è¾ƒå¥½ç†è§£ã€‚ç”¨ç®€æ´çš„è¯­è¨€æ¦‚æ‹¬ä¸ºï¼š

1. å¯¹æ¯ä¸€ä¸ª scale/levelï¼Œè®¡ç®—æ‰€æœ‰ sampling points åœ¨è¯¥ level æ’å€¼å¾—åˆ°çš„ç‰¹å¾å‘é‡
2. å¯¹æ’å€¼å¾—åˆ°çš„ multi scale + multi points çš„ç‰¹å¾å‘é‡è¿›è¡Œæ³¨æ„åŠ›åŠ æƒæ•´åˆ
3. åˆå¹¶å¤šä¸ª head çš„ç‰¹å¾å‘é‡

```python
def multi_scale_deformable_attn_pytorch(
    value: torch.Tensor,
    value_spatial_shapes: torch.Tensor,
    sampling_locations: torch.Tensor,
    attention_weights: torch.Tensor,
) -> torch.Tensor:

    bs, _, num_heads, embed_dims = value.shape
    _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape
    value_list = value.split([H_ * W_ for H_, W_ in value_spatial_shapes], dim=1)
    sampling_grids = 2 * sampling_locations - 1
    sampling_value_list = []
    
    for level, (H_, W_) in enumerate(value_spatial_shapes):
        # bs, H_*W_, num_heads, embed_dims -> bs*num_heads, embed_dims, H_, W_
        value_l_ = (
            value_list[level].flatten(2).transpose(1, 2).reshape(bs * num_heads, embed_dims, H_, W_)
        )
        # bs, num_queries, num_heads, num_points, 2 -> bs*num_heads, num_queries, num_points, 2
        sampling_grid_l_ = sampling_grids[:, :, :, level].transpose(1, 2).flatten(0, 1)
        # bs*num_heads, embed_dims, num_queries, num_points
        sampling_value_l_ = F.grid_sample(
            value_l_, sampling_grid_l_, mode="bilinear", padding_mode="zeros", align_corners=False
        )
        sampling_value_list.append(sampling_value_l_)
    # (bs, num_queries, num_heads, num_levels, num_points) -> (bs*num_heads, 1, num_queries, num_levels*num_points)
    attention_weights = attention_weights.transpose(1, 2).reshape(
        bs * num_heads, 1, num_queries, num_levels * num_points
    )
    output = ( # (bs*num_heads, embed_dims, num_quries, num_levels*num_points)
        (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights) 
        .sum(-1)
        .view(bs, num_heads * embed_dims, num_queries)
    )
    return output.transpose(1, 2).contiguous()
```

### Decoder

çœ‹å®Œäº† Deformable Attentionï¼Œå®é™…ä¸Šåœ¨ Decoder ä¸Šçš„ä¸œè¥¿ä¹Ÿå¾ˆä¸ä¸€æ ·ã€‚ä»æ­¤å¼€å§‹ Decoder ä¸å†ä¿æŒåŸå§‹ DETR çš„ç®€æ´æ€§ï¼Œä¸ºäº†è®©æ”¶æ•›æ›´å¿«ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨æ›´å¼ºçš„å…ˆéªŒå‡è®¾

1. query selection
2. encode new query

query -> region of interest
