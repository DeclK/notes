# DETR

[zhihu](https://zhuanlan.zhihu.com/p/348060767)	[bilibili](https://www.bilibili.com/video/BV1GB4y1X72R)

DETR çš„ç‰¹ç‚¹ï¼šç®€å•ï¼ç«¯åˆ°ç«¯ï¼no anchorï¼åœ¨æ€§èƒ½ï¼ˆè¡¨ç°/é€Ÿåº¦ï¼‰ä¸Šå’Œ Faster RCNN ç›¸ä¼¼ã€‚è™½ç„¶å’Œå½“æ—¶æœ€å¥½çš„æ–¹æ³•ç›¸å·®10ä¸ªç‚¹ï¼Œä½†æ˜¯è¿™ä¸ªæ¡†æ¶å¤ªå¥½äº†ï¼Œæ˜¯ä¸€ä¸ªæŒ–å‘æ€§è´¨çš„æ–‡ç« ï¼Œæ‰€ä»¥è¿™ä¹Ÿæ˜¯ä»¶å¥½äº‹å„¿

## Intro

ç°é˜¶æ®µç›®æ ‡æ£€æµ‹å™¨å¾ˆå¤§ç¨‹åº¦ä¸Šéƒ½å—é™äºåå¤„ç† **NMS** çš„æ–¹æ³•ï¼Œä¸ç®¡æ˜¯ anchor-based or anchor-freeï¼ŒRCNN or SSD or YOLOï¼Œéƒ½æœ‰è¿™ä¸ªé—®é¢˜ï¼Œè¿™ä¹Ÿè®©ç›®æ ‡æ£€æµ‹åœ¨ç›®å‰çš„æ·±åº¦å­¦ä¹ æ–¹æ³•é‡Œéƒ½æ˜¯æ¯”è¾ƒå¤æ‚çš„ï¼Œåœ¨ä¼˜åŒ–å’Œè°ƒå‚çš„éš¾åº¦ä¸Šéƒ½æ¯”è¾ƒå¤§

DETR çš„ç½‘ç»œæµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤º

<img src="DETR/image-20220919001758670.png" alt="image-20220919001758670" style="zoom:67%;" />

ç”¨è¯­è¨€æ€»ç»“å¦‚ä¸‹ï¼š

1. ä½¿ç”¨ CNN æŠ½å–å›¾åƒç‰¹å¾
2. ä½¿ç”¨ Transformer encoder è·å¾—å…¨å±€ç‰¹å¾
3. ä½¿ç”¨ Transformer decoder è·å¾—é¢„æµ‹æ¡†
4. å°†é¢„æµ‹æ¡†å’Œ ground truth boxes åšåŒ¹é…å¹¶è®¡ç®— loss

Transformer å¤§è‡´ç»“æ„å¦‚ä¸‹

<img src="C:\Data\Projects\notes\dev_notes\DETR\image-20221215140623383.png" alt="image-20221215140623383" style="zoom:50%;" />

å›¾ä¸­ Decoder è¿™è¾¹çš„ç»“æ„æ˜¯ä¸å¤Ÿå®Œæ•´çš„ï¼Œæˆ–è€…è¯´ä¸å¤Ÿæ­£ç¡®çš„ã€‚å®é™…ä¸Š Decoder å…³äº query çš„è¾“å…¥æœ‰ä¸¤ä¸ªï¼š1

1. Query itself
2. Query positional encoding, **i.e. Object Query**

å›¾ä¸­æ²¡æœ‰æŠŠ Query æœ¬èº«ç»™ç”»å‡ºæ¥ï¼Œè®ºæ–‡é‡Œæ˜¯**åˆå§‹åŒ–ä¸º0**ï¼Œæ‰€ä»¥å›¾ä¸­ç›´æ¥çœç•¥ä¸ç”»äº†ï¼Œå¯¼è‡´ Decoder ä¸‹ä¾§çš„ `+` å·æ„ä¹‰ä¸æ¸…æ™°

### ä¸€äº›ç»“è®º

å—ç›Šäº transformer çš„å…¨å±€å»ºæ¨¡èƒ½åŠ›ï¼ŒDETR å¯¹äºå¤§ç‰©ä½“çš„æ£€æµ‹èƒ½åŠ›éå¸¸å¼ºï¼Œä½†æ˜¯å¯¹å°ç‰©ä½“çš„æ¯”è¾ƒå·®ï¼Œå¹¶ä¸” DETR æ”¶æ•›çš„é€Ÿåº¦éå¸¸æ…¢ã€‚æ”¹è¿›æ–¹æ³•åœ¨ Deformable DETR ä¸­æå‡ºï¼Œä¾ç„¶æ˜¯ä½¿ç”¨å¤šå°ºåº¦çš„ç‰¹å¾å›¾è°± + Deformable attention

å‰äººä¹Ÿæœ‰ä½¿ç”¨äºŒåˆ†å›¾åŒ¹é…çš„æ–¹æ³•ï¼Œæˆ–è€…ä½¿ç”¨ RNN åš encoder-decoder æ¥è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œä½†æ˜¯éƒ½æ²¡æœ‰ç”¨ transformer æ‰€ä»¥æ€§èƒ½ä¸Šä¸å»ã€‚æ‰€ä»¥è¯´ DETR çš„æˆåŠŸï¼Œä¹Ÿæ˜¯ transformer çš„æˆåŠŸ

##  Model

### Bipartite Matching Loss

è®ºæ–‡è®¤ä¸ºç»“æ„éƒ½æ˜¯æ¯”è¾ƒç®€å•å¥½ç†è§£çš„ï¼Œæ‰€ä»¥å…ˆè®²äº†æŸå¤±å‡½æ•°è¿™ä¸€å—ï¼šå¦‚ä½•ä½¿ç”¨äºŒåˆ†å›¾åŒ¹é…æ¥è®¡ç®—æŸå¤±

DETR é¢„æµ‹è¾“å‡ºæ˜¯ä¸€ä¸ªå›ºå®šå€¼ï¼Œå³é¢„æµ‹å›ºå®šçš„ N(=100) ä¸ªé¢„æµ‹

å…³äºäºŒåˆ†å›¾åŒ¹é…ç®—æ³•ï¼ˆåŒˆç‰™åˆ©ç®—æ³•ï¼‰ï¼Œæˆ‘åœ¨ä¹‹å‰çš„åšå®¢ **å›¾è®ºç®—æ³•** é‡Œæœ‰ä¸€äº›æ€»ç»“å¯ä»¥å‚è€ƒï¼Œåœ¨ DETR çš„åœºæ™¯ä¸‹è¯¥åŒ¹é…ç®—æ³•çš„ä½œç”¨ä¸ºï¼šå°† N ä¸ª prediction ä¸ N ä¸ª gt è¿›è¡Œé…å¯¹ï¼ˆæ²¡æœ‰ N ä¸ª gt åˆ™éœ€è¦ paddingï¼‰ã€‚é¢„æµ‹æœ‰äº† gt è¿‡åå°±å¯ä»¥è®¡ç®—æŸå¤±å‡½æ•°äº†

é…å¯¹ä½¿ç”¨çš„ cost matrix è®¡ç®—å…¬å¼å¦‚ä¸‹
$$
\hat{\sigma}=\underset{\sigma \in \mathfrak{S}_{N}}{\arg \min } \sum_{i}^{N} \mathcal{L}_{\operatorname{match}}\left(y_{i}, \hat{y}_{\sigma(i)}\right) \\
\mathcal{L}_{\operatorname{match}}\left(y_{i}, \hat{y}_{\sigma(i)}\right)=
-\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \hat{p}_{\sigma(i)}\left(c_{i}\right)+\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \mathcal{L}_{\mathrm{box}}\left(b_{i}, \hat{b}_{\sigma(i)}\right)\\

\mathcal{L}_{\mathrm{box}}=\lambda_{\text {iou }} \mathcal{L}_{\text {iou }}\left(b_{i}, \hat{b}_{\sigma(i)}\right)+\lambda_{\mathrm{L} 1}\left\|b_{i}-\hat{b}_{\sigma(i)}\right\|_{1}
$$
å…¶ä¸­ $\sigma$ å¯ä»¥çœ‹ä½œä¸€ä¸ªæ’åˆ—æˆ–è€…æ˜ å°„ï¼Œ$\sigma(i)$ ä»£è¡¨ç¬¬ i ä¸ª gt æ‰€åŒ¹é…çš„é¢„æµ‹çš„ indexï¼Œbox æŸå¤±ä½¿ç”¨çš„æ˜¯ GIoU æŸå¤±å’Œ L1 æŸå¤±çš„åŠ æƒã€‚æ³¨æ„åˆ°ç©º gt å’Œä»»ä½• prediction çš„ cost éƒ½æ˜¯ 0ï¼Œæ‰€ä»¥æœ¬è´¨ä¸Šå°±æ˜¯ N ä¸ª prediction å’Œ M ä¸ª gt ä¹‹é—´çš„åŒ¹é…ï¼Œç”¨äºç¡®å®š M ä¸ªæ­£æ ·æœ¬ prediction å’Œ N - M ä¸ªè´Ÿæ ·æœ¬ prediction

### Detection Loss

åŒ¹é…å®Œæˆåï¼Œå°±å¯ä»¥è®¡ç®—æŸå¤±å‡½æ•°
$$
\mathcal{L}_{\text {Hungarian }}(y, \hat{y})=\sum_{i=1}^{N}\left[-\log \hat{p}_{\hat{\sigma}(i)}\left(c_{i}\right)+\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \mathcal{L}_{\text {box }}\left(b_{i}, \hat{b}_{\hat{\sigma}}(i)\right)\right]
$$
è®ºæ–‡æåˆ°åœ¨è®¡ç®—åˆ†ç±»æŸå¤±æ—¶ï¼Œå¯¹äºç©ºç±» gt $\varnothing$ çš„åˆ†ç±»æŸå¤±è¦é™¤ä»¥ 10 ç”¨äºå¹³è¡¡æ­£è´Ÿæ ·æœ¬ï¼Œåœ¨å®ç°ä¸Šæ˜¯ç›´æ¥æŒ‡å®š `F.cross_entropy(..., weight=)` å®Œæˆ

å¦å¤–è¿˜æ˜¯ä½¿ç”¨äº†ä¸­é—´ç›‘ç£ï¼Œæˆ–è€…ç§°ä¸ºè¾…åŠ©æŸå¤±ã€‚å³æŠŠ decoder çš„ä¸­é—´ block çš„è¾“å‡ºä¹Ÿä½œä¸ºé¢„æµ‹ç»“æœï¼Œå¹¶è®¡ç®—æ£€æµ‹æŸå¤±

swin v2

DC5ï¼Œæ˜¯ dilated convolution at resnet stage 5 çš„ä¸€ä¸ªç®€ç§°ï¼Œåœ¨ DETR ä¸­ä½¿ç”¨çš„å…·ä½“æè¿°ä¸º

> Following [21, FCN], we also increase the feature resolution by adding a dilation to the last stage of the backbone and removing a stride from the first convolution of this stage.

ä½†æ˜¯åœ¨åé¢çš„ DETR-based ç›®æ ‡æ£€æµ‹å™¨ä¸­æ²¡æœ‰å¸¸ç”¨

### DETR é—®é¢˜

å¤§ç‰©ä½“å’Œå°ç‰©ä½“æ•ˆæœç›¸å·®å¤§

object query çš„é¢„æµ‹ç»“æœå¯è§†åŒ–

## Deformable DETR & Deformable Attention

### Multi-Scaele Deformable Attention

ä¸€å¼€å§‹çœ‹ Deformable DETR çš„æ—¶å€™ï¼Œæ³¨æ„åŠ›å¾ˆå®¹æ˜“é›†ä¸­åˆ° Deformable ä¸Šï¼Œä½†æˆ‘è§‰å¾— Multi-Scale åŒæ ·éå¸¸é‡è¦ã€‚ä¸ºäº†å½»åº•å¼„æ¸… Multi-Scale Deformable Attentionï¼Œæˆ‘æƒ³ä»å¦‚ä¸‹å‡ ä¸ªé—®é¢˜å…¥æ‰‹

1. å¦‚ä½•è¡¨ç¤º multi-scale feature
2. å¦‚ä½•è¡¨ç¤º multi-scale feature's positional embedding 
3. å¦‚ä½•è¡¨ç¤º reference points
4. å¦‚ä½•å®Œæˆ multi-scale deformable attention

#### Multi-scale Feature

è¿™éå¸¸å¥½ç†è§£ï¼Œå°±æ˜¯ä» backbone **ResNet 50** ä¸­è¾“å‡ºçš„ä¸­é—´ç‰¹å¾å±‚ï¼Œè¶Šæ·±çš„ç‰¹å¾å±‚ï¼Œåˆ†è¾¨ç‡è¶Šå°

```python
# output feature dict
features = self.backbone(images.tensor)

# project backbone features to the reuired dimension of transformer
multi_level_feats = self.neck(features)
```

æœ€ç»ˆ neck æŠŠè¿™äº›ç‰¹å¾å›¾è°±æ˜ å°„åˆ°åŒä¸€ä¸ªç»´åº¦ï¼Œä»¥è¾“å…¥åˆ° transformer ä¸­åšç‚¹ç§¯

<img src="C:\Data\Projects\notes\dev_notes\DETR\image-20221216190202156.png" alt="image-20221216190202156" style="zoom:50%;" />

#### Multi-scale Positional Embedding

**Encoder é˜¶æ®µ**

ç›´æ¥å¤„ç† multi-scale positional embeddingsï¼Œå°±æ˜¯é€ä¸ª level è·å¾—

```python
for feat in multi_level_feats:
    multi_level_masks.append(
        F.interpolate(img_masks[None], size=feat.shape[-2:]).to(torch.bool).squeeze(0)
    )
    multi_level_position_embeddings.append(self.position_embedding(multi_level_masks[-1]))
```

è§£é‡Šï¼špositional embedding æ˜¯æ ¹æ®ä¸€ä¸ª 2D mask ç›´æ¥ç”Ÿæˆçš„ï¼Œå¯¹äºä¸åŒ scale çš„ mask æ˜¯ç›´æ¥æ ¹æ® img mask æ’å€¼è·å¾—ï¼ˆimg mask ä¸­çš„éé›¶å€¼å³è¡¨ç¤ºè¯¥åƒç´ ç‚¹è¢«å¿½ç•¥ï¼‰ 

ä¸ºäº†å¯¹ä¸åŒ scale çš„ positional embedding è¿›è¡ŒåŒºåˆ†ï¼Œå†åŠ å…¥ scale embedding

```python
self.level_embeds = nn.Parameter(torch.Tensor(self.num_feature_levels, self.embed_dim))
# for each scale, pos_embed (B, H*W, C)
for lvl, pos_embed in enumerate(multi_level_pos_embeds):
	lvl_pos_embed = pos_embed + self.level_embeds[lvl].view(1, 1, -1)
```

**Decoder é˜¶æ®µ**

ä¸Šè¿°çš„ multi-scale positional embedding æ˜¯ç»™ encoder query ä½¿ç”¨ï¼Œåœ¨ decoder ä¸­ query ä¸å†æ˜¯å¤æ‚çš„å¤šå°ºåº¦ç‰¹å¾å›¾è°±ï¼Œè€Œå°±æ˜¯ä¸€èˆ¬çš„ embeddingï¼Œæ‰€ä»¥ä½¿ç”¨çš„ query positional embedding å°±æ˜¯ DETR ä¸­çš„ object queryï¼Œä¹Ÿæ˜¯ä¸€èˆ¬çš„ embeddingï¼ˆæ‰€è°“ä¸€èˆ¬ï¼ŒæŒ‡çš„æ˜¯éé¢„è®¾ï¼Œå¦‚ sine embedï¼‰

#### Reference Points

**Encoder é˜¶æ®µ**

reference points å°±æ˜¯æ¯ä¸ªåƒç´ ç‚¹ä¸­å¿ƒçš„**å½’ä¸€åŒ–åæ ‡**ã€‚æ¯ä¸€ä¸ª scale çš„ reference points ä¸ºä¸€ä¸ªå¼ é‡ï¼Œå½¢çŠ¶ä¸º (H, W, 2)ï¼Œé‚£ä¹ˆå¤šä¸ª scale çš„ reference points åˆèµ·æ¥åº”è¯¥æ˜¯ `(B, h1w1 + h2w2 + ..., 2)` æ‰å¯¹ï¼Œä½†å®é™…ä¸Šçš„ä»£ç å¹¶ä¸æ˜¯è¿™ä¹ˆåšçš„

```python
    def get_reference_points(spatial_shapes, valid_ratios, device):
        """
        Args:
            spatial_shapes (Tensor): The shape of all feature maps, has shape (num_level, 2).
            valid_ratios (Tensor): The ratios of valid points on the feature map, has shape (bs, num_levels, 2)
        Returns:
            Tensor: reference points used in decoder, has shape (bs, num_keys, num_levels, 2).
        """
        reference_points_list = []
        for lvl, (H, W) in enumerate(spatial_shapes):
            ref_y, ref_x = torch.meshgrid(
                torch.linspace(0.5, H - 0.5, H, dtype=torch.float32, device=device),
                torch.linspace(0.5, W - 0.5, W, dtype=torch.float32, device=device),
            )
            ref_y = ref_y.reshape(-1)[None] / (valid_ratios[:, None, lvl, 1] * H)   # (1, HW) / (B, 1) -> (B, HW)
            ref_x = ref_x.reshape(-1)[None] / (valid_ratios[:, None, lvl, 0] * W)
            ref = torch.stack((ref_x, ref_y), -1)   # (B, HW, 2)
            reference_points_list.append(ref)
        reference_points = torch.cat(reference_points_list, 1)  # (B, N1+N2+..., 2)
        reference_points = reference_points[:, :, None] * valid_ratios[:, None]
        return reference_points
```

å¯ä»¥çœ‹åˆ°æœ€ç»ˆçš„è¾“å‡ºå½¢çŠ¶æ˜¯ `(bs, num_keys, num_levels, 2)`ï¼Œ**å®é™…ä¸Šè¿™æ˜¯ä¸ºäº†å„ä¸ª scale ä¹‹é—´çš„äº¤äº’**ï¼Œå³æŸä¸ª scale çš„ reference point å¯ä»¥å»å¦ä¸€ä¸ª scale è¿›è¡Œé‡‡æ ·

**Decoder é˜¶æ®µ**

ç”±äº Decoder ä¸­çš„ query æ˜¯ä¸€èˆ¬çš„ embeddingï¼Œæ‰€ä»¥ä¸å¯èƒ½åƒä¸Šè¿°ä¸€æ ·ç”Ÿæˆä¸ä½ç½®å¼ºç›¸å…³çš„ reference pointsï¼Œå°±åªèƒ½ç”¨ä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚å¯¹ embedding è¿›è¡Œè½¬æ¢

```python
self.reference_points = nn.Linear(self.embed_dim, 2)
```

ç„¶åå† sigmoid è¿›è¡Œå½’ä¸€åŒ–ã€‚ä½†ä¹‹åä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜å°±ä½¿ç”¨äº†ä¸¤ä¸ªæŠ€å·§ï¼šquery selection & iterative box refinement

#### MSDeformAttn

ä¸‹é¢æ­£å¼ä»‹ç» multi-scale deformable attention æ¨¡å—ï¼Œå…ˆç®€å•æè¿°ä¸‹ä»£ç å¹²äº†ä»€ä¹ˆäº‹æƒ…

1. åˆ¤æ–­æ˜¯å¦ä¸ºè‡ªæ³¨æ„åŠ›ï¼Œå¦‚æœæ²¡æœ‰ä¼ å…¥ value åˆ™ä½¿ç”¨ query æœ¬èº«

2. ç»™ query åŠ å…¥ positional embeddingï¼Œ**æ³¨æ„ï¼Œè¿™é‡Œæ²¡æœ‰ç»™ key åŠ å…¥ positional embeddingï¼Œæ›´ç¡®åˆ‡åœ°è¯´ï¼Œåœ¨ deformable attetion é‡Œæ²¡æœ‰ key çš„æ¦‚å¿µï¼Œkey æå…¶ attention æ˜¯é€šè¿‡å…¶ä»–æ–¹å¼è·å¾—**

3. value ç»è¿‡ä¸€ä¸ªçº¿æ€§å±‚ï¼Œç»´åº¦ä¸å˜ï¼Œå¹¶ä¸” mask æ‰ä¸éœ€è¦è¿›è¡Œæ³¨æ„åŠ›çš„ç‚¹ã€‚ç„¶åå† view ä¸ºå¤šå¤´çš„å½¢å¼

4. å°† query é€åˆ° `self.sampling_points` çº¿æ€§å±‚ï¼Œè¿›è¡Œåç§»é‡é¢„æµ‹ï¼Œä¹Ÿå¯¹åç§»é‡è¿›è¡Œå½’ä¸€åŒ–

   ```python
   self.sampling_offsets = nn.Linear(embed_dim, num_heads * num_levels * num_points * 2)
   ```

5. å°† query é€åˆ° `self.attention_weight` çº¿æ€§å±‚ï¼Œå¹¶ç”¨ softmax è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°

   ```python
   self.attention_weights = nn.Linear(embed_dim, num_heads * num_levels * num_points)
   ```

6. æœ‰äº†å‰é¢çš„å‡†å¤‡å·¥ä½œï¼Œå°±èƒ½å¤Ÿæ„‰å¿«è®¡ç®—å¯å˜æ³¨æ„åŠ›äº†ï¼Œpure pytorch å®ç°ä¸º `multi_scale_deformable_attn_pytorch`ï¼Œæ³¨æ„è¿™é‡Œçš„è¾“å‡ºå·²ç»åˆå¹¶äº†å¤šå¤´

7. è·å¾—çš„ç»“æœå†è¿‡ä¸€ä¸ªçº¿æ€§å±‚ï¼Œç»´åº¦ä¸å˜

ç”±äºä»£ç è¿‡äº mmlab çš„é£æ ¼ğŸ¤£ï¼Œæˆ‘åˆ é™¤äº†ä¸€äº›ï¼Œä»¥æ–¹ä¾¿ç†è§£

```python
    def forward(
        self,
        query: torch.Tensor,	# (B, N, C)
        key = None,
        value = None,
        identity = None,
        query_pos = None,
        key_padding_mask = None,
        reference_points = None,
        spatial_shapes = None,
        level_start_index = None,
        **kwargs) -> torch.Tensor:
        
        if value is None:	# True, when self-attetion
            value = query
        if identity is None:	# True
            identity = query
        if query_pos is not None:	# True
            query = query + query_pos
        bs, num_query, _ = query.shape
        bs, num_value, _ = value.shape

        value = self.value_proj(value)
        
        if key_padding_mask is not None:	# True
            value = value.masked_fill(key_padding_mask[..., None], float(0))
        value = value.view(bs, num_value, self.num_heads, -1)
        
        sampling_offsets = self.sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2
        )
        # bs, num_query, num_heads, num_levels, num_points, 2
        offset_normalizer = torch.stack([spatial_shapes[..., 1], spatial_shapes[..., 0]], -1)
        sampling_locations = (
            reference_points[:, :, None, :, None, :]
            + sampling_offsets / offset_normalizer[None, None, None, :, None, :]
        )
        
        attention_weights = self.attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        attention_weights = attention_weights.softmax(-1)
        attention_weights = attention_weights.view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points)
        
        output = multi_scale_deformable_attn_pytorch(
                value, spatial_shapes, sampling_locations, attention_weights)
        output = self.output_proj(output)

        return self.dropout(output) + identity
```

pure pytorch ä»£ç å—å¦‚ä¸‹ï¼Œæ¯”è¾ƒéš¾çœ‹çš„æ˜¯å¼ é‡çš„å½¢çŠ¶ï¼Œæˆ‘éƒ½ä»¥æ³¨é‡Šç»™å‡ºï¼Œåº”è¯¥æ¯”è¾ƒå¥½ç†è§£ã€‚ç”¨ç®€æ´çš„è¯­è¨€æ¦‚æ‹¬ä¸ºï¼š

1. å¯¹æ¯ä¸€ä¸ª scale/levelï¼Œè®¡ç®—æ‰€æœ‰ sampling points åœ¨è¯¥ level æ’å€¼å¾—åˆ°çš„ç‰¹å¾å‘é‡
2. å¯¹æ’å€¼å¾—åˆ°çš„ multi scale + multi points çš„ç‰¹å¾å‘é‡è¿›è¡Œæ³¨æ„åŠ›åŠ æƒæ•´åˆ
3. åˆå¹¶å¤šä¸ª head çš„ç‰¹å¾å‘é‡

```python
def multi_scale_deformable_attn_pytorch(
    value: torch.Tensor,
    value_spatial_shapes: torch.Tensor,
    sampling_locations: torch.Tensor,
    attention_weights: torch.Tensor,
) -> torch.Tensor:

    bs, _, num_heads, embed_dims = value.shape
    _, num_queries, num_heads, num_levels, num_points, _ = sampling_locations.shape
    value_list = value.split([H_ * W_ for H_, W_ in value_spatial_shapes], dim=1)
    sampling_grids = 2 * sampling_locations - 1
    sampling_value_list = []
    
    for level, (H_, W_) in enumerate(value_spatial_shapes):
        # bs, H_*W_, num_heads, embed_dims -> bs*num_heads, embed_dims, H_, W_
        value_l_ = (
            value_list[level].flatten(2).transpose(1, 2).reshape(bs * num_heads, embed_dims, H_, W_)
        )
        # bs, num_queries, num_heads, num_points, 2 -> bs*num_heads, num_queries, num_points, 2
        sampling_grid_l_ = sampling_grids[:, :, :, level].transpose(1, 2).flatten(0, 1)
        # bs*num_heads, embed_dims, num_queries, num_points
        sampling_value_l_ = F.grid_sample(
            value_l_, sampling_grid_l_, mode="bilinear", padding_mode="zeros", align_corners=False
        )
        sampling_value_list.append(sampling_value_l_)
    # (bs, num_queries, num_heads, num_levels, num_points) -> (bs*num_heads, 1, num_queries, num_levels*num_points)
    attention_weights = attention_weights.transpose(1, 2).reshape(
        bs * num_heads, 1, num_queries, num_levels * num_points
    )
    output = ( # (bs*num_heads, embed_dims, num_quries, num_levels*num_points)
        (torch.stack(sampling_value_list, dim=-2).flatten(-2) * attention_weights) 
        .sum(-1)
        .view(bs, num_heads * embed_dims, num_queries)
    )
    return output.transpose(1, 2).contiguous()
```

### Decoder

çœ‹å®Œäº† Deformable Attentionï¼Œå®é™…ä¸Šåœ¨ Decoder ä¸Šçš„ä¸œè¥¿ä¹Ÿå¾ˆä¸ä¸€æ ·ã€‚ä»æ­¤å¼€å§‹ Decoder ä¸å†ä¿æŒåŸå§‹ DETR çš„ç®€æ´æ€§ï¼Œä¸ºäº†è®©æ”¶æ•›æ›´å¿«ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨æ›´å¼ºçš„å…ˆéªŒå‡è®¾

1. query selection
2. encode new query

#### Query Selection

æ„Ÿè§‰æœ‰ beam search çš„æ„æ€ï¼šä¸éœ€è¦æ¯æ¬¡å¯¹å…¨éƒ¨çš„é€‰æ‹©è¿›è¡Œæš´åŠ›æœç´¢ï¼Œè€ŒåŸºäºæ’åé å‰çš„é€‰æ‹©ç»§ç»­è¿›è¡Œæœç´¢ã€‚è¿™ä¸ªæ–¹æ³•ä¹Ÿè¢«ç§°ä¸º DETR two stageï¼Œä¸¤é˜¶æ®µæ–¹æ³•ã€‚**Query ç°åœ¨ä¸æ˜¯æ¥è‡ªäºéšæœºåˆå§‹åŒ–çš„ embeddingï¼Œè€Œæ˜¯æ¥è‡ªäº Encoder Output + Preset Anchorï¼ˆæ²¡é”™ï¼Œanchor is EVERYWHERE!ï¼‰**

Query Selection æ˜¾ç„¶éœ€è¦å®Œæˆä¸¤ä»¶äº‹ï¼š

1. ç”Ÿæˆ query / proposalã€‚æ–¹æ³•æ˜¯åŸºäºé¢„è®¾ anchor çš„ proposal é¢„æµ‹ã€‚æ¯ä¸€ä¸ªé¢„è®¾ anchor å¯¹åº”ä¸€ä¸ª encoder output pixelï¼Œç„¶åç”Ÿæˆä¸€ä¸ªé€‰æ¡†æå…¶å¯¹åº”åˆ†ç±»ã€‚æ‰€ä»¥è¯´è¿™é‡Œçš„ **query å°±æ˜¯ proposal**
2. æ’åº query / proposalã€‚å¾—åˆ†é«˜ proposal çš„æ’åºé å‰ï¼Œå¹¶æ³¨æ„åˆ†ç±»ä»»åŠ¡ä¸ºäºŒåˆ†ç±»ï¼Œå³åªåˆ†å‰æ™¯å’ŒèƒŒæ™¯ï¼Œä½†è¿™ä¸ªäºŒåˆ†ç±»ä»»åŠ¡å®Œæˆå¾—å¾ˆå¦™ï¼Œå’ŒåŸæ¥çš„ 80 ç±»ç›´æ¥è¿›è¡Œäº†ä¸€ä¸ªç»Ÿä¸€ï¼šç›´æ¥æŠŠæ‰€æœ‰çš„ label æ ‡ç­¾æŒ‡å®šä¸º0ï¼Œå³å¯å®ŒæˆäºŒåˆ†ç±»

å®Œæˆä¸Šè¿°ä»»åŠ¡éœ€è¦è°ƒç”¨ä¸‹æ–¹å‡½æ•° `gen_encoder_output_proposals`ï¼Œè¿™é‡Œä»…ç»™å‡ºç®€å•æ³¨é‡Š

```python
    def gen_encoder_output_proposals(self, memory, memory_padding_mask, spatial_shapes):
        """
        Args:
            - memory: output of encoder, (B, S, C)
            - memory_padding_mask: (B, S)
            - spatial_shapes: (B, 2)
        Return:
            - output_memory: **MASKED** and projected new memory (B, S, C)
            - output_proposals: reference point **UNsigmoid** anchors (B, S, 4)
        """
```

æ­¤æ—¶è¿˜å¾—åˆ°ä¸€ä¸ªå¥½å¤„ï¼Œæ—¢ç„¶æˆ‘ä»¬çš„ query å·²ç»å’Œä½ç½®ç›¸å…³äº†ï¼Œé‚£ä¹ˆå…¶ reference points ä¹Ÿèƒ½å¤Ÿç›´æ¥ä½¿ç”¨è¯¥ proposal çš„ä½ç½®å’Œå¤§å°

å¦å¤–ä¸€ç‚¹ï¼Œå¯¹äºåˆå§‹åŒ–çš„ anchorï¼Œç½‘ç»œå¯¹å…¶å€¼å¹¶ä¸æ•æ„Ÿ

#### Iterative Box Refinement

è¿™é‡Œæ„Ÿè§‰æ˜¯æ®‹å·®ï¼Œæˆ–è€… step by step çš„æ€æƒ³ï¼Œæ•´ä¸ª trick éå¸¸å¯¹æˆ‘çš„å£å‘³ğŸ‘æˆ‘ä¸äº†è§£ Diffusion Modelï¼Œä¸çŸ¥é“è¿™ç§ step by step æ˜¯ä¸æ˜¯ç±»ä¼¼çš„

åœ¨ DETR ä¸­ decoder æ¯ä¸€å±‚éƒ½ä¼šå»é¢„æµ‹æœ€ç»ˆçš„é€‰æ¡†ï¼Œæ¯ä¸€å±‚çš„é¢„æµ‹å¯èƒ½å·®è·éå¸¸å¤§ï¼Œè¿™æ ·çš„è®­ç»ƒè¿‡ç¨‹æ˜¾ç„¶ä¸å¤Ÿç¨³å®šã€‚é—®é¢˜æå‡ºæ¥äº†ï¼Œæ”¹è¿›æ–¹æ³•ä¹Ÿæ˜¯è·ƒç„¶çº¸ä¸Šï¼šè¦æ±‚æ¯ä¸€å±‚çš„é¢„æµ‹æ˜¯åŸºäºä¸Šä¸€å±‚çš„é¢„æµ‹ï¼Œè¿™æ ·å°±åªé¢„æµ‹ä¸€ä¸ªå˜åŒ–é‡

åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥æ ¹æ®è¿™ä¸ªé¢„æµ‹å»æ›´æ–°æˆ‘ä»¬çš„ reference pointsï¼Œå› ä¸ºé¢„æµ‹çš„æ¡†ä¼šæœç€ gt å»æ¢ç´¢ï¼Œreference points ä¸å…¶ä¸€èµ·æ›´æ–°ä¼šæ”¶æ•›æ›´å¿«ï¼Œè€Œä¸æ˜¯ä»å¤´åˆ°å°¾ä½¿ç”¨åŒä¸€å¥— reference pointsï¼Œå¹¶ä¸”æ­¤æ—¶ referece points ä¸º reference boxï¼Œæœ€åä¸€ä¸ªç»´åº¦æ˜¯ 4

æ³¨æ„ï¼Œæ¯ä¸ª scale/level çš„ `bbox_embed or class_embed` éƒ½æ˜¯ç‹¬ç«‹çš„

```python
for layer_idx, layer in enumerate(self.layers):
    output = layer(q, k, v, ...)
    
    tmp = self.bbox_embed[layer_idx](output)
    new_reference_points = tmp + inverse_sigmoid(reference_points)
    new_reference_points = new_reference_points.sigmoid()
    
	reference_points = new_reference_points.detach()	# no detach is better!
    
    intermediate.append(output)
    intermediate_reference_points.append(reference_points)
```

æˆ‘è§‰å¾—å¯ä»¥ç›´æ¥å¯¹ reference points è®¡ç®—æŸå¤±ï¼Œçœç•¥è®¡ç®—ï¼Œä½†åŸä»£ç é‡æ–°è®¡ç®—äº†

```python
        for lvl in range(inter_states.shape[0]):
            if lvl == 0:
                reference = init_reference
            else:
                reference = inter_references[lvl - 1]
            reference = inverse_sigmoid(reference)
            outputs_class = self.class_embed[lvl](inter_states[lvl])
            tmp = self.bbox_embed[lvl](inter_states[lvl])
            tmp += reference
            
            outputs_coord = tmp.sigmoid()
            outputs_classes.append(outputs_class)
            outputs_coords.append(outputs_coord)
```

## DeNoising Training

TODO å¯ä»¥ç†è§£ä¸ºè¾…åŠ©ä»»åŠ¡

## Q

### ä¸ºä»€ä¹ˆä¸éœ€è¦ NMS

éš¾é“è¯´ DETR çš„è¯¯æ£€ç‡åˆ°åº•å¦‚ä½•ï¼Ÿ

### Encoder ä¸­ä¹ŸåŠ å…¥ç›‘ç£ä¿¡å·

ä¸çŸ¥é“æœ‰æ²¡æœ‰ç±»ä¼¼çš„å·¥ä½œï¼Œæ„Ÿè§‰åƒä¸­å­¦çš„é˜…è¯»ç†è§£ï¼Œä¼šå…ˆçœ‹ä¸€ä¸‹é—®é¢˜ï¼Œç„¶åå†å»çœ‹æ–‡ç« ï¼Œå¸¦ç€é—®é¢˜å»é˜…è¯»

Forward-Forward
