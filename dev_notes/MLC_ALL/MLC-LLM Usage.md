# MLC-LLM Usage

å¦‚ä½•ä½¿ç”¨ MLC-LLM å·¥å…·é“¾ï¼Œéœ€è¦å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

1. MLC-LLM çš„å®‰è£…å¦‚ä½•è¿è¡Œ
2. å¦‚ä½•æ„å»ºæ¨¡å‹
3. å¦‚ä½•å°†æ¨¡å‹è¿›è¡Œç¼–è¯‘å¹¶è¿è¡Œ

## Concept

- å®‰è£… [tvm unity install](https://llm.mlc.ai/docs/install/tvm.html)

   æœ€ç®€å•çš„æ–¹å¼æ˜¯é€šè¿‡ pre-built package ä¸€é”®å®‰è£…ï¼Œä¸‹é¢æ˜¯å®‰è£…äº† cuda 12.2 ç‰ˆæœ¬çš„ tvm unity

   ```shell
   pip install --pre -U -f https://mlc.ai/wheels mlc-ai-nightly-cu122
   ```

   å¦‚æœéœ€è¦å®‰è£…å…¶ä»– cuda ç‰ˆæœ¬åŒ…ï¼Œå¯ä»¥å» https://mlc.ai/wheels å¯»æ‰¾å¯¹åº”çš„ wheel åŒ…ï¼Œä¸‹è½½åå®‰è£…

   > pip install -pre ä¼šå°† pre-release version packages æ”¾å…¥æœç´¢è·¯å¾„å½“ä¸­

   > tvm unity æ˜¯ tvm çš„ä¸€ä¸ªåˆ†æ”¯ç‰ˆæœ¬ï¼Œä½ å¯ä»¥åœ¨ tvm github çš„ branch ä¸­çœ‹åˆ°ä»–ã€‚å®é™…ä¸Š tvm unity å°±æ˜¯ tvm çš„æœ€æ–°ç‰ˆæœ¬ï¼ŒåŠŸèƒ½å’Œ tvm ä¸€æ ·ï¼Œæä¾›æ¨¡å‹ç¼–è¯‘åŠŸèƒ½

   é™¤æ­¤ä¹‹å¤–è¿˜å¯èƒ½è¦å®‰è£… xgboost

   ```python
   pip install xgboost --force
   ```

   > æˆ‘ä½¿ç”¨ --forece å¼ºåˆ¶å®‰è£…ï¼Œä¹Ÿå¯ä»¥ç”¨ -U æ›´æ–°å®‰è£…

   æ£€éªŒå®‰è£…æ˜¯å¦å®Œæˆ

   ```shell
   python -c "import tvm; print(tvm.__file__)"
   python -c "import tvm; print(tvm._ffi.base._LIB)"
   python -c "import tvm; print(tvm.cuda().exist)"
   ```

- å®‰è£… [mlc-llm install](https://llm.mlc.ai/docs/install/mlc_llm.html)

   æ¨¡å‹éƒ¨ç½²å¯ä»¥åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š compile & runtimeï¼Œtvm unity æä¾›äº† compile åŠŸèƒ½ï¼Œè€Œ mlc åˆ™æä¾›äº† runtime åŠŸèƒ½ï¼ˆä¸ªäººç†è§£ï¼‰

   åŒæ ·åœ°ï¼Œé€šè¿‡ä¸‹è½½ wheels è¿›è¡Œæœ¬åœ°å®‰è£…

   ```python
   pip install --pre -U -f https://mlc.ai/wheels mlc-llm-nightly-cu122 mlc-ai-nightly-cu122
   ```

- Overview Function of tvm & mlc-llm

   æˆ‘ä»¬è¦åšçš„äº‹æƒ…éå¸¸æ˜ç¡®ï¼šæ„å»ºæ¨¡å‹ -> ç¼–è¯‘æ¨¡å‹ -> è¿è¡Œæ¨¡å‹

   tvm å…¶å®ä¸ºè¿™ä»¶äº‹æƒ…æä¾›äº†å®Œæ•´çš„èƒ½åŠ›æ”¯æ’‘ï¼šæˆ‘ä»¬å¯ä»¥é€šè¿‡ tvm python api (relax) æ¥å®Œæˆæ¨¡å‹æ„å»ºï¼Œç„¶åå¯¹æ‰€æ„å»ºçš„æ¨¡å‹è¿›è¡Œç¼–è¯‘ï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¯¹åº”çš„å¹³å°ä¸Šè¿è¡Œ

   è€Œ mlc-llm åˆ™æ˜¯ä¸€ä¸ªç”± tvm æ”¯æ’‘çš„ï¼Œé¢ç›¸ LLM çš„å¼€å‘ä»“åº“ã€‚mlc-llm åŒ…å«äº†è®¸å¤šç”± tvm python api æ„å»ºçš„è¯­è¨€æ¨¡å‹ã€‚å¹¶ä¸”å®šä¹‰äº†è®¸å¤š chat é…ç½®æ¨¡ç‰ˆï¼Œå¯ä»¥æ–¹ä¾¿åœ°è°ƒç”¨æ¨¡å‹æ¥æ„å»ºå®Œæ•´çš„èŠå¤©åº”ç”¨

- å¦‚ä½•é€šè¿‡ relax (tvm python api) æ¥æ„å»ºæ¨¡å‹ï¼Œå¹¶ç¼–è¯‘è¿è¡Œï¼Œä»¥ linear å±‚ä¸ºä¾‹

   å‚è€ƒ [notebook](https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_add_new_model_architecture_in_tvm_nn_module.ipynb) [define_new_models](https://llm.mlc.ai/docs/compilation/define_new_models.html)

   ç›´æ¥è·‘å®˜æ–¹çš„ notebook è‚¯å®šæ˜¯è·‘ä¸é€šçš„ï¼Œç»§æ‰¿äº† tvm æ–‡æ¡£çš„ä¸€è´¯é£æ ¼ğŸ˜‚åŸå› åœ¨äº tvm/relax api å˜åŒ–å¤ªå¿«ï¼Œä½†æ˜¯æ–‡æ¡£å®Œå…¨æ²¡æœ‰è·Ÿä¸Šï¼Œä¸‹é¢å°±æ˜¯å…¶ä¸­ä¸€ä¸ªä¾‹å­

   ```python
   # before
q = q.permute_dims([0, 2, 1, 3])  # [b, h, s, d]
   k = k.permute_dims([0, 2, 1, 3])  # [b, h, t, d]
   v = v.permute_dims([0, 2, 1, 3])  # [b, h, t, d]
   
   # after
q = q.permute_dims(0, 2, 1, 3)  # [b, h, s, d]
   k = k.permute_dims(0, 2, 1, 3)  # [b, h, t, d]
   v = v.permute_dims(0, 2, 1, 3)  # [b, h, t, d]
   ```
   
   æ— æ³•å®Œæˆå¯¹ `nn.KVCache` çš„ä¸€äº›æ“ä½œ

- Expression Node

   

- TIR & AST

   ä¸‹é¢çš„æ•´ç†åŸºæœ¬ä¸Šæ¥è‡ªäº [TVM è‡ªåº•å‘ä¸Šï¼ˆäºŒï¼‰ï¼šTIR çš„æ¦‚å¿µå’Œç¼–è¯‘åŸç†](https://zhuanlan.zhihu.com/p/533161438)

   TIR æ˜¯ TVM ä¸­æœ€æ¥è¿‘ç›®æ ‡ç¡¬ä»¶çš„æ•°æ®ç»“æ„ï¼Œæ˜¯å¯ä»¥è¢«ç¼–è¯‘ä¸ºç›®æ ‡ç¡¬ä»¶ç¼–ç¨‹è¯­è¨€ï¼ˆC++ã€CUDAã€LLVM IRç­‰ï¼‰çš„ä¸­é—´è¡¨ç¤ºã€‚å¯¹äºæ²¡æœ‰å­¦ä¹ è¿‡ç¼–è¯‘åŸç†çš„è¯»è€…è€Œè¨€ï¼Œä¼šæ¯”è¾ƒéš¾ä»¥ç†è§£ï¼š**å„ç§ç¼–ç¨‹è¯­è¨€çš„è¯­æ³•ï¼ŒåŒºåˆ«éƒ½éå¸¸å¤§ï¼Œé‚£ä¹ˆ TIR æ˜¯å¦‚ä½•åšåˆ°å¯ä»¥ç¼–è¯‘ä¸ºä»»ä½•ä¸€ç§è¯­è¨€çš„ï¼Ÿ**

   **è¿™é‡Œå°±è¦å¼•å…¥ä¸€ä¸ªæ–°çš„æ¦‚å¿µï¼Œå«æŠ½è±¡è¯­æ³•æ ‘ï¼ˆAbstract Syntax Treeï¼Œä»Šåéƒ½ç®€ç§°ä¸º ASTï¼‰ã€‚ä¸ç®¡ä»»ä½•ç¼–ç¨‹è¯­è¨€ï¼Œæœ‰ä»€ä¹ˆç‰¹æ€§å¦‚ä½•ï¼Œè¯­æ³•å¦‚ä½•ï¼Œä¸€æ®µç¨‹åºéƒ½æ˜¯å¦‚ä¸‹å…ƒç´ ç»„æˆçš„ï¼š**

   - **å˜é‡çš„å£°æ˜ã€å˜é‡åˆå§‹åŒ–ï¼ˆèµ‹å€¼æˆ–è€…å†…å­˜åˆ†é…ï¼‰**
   - **å˜é‡çš„è¿ç®—ï¼ˆå››åˆ™è¿ç®—ã€å¸ƒå°”è¿ç®—ç­‰ï¼‰ã€å‡½æ•°çš„è°ƒç”¨**
   - **æµç¨‹çš„æ§åˆ¶ï¼ˆif-else æ¡ä»¶åˆ¤æ–­ï¼Œå¾ªç¯ç­‰ï¼‰**

   **é‚£ä¹ˆï¼Œä»»ä½•ä¸€æ®µä»£ç ï¼Œéƒ½å¯ä»¥è¡¨è¾¾ä¸ºç±»ä¼¼äºå¦‚ä¸‹çš„æ ‘ç»“æ„ï¼š**

   <img src="MLC-LLM Usage/image-20240323151539137.png" alt="image-20240323151539137" style="zoom:50%;" />

   é€šå¸¸ï¼ŒAST æ˜¯æŒ‰ç…§ä¸­åºéå†æ¥é˜…è¯»çš„ï¼Œé‚£ä¹ˆï¼Œä¸Šé¢çš„ä»£ç ç¿»è¯‘ä¸º C++ï¼Œå°±æ˜¯ï¼š

   ```c++
   void main(int x) {
       if (x < 5) {
           x = x * 2;
       }
   }
   ```

   ç¿»è¯‘æˆ Pythonï¼Œå°±æ˜¯ï¼š

   ```python
   def main(x):
       if x < 5:
           x = x * 2
   ```

   **å³ä½¿ C++ å’Œ Python çš„è¯­æ³•ä¸åŒï¼Œåªè¦å®ç°ç›¸åº”çš„ç¿»è¯‘å™¨ï¼ˆä¹Ÿå°±æ˜¯ CodeGenï¼‰ï¼Œåœ¨ä¸­åºéå†çš„è¿‡ç¨‹ä¸­ï¼Œå°†æ ‘èŠ‚ç‚¹ç¿»è¯‘ä¸ºç›¸åº”è¯­æ³•çš„å­—ç¬¦ä¸²ï¼Œå°±å¯ä»¥å¾—åˆ°æœ€ç»ˆçš„æºä»£ç äº†ã€‚**

   <img src="MLC-LLM Usage/image-20240323151942538.png" alt="image-20240323151942538" style="zoom:50%;" />

   **æœ‰äº† ASTï¼ŒTIR å°±èƒ½è§£å†³å¦‚ä¸‹ç—›ç‚¹ï¼šç›¸åŒçš„è®¡ç®—æŒ‡ä»¤ã€åŠ é€ŸæŒ‡ä»¤åœ¨ä¸åŒç¡¬ä»¶ä¹‹é—´çš„è½¬æ¢ã€‚**

   ç›®å‰å¯¹ AST æœ‰äº†ä¸€å®šäº†è§£ï¼Œé‚£åˆ°åº•ä»€ä¹ˆæ˜¯ TIR å‘¢ï¼Ÿå¦‚ä½•æ„å»ºä¸€ä¸ª TIRï¼Ÿå¹¶ä¸” TIR åˆæ˜¯å¦‚ä½•è½¬å˜ä¸º AST çš„ï¼Ÿ

   ä»¥ä¸‹æ˜¯ä¸ªäººç†è§£ï¼š

   > TensorIR å°±æ˜¯ AST æœ¬èº«ï¼Œè€Œé€šå¸¸æˆ‘ä»¬æ‰€è¯´çš„ TensorIR æ˜¯ TensorIR AST çš„ç®€ç§°ï¼Œä¸ºäº†è®©æ¦‚å¿µå˜å¾—æ›´åŠ æ¸…æ™°å°† TensorIR code & TensorIR AST è¿›è¡ŒåŒºåˆ†
   >
   > AST å¯ä»¥ç¿»è¯‘æˆä¸ºå„ç§è¯­è¨€ï¼Œä½†æˆ‘ä»¬é€šå¸¸éœ€è¦é€‰æ‹©ä¸€ç§ä¸»è¯­è¨€æ¥å®Œæˆ TensorIR code ä¸ TensorIR AST ä¹‹é—´çš„è½¬æ¢ï¼Œè€Œ python æ˜¾ç„¶æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©
   >
   > ä½¿ç”¨ Python æ‰€å®ç°çš„ python TensorIR codeï¼Œæˆ‘ä»¬å°±ç§°ä¹‹ä¸º TVMScriptã€‚TVMScript å€ŸåŠ© python AST èƒ½å¤Ÿå°† code è½¬æ¢ä¸º TensorIR ASTï¼Œä»è€Œå†åˆ©ç”¨å…¶ä»–è¯­è¨€çš„ CodeGen è½¬æ¢ä¸ºå…¶ä»–è¯­è¨€ã€‚

- IRModule & PrimFunc & CodeGen

   ä½¿ç”¨ TVMScrip æ¥ç¼–å†™ python TensorIR code æœ‰å‡ ä¸ªé‡è¦çš„ç»„ä»¶ï¼š

   1. IRModuleï¼ŒIRModule æ˜¯å¯ä»¥è¢«ç¼–è¯‘çš„æœ€å°å•å…ƒï¼Œæ‰€æœ‰çš„ TensorIR code éƒ½å¿…é¡»åœ¨ IRModule ä¸­å®ç°
   2. PrimFuncï¼ŒPrimFunc æ˜¯ä¸€ä¸ªå®Œæ•´çš„å‡½æ•°ï¼Œèƒ½å¤Ÿä½œä¸º API å…¥å£è¢«ç¼–è¯‘åçš„ IRModule è°ƒç”¨

   ```python
   import tvm
   from tvm.ir.module import IRModule
   from tvm.script import tir as T
   import numpy as np
   
   @tvm.script.ir_module
   class MyModule:
       @T.prim_func
       def main(a: T.handle, b: T.handle):
           # We exchange data between function by handles, which are similar to pointer.
           T.func_attr({"global_symbol": "main", "tir.noalias": True})
           # Create buffer from handles.
           A = T.match_buffer(a, (8,), dtype="float32")
           B = T.match_buffer(b, (8,), dtype="float32")
           for i in range(8):
               # A block is an abstraction for computation.
               with T.block("B"):
                   # Define a spatial block iterator and bind it to value i.
                   vi = T.axis.spatial(8, i)
                   B[vi] = A[vi] + 1.0
   
   
   ir_module = MyModule
   print(type(ir_module))
   print(ir_module.script())
   ```

   3. Compileï¼Œç¼–è¯‘ IRModuleï¼Œçœ‹ä¸‹æ•ˆæœ

   ```python
   import numpy as np
   
   mod = tvm.build(ir_module, target="llvm")
   # mod = tvm.build(ir_module, target="cuda")
   
   a = tvm.nd.array(np.arange(8).astype("float32"))
   # [0. 1. 2. 3. 4. 5. 6. 7.]
   
   b = tvm.nd.array(np.zeros((8,)).astype("float32"))
   mod(a, b)
   # [1. 2. 3. 4. 5. 6. 7. 8.]
   ```

   `tvm.build` çš„æœ€åä¸€ä¸ªå‚æ•° targetï¼Œå°±æ˜¯ç”¨æ¥é€‰æ‹©ç”¨å“ªä¸€ä¸ª CodeGen æ¥ç¼–è¯‘ TIR AST

   > TIR AST -> C++/CUDA -> bin

- Algebraic Data Types (ADTs)

   é€šè¿‡ç»„åˆæ—§çš„æ•°æ®ç±»å‹æ¥å®šä¹‰æ–°çš„æ•°æ®ç±»å‹

   > ADTs are trying to define new types by combining exsisting types. (result from GPT)

- Expression & Statement

   [TVM è‡ªåº•å‘ä¸Šï¼ˆäºŒï¼‰ï¼šTIR çš„æ¦‚å¿µå’Œç¼–è¯‘åŸç†](https://zhuanlan.zhihu.com/p/533161438)

   CodeGenC ä¼šéå†åˆ°ä¸¤ç§ TIR Nodeï¼šExpressionï¼ˆè¡¨è¾¾å¼ï¼‰ å’Œ Statementï¼ˆè¯­å¥ï¼‰ã€‚Expressionï¼ˆè¡¨è¾¾å¼ï¼‰ä¸­åŒ…å«äº†å¸¸è§çš„å˜é‡å£°æ˜ã€è¿ç®—ã€åˆ¤æ–­ã€å‡½æ•°è°ƒç”¨ï¼Œè€Œ Statementï¼ˆè¯­å¥ï¼‰ä¸­åŒ…å«äº†æ§åˆ¶æµï¼ˆif-elseï¼ŒLoop ç­‰ï¼‰ã€å†…å­˜ç®¡ç†ã€èµ‹å€¼ï¼ˆAssignmentï¼‰ç­‰æ“ä½œ

- Frontend & Backbend & RelayIR & TensorIR

   [TVM åŸºæœ¬æ¡†æ¶å’Œæ¦‚å¿µ](https://zhuanlan.zhihu.com/p/532873577)

   å‘ä¸Šï¼Œå…¼å®¹æ‰€æœ‰ä¸åŒçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆä¹Ÿå« frontendï¼‰ï¼Œä¾‹å¦‚ pytorchã€TensorFlowã€onnx

   å‘ä¸‹ï¼Œå…¼å®¹æ‰€æœ‰ä¸åŒçš„åº•å±‚ç¡¬ä»¶å’Œæ¨ç†æ¡†æ¶ï¼ˆä¹Ÿå« backendï¼‰ï¼ŒåŒæ—¶æ€§èƒ½æœ€å¤§åŒ–ï¼Œä¾‹å¦‚ x86 cpuã€arm cpuã€mali gpuã€nvidia gpu

   **å› æ­¤ï¼Œä¸ºäº†è¦†ç›–ä¸Šè¿°çš„å…¨éƒ¨åœºæ™¯ï¼ŒTVM ä¸­å¼•å…¥äº†ä¸¤ä¸ª IR**ï¼ˆIntermediate Representationï¼Œå³ä¸­é—´è¡¨ç¤ºï¼‰ï¼š

   1. ä¸ºäº†å‘ä¸Šå…¼å®¹çš„ **Relay IR**ï¼ˆç®€ç§° Relayï¼‰ï¼ŒåŸºäºä¸åŒæ·±åº¦å­¦ä¹ å¹³å°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œåœ¨è¿›å…¥ TVM åéƒ½ä¼šé¦–å…ˆè¢«è½¬æ¢ä¸º Relay çš„è¡¨ç¤ºï¼Œæ¶ˆé™¤è¡¨ç¤ºå·®å¼‚ï¼›

   2. ä¸ºäº†å‘ä¸‹å…¼å®¹çš„ **Tensor IR**ï¼ˆç®€ç§° TIRï¼‰ï¼Œæ‰€æœ‰æ¨¡å‹åœ¨ç¼–è¯‘ä¸ºæŒ‡å®šç¡¬ä»¶ä¸Šçš„æºä»£ç ä¹‹å‰ï¼Œéƒ½è¦å…ˆ lower ä¸º TIRã€‚

      ![image-20240319111441580](MLC-LLM Usage/image-20240319111441580.png)

   ä¸ºä»€ä¹ˆè¦ä½¿ç”¨ä¸¤ç§ IRï¼Œä¸ºä»€ä¹ˆä¸åªè®¾è®¡ TensorIRï¼Œç›´æ¥ä» onnx -> cuda ä¸€æ­¥åˆ°ä½ã€‚ä¸ªäººç†è§£æœ‰ä»¥ä¸‹åŸå› ï¼š

   1. ä» pytorch/onnx è¡¨ç¤ºåˆ° CUDA è¡¨ç¤ºæœ‰è®¸å¤šä¼˜åŒ–æ˜¯å¯ä»¥è¿›è¡Œçš„ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å±‚çº§å¼çš„ä¼˜åŒ–æ˜¯æ›´å®¹æ˜“å®ç°çš„
   2. æ›´é«˜æŠ½è±¡çš„ IR æœ‰åˆ©äºå¿«é€Ÿåœ°è¡¨ç¤ºè®¡ç®—å›¾ï¼Œä¾‹å¦‚åœ¨ TVM ä¸­é€šå¸¸å°±ä¼šä½¿ç”¨ Relax IR æ¥å¿«é€Ÿæ­å»ºç½‘ç»œã€‚ä¹Ÿå°±æ˜¯è¯´æˆ‘ä»¬å¯ä»¥ç›´æ¥èˆå¼ƒä» ONNX/pytorch è½¬ Relax/RelayIR çš„è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´æ¥ä½¿ç”¨ IR æ¥æ­å»ºç½‘ç»œ
   3. æ›´é«˜æŠ½è±¡çš„ IR æœ‰åˆ©äºåœ¨å›¾èåˆå±‚é¢è¿›è¡Œä¼˜åŒ–
   4. æ›´ä½æŠ½è±¡çš„ IR æœ‰åˆ©äºè®¾è®¡æ›´å¤šè¿ç®—ç»†èŠ‚ï¼Œä¾‹å¦‚é¢å‘ä¸åŒçš„ç¡¬ä»¶è®¾è®¡ä¸åŒçš„å¹¶è¡Œæ–¹å¼

## -----


- 

- å¦‚ä½•ä½¿ç”¨ Git-LFS (Large File System)

   git-lfs æ˜¯ç”¨äºç®¡ç† repo ä¸­çš„å¤§å‹æ–‡ä»¶ã€‚å®‰è£…æ–¹å¼ [link](https://packagecloud.io/github/git-lfs/install)

   ```shell
   curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash
   apt install git-lfs
   ```

   åœ¨ huggingface ä¸­ clone ä¸€ä¸ªæ¨¡å‹é€šå¸¸ä¼šå…ˆè¿è¡Œ `git lfs install`ï¼Œè¿™ä¸€æ­¥å°±ä¼šå¯¹ git è¿›è¡Œå…¨å±€çš„è®¾ç½®ï¼Œèƒ½å¤Ÿåœ¨ä½¿ç”¨ `git clone` çš„æ—¶å€™ä¸‹è½½ä»“åº“ä¸­çš„ LFS æ–‡ä»¶ï¼ˆä¸€èˆ¬ä¸ºæ¨¡å‹æƒé‡ï¼‰

   ```shell
   git lfs install
   git clone https://huggingface.co/openai-community/gpt2
   # If you want to clone without large files - just their pointers
   GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/openai-community/gpt2
   ```

   è¿™é‡Œçš„è®¾ç½®æ˜¯å…¨å±€çš„ï¼Œåªç”¨è®¾ç½®ä¸€æ¬¡ã€‚ä½†æ˜¯æœ‰æ—¶å€™ä»“åº“ä¸­æœ‰å¾ˆå¤š LFS æ–‡ä»¶ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„æ–‡ä»¶éƒ½æ˜¯æˆ‘ä»¬éœ€è¦çš„ï¼Œæ‰€ä»¥å¯ä»¥ä½¿ç”¨å¦‚ä¸‹æ–¹æ³•

   ```shell
   # skip lfs globally
   git lfs install --skip-smudge
   # clone repo first
   git clone https://huggingface.co/openai-community/gpt2
   
   # pull certain file you need
   git lfs pull --include "model.safetensors"
   # pull all lfs
   git lfs pull
   ```

   æˆ‘ä»¬å…ˆ clone ä»“åº“ï¼Œç„¶åå†è¿›å…¥ä»“åº“ï¼Œå¯¹è¯¥ä»“åº“è¿›è¡Œå•ç‹¬çš„é…ç½®ï¼Œæœ€åä½¿ç”¨ `git lfs pull` å•ç‹¬ä¸‹è½½ LFS æ–‡ä»¶

   å¯ä»¥é€šè¿‡ `git lfs uninstall` æ¥å–æ¶ˆé…ç½®

- Models and model lib

   æƒ³è¦ä½¿ç”¨ mlc-llm è¿è¡Œä¸€ä¸ª chat model éœ€è¦ä¸¤ä»¶äº‹æƒ…ï¼šç¬¦åˆ mlc è¦æ±‚çš„æ¨¡å‹æƒé‡å’Œæ¨¡å‹åº“ï¼ˆmodel weights and model libraryï¼‰

   è·å–é€”å¾„æœ‰ä¸¤ä¸ª

   1. ä½¿ç”¨ mlc-llm å·²ç»å‡†å¤‡å¥½çš„æ¨¡å‹æƒé‡ [model cards hf](https://huggingface.co/mlc-ai)ï¼Œæ¨¡å‹åº“ [binary-mlc-llm-libs](https://github.com/mlc-ai/binary-mlc-llm-libs)

      ```python
      # Download pre-conveted weights
      git lfs install && mkdir dist/
      git clone https://huggingface.co/mlc-ai/Llama-2-7b-chat-hf-q4f16_1-MLC \
                                         dist/Llama-2-7b-chat-hf-q4f16_1-MLC
      
      # Download pre-compiled model library
      git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt_libs
      ```

   2. è‡ªå·±ç¼–è¯‘æ¨¡å‹æƒé‡å’Œæ¨¡å‹åº“ [convert model weights via mlc](https://llm.mlc.ai/docs/compilation/convert_weights.html)ï¼Œ[compile model libraries](https://llm.mlc.ai/docs/compilation/compile_models.html)

- ä½¿ç”¨ Python API è¿è¡Œ chat model

- é…ç½® MLCChatï¼Œmlc-llm æä¾›ä¸¤ä¸ª dataclass æ¥è®¾å®šé…ç½®

- **Convert Model Weights** and **Compile Model Library**

   pre-request: tvm unity compiler & mlc_chat

   ç›´æ¥ä» huggingface ä¸Šæ‹‰å–æ¨¡å‹ï¼Œç„¶åä½¿ç”¨å‘½ä»¤è¡Œå·¥å…·è½¬æ¢

   ```python
   # Create directory
   mkdir -p dist/models && cd dist/models
   # Clone HF weights
   git lfs install
   git clone https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-3B-v1
   cd ../..
   # Convert weight
   mlc_chat convert_weight ./dist/models/RedPajama-INCITE-Instruct-3B-v1/ \
       --quantization q4f16_1 \
       -o dist/RedPajama-INCITE-Instruct-3B-v1-q4f16_1-MLC
   ```

   æ•™ç¨‹è¿˜è®©æˆ‘ä»¬ç”Ÿæˆ MLC Chat Configï¼Œä¸ºä¹‹åç”Ÿæˆ model libraries æä¾›ä¸€äº›ä¿¡æ¯

   ```shell
   mlc_chat compile ./dist/RedPajama-INCITE-Chat-3B-v1-q4f16_1-MLC/mlc-chat-config.json \
       --device cuda -o dist/libs/RedPajama-INCITE-Chat-3B-v1-q4f16_1-cuda.so
   ```

## llm_chat.cc

ç†æ¸… llm_chat.cc çš„è°ƒç”¨é€»è¾‘ï¼Œç†æ¸…ä¹‹åéœ€è¦éªŒè¯ä¸¤ä»¶äº‹æƒ…ï¼š

1. batch verify æ˜¯å¦ç¬¦åˆæˆ‘ä»¬çš„æœŸæœ›
2. å¦‚ä½• pop kv cache

### Concept

- Reload ä¼¼ä¹ä¸æ˜¯ä¸€ä¸ªå¸¸ç”¨çš„å‡½æ•°ï¼Œåªä¼šåœ¨åˆå§‹åŒ– `ChatModule` çš„æ—¶å€™ä½¿ç”¨ï¼Œreset chat å€’æ˜¯åœ¨ä¹‹åé‡å¯ä¼šè¯ä¼šç»å¸¸ç”¨åˆ°

- Init & Init functions åŠŸèƒ½

- `FunctionTable`

  æ˜¯ä¸€ä¸ªå·¨å¤§çš„ struct ç»“æ„ä½“ï¼Œä½¿ç”¨äº† `_InitFunctions & _TryInitKVState` ä¸¤ä¸ªæ–¹æ³•ï¼Œå°† python ä¸­ç”¨ TIR æ‰€å†™çš„ä¸€äº›å¼ é‡å‡½æ•°ä»¥åŠä¸€äº›åœ¨ tvm é‡Œé¢„å…ˆå®šä¹‰å¥½çš„ global function æ”¾åˆ°ä¸€å—

  **åœ¨ä¹‹åä½¿ç”¨ `ft_.xx_funtion` ç›´æ¥è°ƒç”¨è¿™äº›å‡½æ•°**

  **è¿™ä¸€å—éš¾é“ä¸åº”è¯¥å•ç‹¬æ”¾ä¸€ä¸ªæ–‡ä»¶å—ï¼Ÿæ¨¡å—åŒ–çš„è¡¨è¾¾æˆ–è®¸ä¼šæ›´å¥½ä¸€äº›**

  å¹¶ä¸” mlc_llm çš„åŒå­¦å–œæ¬¢ä½¿ç”¨ `_` ä¸‹åˆ’çº¿æ¥æ ‡å¿—è¯¥å˜é‡æ˜¯ä¸€ä¸ªç±»æˆå‘˜ï¼Œè€Œä¸å–œæ¬¢ä½¿ç”¨ `this->` æ¥è¡¨æ˜

- `class LLMChat`

  è¿™ä¸ªå°±æ˜¯è¯¥æ–‡ä»¶çš„æœ€é‡è¦æœ€æ ¸å¿ƒçš„ä¸»ç±»

  åœ¨ç±»çš„æœ«å°¾å®šä¹‰äº†ä¸€å †æˆå‘˜ï¼Œå¯ä»¥åœ¨ç±»çš„æ–¹æ³•ä¸­ç›´æ¥ä½¿ç”¨ï¼ŒåŒ…å«ä¸Šé¢æåˆ°çš„ `FunctionTable`ï¼Œå¦‚æœçœ‹åˆ°å•¥ä¹Ÿæ²¡æœ‰çš„åœ°æ–¹

  - `GetInputTokens` è·å¾— tokens, vector of ints

  - `GetInputTokenNDArray` é€šè¿‡ vector of intsï¼Œè·å¾—ä¸€ä¸ª NDArray tensor

  - `PrepareBeforeEmbedding`ï¼Œreset chatï¼Œappend message to conversationsï¼Œæœ€åè°ƒç”¨ `GetInputTokens` è·å¾— token ids (vector of ints)

  - `EmbedStep`ï¼Œä½¿ç”¨ `PrepareBeforeEmbedding` è·å¾— token idsï¼Œç„¶åä½¿ç”¨ `GetInputTokenNDArray` è½¬æ¢æˆ NDArrayï¼Œæœ€åä½¿ç”¨ `ft_.embed_func_` æ¥è¿›è¡Œè¯åµŒå…¥

    **Shape: (B, N, C)**, batch åº”è¯¥åªèƒ½æ˜¯ 1

    è¿™é‡Œè¿˜ä½¿ç”¨ `auto tend = std::chrono::high_resolution_clock::now();` è¿›è¡Œäº†è®¡æ—¶ï¼Œè®¡ç®— embedding time

  - `ForwardEmbeddings`

    `Downcast` æ˜¯ç”¨äºç±»å‹è½¬æ¢ `Downcast<NDArray>` å°±æ˜¯å°†è¿”å›ç±»å‹è½¬ä¸º NDArray ç±»å‹

    è¿™é‡Œç›´æ¥è°ƒç”¨äº† `ft_.prefill_with_embed_func_` å®Œæˆ prefill æ“ä½œï¼Œè€Œ `ft_.prefill_with_embed_func_` å®é™…ä¸Šåœ¨æ‰€æœ‰çš„æ¨¡å‹ä¸­éƒ½æ²¡æœ‰è¢«å®šä¹‰

  - `ForwardTokens`

    **è¿™æ˜¯å‰å‘çš„æ ¸å¿ƒè°ƒç”¨æ–¹æ³•**

    è¯¥æ–¹æ³•ä½¿ç”¨ä¸€ä¸ª if åˆ¤æ–­ï¼Œåˆ†åˆ«è°ƒç”¨ prefill å’Œ decode

    ```c++
    if (input_tokens.size() > 1 &&ft_.prefill_func_defined())
    ```

    æ ¸å¿ƒä»£ç 

    ```c++
    IntTuple seq_ids_tuple({0});
    // get input shape
    ShapeTuple input_len_shape{input_len};
    
    // prepare kvcache
    ft_.kv_cache_begin_forward_func_(kv_cache_, seq_ids_tuple, input_len_shape);
    
    // reshape input data
    input_data = ft_.nd_view_func_(input_data, input_len_shape);
    
    // embed function
    auto embed = ft_.embed_func_(input_data, params_);
    
    // reshape embed shape
    ShapeTuple embedding_shape = {1, input_len, GetHiddenSizeFromEmbedding(embed)};
    embed = ft_.nd_view_func_(embed, embedding_shape);
    
    // prefill or decode
    ret = ft_.prefill_func_(embed, kv_cache_, params_); // ret = ft_.decode_func_(embed, kv_cache_, params_)
    
    // end kv cache
    ft_.kv_cache_end_forward_func_(kv_cache_);
    ```

    åœ¨å…¶ä¸­è°ƒç”¨äº† kv cache ç›¸å…³çš„æ–¹æ³•ï¼Œè¿™é‡Œä¹Ÿæ€»ç»“ä¸€ä¸‹ä»–ä»¬çš„ä½œç”¨ï¼š

    1. `ft_.kv_cache_begin_forward_func_` å®é™…ä¸Šè°ƒç”¨çš„æ˜¯ `rnn_state.cc` ä¸­çš„ `BeginForward` æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¼šæ›´æ–°ä¸‰ä¸ªç±»æˆå‘˜

       1. `cur_batch_size = seq_ids.size()`
       2. `cur_append_lengths = append_lengths`
       3. `cur_seq_ids = seq_ids`

       é€šå¸¸æ¥è¯´ï¼Œæˆ‘ä»¬ä¼šå›ºå®š batch size ä¸º 1ï¼Œæ‰€ä»¥é‡ç‚¹å°±æ˜¯æ›´æ–°äº† `append_lengths` è¿™ä¸ªæ–¹æ³•

    2.  `ft_.kv_cache_end_forward_func_`ï¼Œè¯¥æ–¹æ³•æ˜¯è°ƒç”¨çš„ `paged_kv_cache.cc` ä¸­çš„ `EndForward` æ–¹æ³•ã€‚è¯¥æ–¹æ³•ä¼šæ›´æ–°å¯¹åº” `seq_id` çš„ `seq_length` æŒ‡é’ˆ

       ```c++
       auto it = seq_map_.find(seq_id);
       it->second.seq_length += seq_length;
       ```

       é™¤æ­¤ä¹‹å¤–è¿˜è°ƒæ•´äº† `available_history_num`

       ```c++
       if (seq_length > 1) {
       // We cannot rollback the prefill input
       it->second.available_history_num = 0;
       } else {
       it->second.available_history_num =
           std::min(it->second.available_history_num + 1, max_history_ - 1);
       }
       ```

       è¿™é‡Œè®¤ä¸º `seq_length > 1` å°±æ˜¯åœ¨è¿›è¡Œ `prefill`ï¼Œæ‰€ä»¥ä¼šç›´æ¥è®¾ç½® `available_history_num` ä¸º 0ï¼Œè¯¥è®¾ç½®ä¼šé˜»æ­¢ `PopN` æ–¹æ³•æ¥å¯¹ kv cache çš„ä½ç½®è¿›è¡Œå›é€€

  - `PrefillWithEmbedStep`

    è¿™ä¸ªå‡½æ•°æ˜æ˜¾æ˜¯å¸Œæœ›è°ƒç”¨ `ForwardEmbeddings` æ¥å®Œæˆ prefillï¼Œä½†æ˜¯æˆ‘ä»¬æ‰€å®šä¹‰çš„æ¨¡å‹é‡Œé¢å¹¶æ²¡æœ‰ `prefill_with_embed` çš„æ¥å£ï¼Œæ‰€ä»¥è¿™ä¸ªå‡½æ•°åº”è¯¥æ˜¯ä¸ä¼šè¢«ç”¨åˆ°çš„

- tvm python å’Œ C++ çš„ç›¸äº’è°ƒç”¨

  è¿™æ˜¯ç†è§£å¦‚ä½•ä½¿ç”¨ tvm çš„å…³é”®ä¸€æ­¥ï¼Œç”±äº python å’Œ C++ ä¹‹é—´èƒ½å¤Ÿç›¸äº’è°ƒç”¨ï¼Œæ‰€ä»¥åœ¨çœ‹ä»£ç çš„æ—¶å€™ä¼šå˜å¾—éå¸¸æ··ä¹±

  è¿™é‡Œé€šè¿‡ 4 ä¸ªæ–‡ä»¶æ¥å®Œæˆè¿™ä¸ªè¿‡ç¨‹çš„ç†è§£

  1. in mlc `kv_cache.py`
  2. in tvm `paged_kv_cache.cc`
  3. in tvm `kv_state.cc`
  4. in mlc `llm_chat.cc`

  æœ€å…ˆæ¥è§¦åˆ°çš„å³ä¸º `kv_cache.py` ä¸­å¯¹äº `PagedKVCache` çš„ python å®šä¹‰ï¼Œæˆ‘ä»¬å°±ä»¥æ­¤ä¸ºèµ·ç‚¹ï¼Œè¯´æ˜å¦‚æœåˆ›å»ºä¸€ä¸ª kv cacheï¼Œå¹¶ä¸”å¦‚ä½•å®Œæˆå¯¹ kv cache çš„æ“ä½œ

  åœ¨ **`kv_cache.py`** ä¸­å®Œæˆäº†åˆ›å»º kv cache çš„æ“ä½œï¼Œä½¿ç”¨çš„æ˜¯è°ƒç”¨ C++ `vm.builtin.paged_attention_kv_cache_create_reduced`ï¼Œè¯¥å‡½æ•°åœ¨ **`paged_kv_cache.cc`** ä¸­åº•éƒ¨ã€‚è°ƒç”¨è¯¥æ–¹æ³•æ‰€ä¼ å…¥çš„å‚æ•°éƒ½æ˜¯ä½¿ç”¨ python æ„å»ºçš„ TIR functionï¼Œè¿™äº› function å°†æˆä¸º C++ ä¸­çš„ `PagedAttentionKVCacheObj` çš„ `PackedFunc` æˆå‘˜

  ä½†è¿™äº› kv cache packed function éƒ½æ˜¯ä¸ºäº†åœ¨ kv cache å†…éƒ¨ä½¿ç”¨ï¼Œè€Œä¸ä¼šæš´éœ²ç»™å¤–éƒ¨å‡½æ•°ã€‚çœŸæ­£æš´éœ²ç»™å¤–éƒ¨ä½¿ç”¨çš„æ¥å£ä¸º `PagedAttentionKVCacheObj` ä¸­æˆå‘˜å‡½æ•° `BeginForward & EndForward AttentionWithFusedQKV` ç­‰ç­‰

  è¿™äº›æ ¸å¿ƒæˆå‘˜å‡½æ•°é€šè¿‡ **`kv_state.cc`** ä¸­çš„**æ³¨å†Œæœºåˆ¶** `TVM_REGISTER_GLOBAL & set_body_typed & set_body_method` å®Œæˆæ³¨å†Œã€‚æ³¨å†Œå®Œæˆè¿‡åå°±èƒ½å¤Ÿè¢« C++ å’Œ python ä»»æ„è°ƒç”¨ã€‚å¦‚æœä½¿ç”¨ `set_body_method` æ–¹æ³•æ³¨å†Œï¼Œåˆ™åœ¨è°ƒç”¨çš„æ—¶å€™éœ€è¦ä¼ å…¥ kv cache object æœ¬èº«ï¼Œä¸ç„¶ä½ å¯¹è±¡éƒ½æ²¡åˆ›å»ºï¼Œæ€ä¹ˆè°ƒç”¨å…¶æ–¹æ³•ï¼Ÿè€Œä½¿ç”¨ `set_body_typed` æ–¹æ³•åˆ™æ˜¯ç›´æ¥ä½¿ç”¨ lambda å‡½æ•°å°† kv cache object ç›´æ¥æ˜¾å¼ä½œä¸ºå‚æ•°ä¼ å…¥

  æœ€åè¿™äº›æ³¨å†Œå®Œçš„å‡½æ•°å°†åœ¨ **`llm_chat.cc`** ä¸­ä½¿ç”¨ï¼Œä½¿ç”¨æ–¹å¼æ˜¯å°†å…¶å­˜å‚¨åœ¨ `FunctionTable` ä¸­

  ![image-20240514211503384](MLC-LLM Usage/image-20240514211503384.png)

  ç®€è€Œè¨€ä¹‹ï¼š

  1. `set_body_method` éœ€è¦ä¼ å…¥å¯¹è±¡ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°
  2. `set_body_typed` é‡‡ç”¨ lambda å‡½æ•°æ˜¾å¼ä¼ å…¥å‚æ•°
  3. `TVM_REGISTER_GLOBAL` æ³¨å†Œçš„å‡½æ•°èƒ½å¤Ÿè¢« python å’Œ C++ ä»»æ„è°ƒç”¨
  4. python TIR function in KV Cache is not meant to be used by users, but to build APIs. The APIs are actually something you want to use

  å†ä»æ–‡ä»¶çš„è§’åº¦æ¥åˆ†æ

  1. in mlc `kv_cache.py`

     å®šä¹‰åŸºç¡€ TIR Function

  2. in tvm `paged_kv_cache.cc`

     æ„å»º KV Cache class & API

  3. in tvm `kv_state.cc`

     æ³¨å†Œ API

  4. in mlc `llm_chat.cc`

     ä½¿ç”¨ API

- `picojson` èƒ½å¤Ÿå¤„ç† json æ–‡ä»¶ï¼Œé€šè¿‡ key æ¥è·å¾—å…¶ä¸­ valueï¼Œç„¶åä½¿ç”¨ `.get<type>` æ¥å¯¹ value è¿›è¡Œè½¬æ¢

- `conversation` ç”¨äºå­˜å‚¨å¯¹è¯å’Œç”Ÿæˆçš„ token stringã€‚åœ¨ huggingface transformers ä¸­ `add_bos_token` å¯ä»¥è®¾ç½®åœ¨ tokenizer é‡Œé¢ï¼Œä½†æ˜¯åœ¨ mlc_llm é‡Œæ˜¯è®¾ç½®åœ¨ `conversation` é‡Œé¢ã€‚ä½†æ˜¯åœ¨æ–°ç‰ˆçš„ conversation é‡Œé¢åˆæ²¡æœ‰è®¾ç½® add bos token çš„é€‰é¡¹äº†

## Install from source

ç”±äºè‡ªå·±éœ€è¦å¯¹ C++ æ–‡ä»¶åšä¸€äº›ä¿®æ”¹ï¼Œæ‰€ä»¥éœ€è¦ä»æºç è¿›è¡Œç¼–è¯‘

å¯ä»¥ä½¿ç”¨ pip æ¥å®‰è£… tvmï¼Œç„¶åå†æŒ‰ç…§æºç ç¼–è¯‘çš„æ–¹å¼å®‰è£… `mlc_llm`

```shell
# clone from GitHub
git clone --recursive https://github.com/mlc-ai/mlc-llm.git && cd mlc-llm/
# create build directory
mkdir -p build && cd build

# generate build configuration
## choose only cuda related to be true
## ROCm Vulkan Metal OpenCL to be false
python ../cmake/gen_cmake_config.py

# build mlc_llm libraries
cmake .. && cmake --build . --parallel $(nproc) && cd ..
```

mlc_llm å’Œ tvm çš„ç‰ˆæœ¬å…³ç³»æ˜¯å¼ºç»‘å®šçš„ï¼Œæœ€å¥½éƒ½ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ï¼æˆ–è€…éƒ½ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬çš„ build from source

å¯ä»¥å°è¯•å…³é—­ flash infer çš„ç¼–è¯‘ï¼Œå› ä¸ºè¿™ä¸ªç¼–è¯‘æ‰€ä½¿ç”¨çš„æ—¶é—´å¾ˆé•¿ï¼ŒåŒ…ä¹Ÿæ¯”è¾ƒé‡

## Question

- mlc-llm ä¼¼ä¹æ²¡æœ‰ tvm çš„ auto tune åŠŸèƒ½ï¼Œè€Œæ˜¯é€‰æ‹©ä½¿ç”¨æ‰‹å·¥å®ç° tirï¼Œä¼¼ä¹åˆå›åˆ°äº†æ‰‹å·¥è®¾è®¡ç®—å­çš„æ—¶ä»£ [[Question] performance optimization](https://github.com/mlc-ai/mlc-llm/issues/1800)ã€‚ç°åœ¨ AutoTune è¿™ä¸ªåŠŸèƒ½å·²ç»ä¸æ˜¯å…¶æœ€å¤§çš„å–ç‚¹äº†ï¼
- mlc container on Orin [github](https://github.com/dusty-nv/jetson-containers/tree/dev/packages/llm/mlc)
- tvm ndarray ä½¿ç”¨æ–¹æ³•
- tvm build & function
- æˆ‘å¸Œæœ›ç»™ attention ä¼ å…¥å®šåˆ¶çš„ maskï¼Œå¦‚ä½•å®Œæˆè¯¥æ“ä½œï¼Ÿ
