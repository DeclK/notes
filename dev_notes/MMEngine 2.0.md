# MMEngine 2.0

ç³»ç»Ÿåœ°å¯¹ mmengine è¿›è¡Œæ€»ç»“ï¼ŒæŒ‰ç…§ç›®å‰çš„ç¬”è®°é£æ ¼ï¼Œå¯¹æ¦‚å¿µè¿›è¡Œæ•´ç†

## Registry

åœ¨ mmengine ä¸­ Registry ä»ç„¶æ˜¯ä¸€ä¸ªå¾ˆé‡çš„è®¾è®¡ï¼Œé€»è¾‘é“¾æ¡å¤æ‚ã€‚è¿™ä¸ªè®¾è®¡çš„ä¼˜ç‚¹åœ¨äºï¼šèƒ½å¤Ÿæ–¹ä¾¿åœ°ä½¿ç”¨é…ç½®æ–‡ä»¶æ„å»ºæ‰€éœ€çš„å®ä¾‹ã€‚ä½†æ˜¯**ç¼ºç‚¹ä¹Ÿéå¸¸æ˜æ˜¾**ï¼š

1. æŸå®³äº†ä»£ç çš„æ˜“è¯»æ€§ï¼Œæ— æ³•é“¾æ¥åˆ°å¯¹è±¡ï¼Œä»£ç éƒ½æ‰¾ä¸åˆ°åœ¨å“ªå„¿ğŸ§
2. åœ¨è·¨é¡¹ç›®ä½¿ç”¨çš„æ—¶å€™å­˜åœ¨åˆ‡æ¢ scope çš„é—®é¢˜
3. Registry æœ¬èº«çš„ä»£ç é€»è¾‘ä¸å¥½è¯»
4. ç”±äºæ³¨å†Œé€»è¾‘çš„å­˜åœ¨ï¼Œåœ¨ build çš„æ—¶å€™å¢åŠ äº†ä»£ç å¤æ‚ç¨‹åº¦

æ‰€ä»¥ä½¿ç”¨çš„æœ€ä½³æ–¹å¼ä¸ºï¼šåœ¨ config ä¸­æŒ‡å®š scope åç§°ä¸ºé¡¹ç›®åŒ…åç§°ï¼ˆå¯ä»¥ importï¼‰ï¼Œç„¶ååœ¨é¡¹ç›®ä¸­æ–°å»ºä¸€ä¸ª `registry.py`ï¼Œé€šè¿‡ `locations` æ¥æ³¨å†Œæ¨¡å—

```python
from mmengine.registry import DATASETS as MMENGINE_DATASETS

DATASETS = Registry(
    'dataset',
    parent=MMENGINE_DATASETS,
    locations=['PROJECT.datasets'])
```

è¿™é‡Œå¿…é¡»è¦åŠ å…¥ `parent & locations` å‚æ•°ï¼Œæ‰èƒ½æˆåŠŸæ³¨å†Œã€‚åŸå› æ˜¯åœ¨ Runner ä¸­ä½¿ç”¨çš„æ˜¯ mmengine çš„æ ¹æ³¨å†Œå™¨ï¼Œåœ¨è¿™é‡ŒåŠ å…¥ parentï¼Œæ‰èƒ½å°†æˆ‘ä»¬çš„æ³¨å†Œå™¨é“¾æ¥åˆ°æ ¹æ³¨å†Œå™¨ä¸­ï¼Œé“¾æ¥æ–¹å¼ä¸º `import_module(scope.registry)`ã€‚è€ŒåŠ å…¥ locations ä½¿å¾—æ³¨å†Œå™¨é€šè¿‡ `import_from_location` æ–¹æ³•ï¼Œå°†æ¨¡å—çœŸæ­£åœ°æ³¨å†Œè¿›å»

ä¹‹åï¼Œæ— è®ºä½ æ˜¯ä½¿ç”¨ `from mmengine.registry import xxx` è¿˜æ˜¯ä½¿ç”¨ `from PROJECT.registry import xxx`ï¼Œä½  PROJECT ä¸­æ³¨å†Œçš„æ¨¡å—ä¸€å®šèƒ½è¢«æ‰¾åˆ°

## Config

åœ¨ä¹‹å‰çš„ç¬”è®°ä¸­å·²ç»æ€»ç»“å¾—éå¸¸å¥½äº†ï¼Œç¼ºç‚¹ä»ç„¶æ˜¯ä»£ç çš„æ˜“è¯»æ€§ä¸å¤Ÿå¥½ã€‚åœ¨ç»§æ‰¿ä»£ç é…ç½®çš„æ—¶å€™ï¼Œä¹Ÿå¾ˆéš¾é€šè¿‡è·³è½¬çš„æ–¹å¼å¾—åˆ°åŸºæœ¬é…ç½®æ–‡ä»¶

## Dataset

ä¸€èˆ¬æ¥è¯´éœ€è¦è‡ªå·±é‡æ„ï¼Œä¸éœ€è¦åšè¿‡å¤šçš„å®šä¹‰ã€‚å¯¹äº mmengine æ¥è¯´ï¼Œè®¾è®¡äº†å¦‚ä¸‹ç»“æ„ï¼š

1. meta infoï¼Œç”¨äºä¿å­˜æ•°æ®é›†çš„é¢å¤–ä¿¡æ¯ï¼Œä¾‹å¦‚æ•°æ®é›†åç§°ã€æ•°æ®é›†ç±»åˆ«
2. lazy initï¼Œåœ¨åˆ›å»º dataset class æ—¶ä¸å¿…å°†æ‰€æœ‰çš„ data list è½½å…¥
3. `load_data_list`ï¼Œè·å¾—æ•´ä¸ª data list
4. `get_data_info`ï¼Œæ˜¯é€šè¿‡ idx è·å¾— data list item çš„æ ·æ¿å‡½æ•°ï¼Œå…¶ä¸­å®ç°äº† `serialize_data` é€»è¾‘ï¼Œä½œç”¨æ˜¯è®©å¤šä¸ª dataloader worker å…±äº«å†…å­˜ç©ºé—´æ¥ [Save memory](https://mmengine.readthedocs.io/en/latest/advanced_tutorials/basedataset.html#save-memory)
5. `prepare_data`ï¼Œæ˜¯é€šè¿‡ idx è·å¾— input data çš„æ ·æ¿å‡½æ•°ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­è°ƒç”¨ `get_data_info` è·å¾— data list itemï¼Œç„¶åä½¿ç”¨ `pipeline` è·å¾— input data ä»¥åŠè¿›è¡Œæ•°æ®å¢å¼º

å…¶ä¸­ï¼Œæ‰€è°“çš„ `pipeline` å°±æ˜¯ä¸€ä¸ª `Compose` ç±»ï¼Œå…¶åŒ…å«äº†ä¸€ç³»åˆ— TRANSFORM ç±»ï¼Œè¿™äº›ç±»å®šä¹‰å¥½äº†å‰å‘æ–¹æ³• `__call__(data_item)`ï¼Œå½“ data item é¡ºåºé€šè¿‡è¿™äº›ç±»åï¼Œè·å¾—æœ€ç»ˆçš„ input data

## ManagerMixin

ManagerMixin èƒ½å¤Ÿå¯¹ç±»çš„å®ä¾‹è¿›è¡Œç®¡ç†ï¼Œä»è€Œèƒ½å¤Ÿä»å…¨å±€ä½¿ç”¨åŒä¸€ä¸ªå…±äº«å¯¹è±¡ã€‚å…¶å®ç°é€šè¿‡è®¾ç½®å…ƒç±» `metaclass=ManagerMeta` å®Œæˆ

1. `ManagerMeta` è®¾ç½®äº†ä¸€ä¸ª `cls._instance_dict` æ¥ä¿å­˜ç±»çš„å®ä¾‹ï¼Œå¹¶ä¸”è§„å®šç±»çš„`__init__` å‚æ•°å¿…é¡»åŒ…å« `name`ã€‚è¿™ä¿è¯æ¯ä¸€ä¸ªç±»çš„å¯¹è±¡éƒ½æœ‰ä¸€ä¸ªå¯¹åº”çš„ name ä½œä¸ºå…¶æ ‡è¯†ç¬¦
2. `ManagerMixin` å®ç°äº†ä¸¤ä¸ªæ–¹æ³•ï¼š`get_instance & get_current_instance`ï¼Œä»–ä»¬é€šè¿‡ name æ¥è·å¾—ç±»ä¸­çš„å®ä¾‹ã€‚å…¶ä¸­ `get_instance` ä¼šåˆ›å»ºå¯¹è±¡ï¼Œå¦‚æœå¯¹è±¡ä¹‹å‰æ²¡è¢«åˆ›å»ºè¿‡

## MessageHub & Logger

è¿™ä¸¤è€…éƒ½æ˜¯åŸºäº ManagerMixin çš„ç±»ï¼Œæ‰€ä»¥å…¨å±€éƒ½æ˜¯ä½¿ç”¨çš„åŒä¸€ä¸ªå®ä¾‹

**Message hub** ç”¨äºå­˜å‚¨ä¸¤ä¸ªéƒ¨åˆ†ï¼š

1. log informationï¼Œä¸»è¦ä¸º lr, loss è¿™ç§è®­ç»ƒæ—¶äº§ç”Ÿæ—¥å¿—ï¼Œä¿å­˜åœ¨ `self._log_scalars: Dict[name, HistoryBuffer]`ã€‚è¦å­˜å‚¨è¿™äº›ä¿¡æ¯ï¼Œè¿˜ä½¿ç”¨äº† `HistoryBuffer` æ¥å­˜å‚¨ï¼Œå®é™…ä¸Šè¿™ä¸ª buffer å°±æ˜¯ä¸€ä¸ªå¾ˆé•¿çš„ arrayï¼Œå¯ä»¥ä¸æ–­åœ°æ›´æ–°æ—¥å¿—ï¼ˆarray å˜é•¿ï¼‰ï¼Œæ›´æ–°æ–¹å¼ä¸ºï¼š

   - `message_hub.update_scalar('train/loss', loss)`
   - `message_hub.update_scalars(log_dict)`

   è¿™äº›ä¿¡æ¯ä¼šç”¨äºè¾“å‡ºæ—¥å¿—ï¼Œä»¥åŠ visualzation backend `TensorboardVisBackbend`ã€‚mmengine å¯¹ visualizer ç»Ÿä¸€äº†æ¥å£ï¼Œä½¿ç”¨ `add_scalars` æ¥å‘è¿™äº›å¯è§†åŒ–åç«¯æ·»åŠ è®°å½•

2. runtime information, ä¸»è¦æ˜¯ iter times, meta information ç­‰ä¿¡æ¯ï¼Œä¿å­˜åœ¨ `self._runtime_info[name, value]`ä¸ä½¿ç”¨ `HistoryBuffer` å­˜å‚¨

**MMLogger** çš„ä½¿ç”¨éå¸¸ç®€å•ï¼Œåœ¨ä»£ç ä¸­å¯ä»¥ç›´æ¥è·å¾— logger

```python
from mmengine.logging import MMLogger 

# get instance
logger = MMLogger.get_instance(name='mmengine')
# or get current instance
logger = MMLogger.get_current_instance()
# log
logger.info(log_string)
```

ä¹Ÿå¯ä»¥ä½¿ç”¨æ¥å£ `print_log`

```python
from mmengine.logging import print_log

print_log(msg, logger='current')
```

## Checkpoint & WeightInit

checkpoint å’Œæƒé‡åˆå§‹åŒ–æ˜¯éå¸¸ç›¸å…³çš„ï¼Œå½“ç„¶æƒé‡åˆå§‹åŒ–ä¹Ÿæœ‰å…¶ä»–çš„ç®€å•æƒ…å†µã€‚è¿™é‡Œæˆ‘å°†äºŒè€…æ”¾åœ¨ä¸€èµ·æ•´ç†

ä¸€èˆ¬è£…è½½ checkpoint åˆ†ä¸º2æ­¥ï¼š

1. ä»æ–‡ä»¶è¯»å– checkpoint åˆ°å†…å­˜å½“ä¸­
2. å°†è¯»å–çš„ checkpoint æ–‡ä»¶è£…è½½åˆ°æ¨¡å‹å½“ä¸­

è¿™ä¸¤éƒ¨åˆ†åˆ«å¯ç”±ï¼š`_load_checkpoint` ä»¥åŠ `_load_checkpoint_to_model` å®Œæˆ

ä¸ºäº†å®Œæˆç¬¬ä¸€æ­¥ï¼Œéœ€è¦åº”å¯¹ä¸åŒçš„æ–‡ä»¶ç±»å‹ï¼Œæœ‰çš„æ–‡ä»¶æ˜¯ä»ç½‘ä¸Šä¸‹è½½çš„ï¼Œæœ‰çš„æ–‡ä»¶æ˜¯ä»æœ¬åœ°è¯»å–çš„ã€‚mmengine ä½¿ç”¨äº† `CheckpointLoader` æ¥å¤„ç†ä¸åŒçš„æ–‡ä»¶ç±»å‹ï¼ŒåŸºæœ¬ä¸Šå¸¸ç”¨çš„å°±æ˜¯ `torch.load` ä»æœ¬åœ°è£…è½½

ä¸ºäº†å®Œæˆç¬¬äºŒæ­¥ï¼Œå®é™…ä¸Šå°±æ˜¯è°ƒç”¨äº† pytorch çš„ `module.load_state_dict` åªæ˜¯æ›´æ”¹äº†é»˜è®¤ `strict=False`ï¼Œå¹¶ä¸”åŠ å…¥äº† logger è®°å½•ä¸åŒ¹é…çš„æƒé‡

è€Œ `PretrainedInit` å®é™…ä¸Šå°±æ˜¯ä½¿ç”¨ `load_checkpoint` å®Œæˆçš„ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ª `_load_checkpoint_with_prefix` ç”¨äºè½½å…¥å›ºå®šå‰ç¼€çš„æƒé‡ï¼ˆkey prefix ä¹Ÿä¼šè¢«åˆ é™¤ï¼‰ï¼Œç„¶åå†ä½¿ç”¨ `load_state_dict` è£…è½½åˆ°æ¨¡å‹

## BaseModel

mmengine è®¾è®¡äº† `BaseModel`ï¼Œæ‰€æœ‰çš„æ¨¡å‹éƒ½åº”è¯¥ç»§æ‰¿è¿™ä¸ªç±»ï¼Œè¯¥ç±»è®¾è®¡äº†ä¸‰ä¸ªæ¥å£ï¼š

1. `train_step`
2. `val_step`
3. `test_step`

`BaseModel` ä¸ç›´æ¥ä½¿ç”¨ `forward` æ–¹æ³•ï¼Œè€Œæ˜¯åœ¨ `xxx_step` ä¸­è°ƒç”¨ï¼Œå› ä¸ºå„ä¸ª step ä¸­è¿˜åŒ…å«äº† data preproceeï¼ˆå°†æ•°æ®ç§»åŠ¨åˆ° cuda ä¸Šï¼‰ï¼Œä»¥åŠæ¨¡å‹å‚æ•°æ›´æ–°ã€‚æ‰€ä»¥æœ€å¥½æŠŠ `BaseModel` çœ‹ä½œå¯¹æ¨¡å‹çš„å°è£…ï¼Œæ˜¯ä¸€ä¸ªæµç¨‹ï¼Œè€Œä¸æ˜¯æ¨¡å‹æœ¬èº«

## ParamScheduler

mmengine è‡ªå·±å®ç°äº†ä¸€ä¸ªåŸºç¡€çš„ `_ParamScheduler` å¯ä»¥å¯¹ lr, momentum è¿›è¡Œè§„åˆ’ã€‚å®é™…ä¸Šåªå¯¹ lr è¿›è¡Œäº†è§„åˆ’ï¼Œå¾ˆå°‘æ¶‰åŠå¯¹ momentum è¿›è¡Œæ§åˆ¶

ç†è§£ `_ParamScheduler` æœ‰å‡ ä¸ªé‡è¦éƒ¨åˆ†ï¼š

1. schedule åœ¨ä½•æ—¶å¼€å§‹ã€ä½•æ—¶ç»“æŸã€‚è¯¥ç±»ä½¿ç”¨äº†3ä¸ªå±æ€§æ¥è¿›è¡Œæ§åˆ¶ï¼š`begin & end & global_step & last_step`ã€‚å½“ global step åœ¨ begin å’Œ end ä¹‹é—´æ—¶ï¼Œæ‰ä¼šè¿›è¡Œ step è¿­ä»£ï¼Œæ­¤æ—¶æ‰ä¼šæ›´æ–° `self.last_step & self.last_value`
2. schedule çš„æ›´æ–°å‘¨æœŸç”± `by_epoch: bool` å‚æ•°æ§åˆ¶
3. step æ˜¯ lr æ›´æ–°çš„æ ¸å¿ƒå‡½æ•°ã€‚é€šè¿‡ `self.get_value` è¿”å›æ¯ä¸€ä¸ª param group çš„æ–°å­¦ä¹ ç‡ã€‚æœ€åå°†æ›´æ–°çš„å­¦ä¹ ç‡ä¿å­˜ä¸º `self.last_value`ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ**æ›´æ–°æ–¹å¼æ˜¯ chainable çš„**ï¼Œå³æ ¹æ®ä¸Šä¸€æ¬¡çš„æ›´æ–°ï¼Œè¿­ä»£å‡ºå½“å‰çš„å­¦ä¹ ç‡ï¼Œè€Œä¸æ˜¯æ ¹æ® epoch or iteration çš„å€¼æ¥è®¡ç®—å½“å‰å­¦ä¹ ç‡æ˜¯å¤šå°‘ã€‚è¿™æ ·çš„å¥½å¤„æ˜¯ï¼šåœ¨å¤šä¸ª scheduler è¿æ¥çš„æ—¶å€™ï¼Œä¸ç”¨è€ƒè™‘è¿æ¥å€¼æ˜¯å¤šå°‘ï¼Œç›´æ¥è¿›è¡Œè¿­ä»£å³å¯

åœ¨ scheduler åˆå§‹åŒ–çš„æ—¶å€™ï¼Œä¼šåˆå§‹åŒ– `self.last_step = -1`ï¼Œç„¶åä¼šç«‹å³æ‰§è¡Œä¸€æ¬¡ stepï¼Œå¹¶ä¸”ä¿è¯äº† scheduler ä¸€å®šåœ¨ optimizer ä¹‹åæ›´æ–°

**å¦å¤–ï¼Œscheduler çš„å…·ä½“æ‰§è¡Œä»£ç åœ¨ hook å½“ä¸­ï¼Œè€Œä¸åœ¨ model or loop å½“ä¸­**

scheduler çš„ `state_dict`ï¼Œæ˜¯ç›´æ¥ä½¿ç”¨äº† `class.__dict__` æ¥ä¿å­˜ï¼Œä½†æ˜¯æ’é™¤äº† `optimizer`

## OptimizerWrapper

**ä¸ºä»€ä¹ˆè¦åšä¸€ä¸ª optimizer wrapperï¼š**

1. ç»Ÿä¸€æ¥å£ï¼Œèƒ½å¤Ÿåœ¨ base model ä¸­ä½¿ç”¨ `update_params`
2. å…·æœ‰æ‰©å±•æ€§ï¼Œå¯ä»¥ä½¿ç”¨æ¢¯åº¦è£å‰ªã€æ¢¯åº¦ç´¯è®¡ã€æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰ç­‰å¤æ‚æ“ä½œ

é¦–å…ˆ `OptimizerWrapper` ç»§æ‰¿äº†åŸºç±» `BaseOptimWrapper`ï¼Œè¿™ä¸ªç±»åˆ«çš„ä½œç”¨å°±æ˜¯ç»Ÿä¸€ wrapper å’ŒåŸå§‹ optimizer çš„æ¥å£ï¼ŒåŒ…æ‹¬ï¼š`step, param_groups` ç­‰ã€‚è€Œ `OptimizerWrapper` å°±å¯ä»¥å®ç°é«˜çº§åŠŸèƒ½ï¼Œä¾‹å¦‚ `update_params`

```python
    def update_params(
            self,
            loss: torch.Tensor,
            step_kwargs: Optional[Dict] = None,
            zero_kwargs: Optional[Dict] = None) -> None:
        loss = self.scale_loss(loss)
        self.backward(loss)
        if self.should_update():	# accumulation
            self.step(**step_kwargs)
            self.zero_grad(**zero_kwargs)
```

å®é™…ä¸Š optimizer çš„è¡Œä¸ºè¿˜è¦æ›´å¤æ‚ä¸€äº›ï¼Œä¾‹å¦‚ï¼šæˆ‘ä»¬éœ€è¦å¯¹ backbone é‡‡å–å•ç‹¬çš„å­¦ä¹ ç‡ï¼Œè¿™éœ€è¦æ¥æ”¶æ¨¡å‹å‚æ•°ï¼Œæ‰€ä»¥ä¸èƒ½ä½¿ç”¨ç®€å•çš„ `Registry` æ–¹å¼æ¥ç›´æ¥åˆ›å»º optimizer wrapperã€‚mmengine ä¸­ä½¿ç”¨äº†ä¸€ä¸ª `DefaultOptimWrapperConstructor` æ¥åˆ›å»º optimizer wrapperã€‚å¯ä»¥é€šè¿‡æ·»åŠ  `paramwise_cfg` æ¥æ§åˆ¶ä¸åŒç½‘ç»œå±‚çš„å­¦ä¹ ç‡

```python
optim_wrapper = dict(type='OptimWrapper', 
                     optimizer=dict(type='AdamW', lr=0.0001, weight_decay=0.01),
                     paramwise_cfg=dict(custom_keys={'layer_name': dict(lr_mult=0.1)}))
```

`paramwise_cfg` é€šè¿‡è®¾ç½® `custom_keys`ï¼Œå¯¹åŒ…å«æœ‰ `layer_name` çš„å‚æ•°å•ç‹¬å°†å…¶å­¦ä¹ ç‡ä¹˜ä»¥ 0.1ï¼Œå¹¶å°†**è¯¥å±‚å•ç‹¬**ä½œä¸ºä¸€ä¸ª group æ·»åŠ åˆ° param groups å½“ä¸­

## Evaluator

Evaluator çš„é€»è¾‘éå¸¸ç®€å•ï¼Œå…¶ä¿å­˜äº†ä¸€ç³»åˆ— metricsï¼Œè¿™äº› metrcis èƒ½å¤Ÿè¯„ä¼°é¢„æµ‹ç»“æœ

åœ¨ val loop è¿‡ç¨‹ä¸­ï¼Œè¾“å‡ºç»“æ„é¦–å…ˆé€šè¿‡ `Evaluator.process` æ–¹æ³•è¿›è¡Œåˆæ­¥å¤„ç†ï¼Œ`Evaluator.process` è°ƒç”¨çš„åˆ™æ˜¯ `metric.process`ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œæ‰“åŒ…ä¸€äº›æ‰€éœ€è¦çš„ç»“æœï¼Œä¿å­˜åœ¨ `metric.result` å½“ä¸­ã€‚ä¹‹åå°±è°ƒç”¨äº† `Evaluator.evaluate -> metric.evaluate` å°†ä¿å­˜åœ¨ `metric.result` ä¸­çš„ç»“æœè¿›è¡Œå¤„ç†ï¼Œè®¡ç®—æœ€ç»ˆæŒ‡æ ‡ã€‚

æ‰€ä»¥å…³é”®è°ƒç”¨å‡½æ•°ä¸º `metric.evaluate`ï¼Œè¿‡ç¨‹å¦‚ä¸‹ï¼š

1. ä½¿ç”¨ `collect_results` å¯¹ä¸åŒ rank çš„ç»“æœè¿›è¡Œæ•´åˆï¼Œå¹¶ä¸”å°†ç»“æœéƒ½æ”¾åœ¨äº† cpu ä¸Š
2. ä½¿ç”¨ `metric.compute_metrics` æ–¹æ³•åœ¨ä¸»è¿›ç¨‹ä¸­è®¡ç®—æœ€ç»ˆçš„æŒ‡æ ‡ï¼Œç„¶åå°†è®¡ç®—ç»“æœå¹¿æ’­åˆ°å…¶ä»–è¿›ç¨‹

æ‰€ä»¥åœ¨å®é™…ä½¿ç”¨è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åªéœ€å®ç° metric ç±»ï¼Œå¹¶ä¸”å®ç°å…³é”®çš„ä¸¤ä¸ªå‡½æ•°å³å¯ï¼š`process & compute_metrics`

å¹¶ä¸”åœ¨å®ç° process çš„è¿‡ç¨‹ä¸­ï¼Œ`self.results` ä¸­æ¯ä¸€ä¸ªæ ·æœ¬ä¸ºå•æ ·æœ¬ï¼Œè€Œä¸æ˜¯ batch æ ·æœ¬ã€‚è¿™æ˜¯å› ä¸ºåœ¨åˆ†å¸ƒå¼æµ‹è¯„æ—¶ï¼Œæˆ‘ä»¬ä¼šä»¥å•æ ·æœ¬çš„æ€»æ•°å»è£å‰ª collected results

## Dist Comm

åˆ†å¸ƒå¼é€šä¿¡åŸºæœ¬ä¸Šå’Œ `torch.distributed` ç›¸ä¼¼ï¼Œä¸è¿‡æ”¹è¿›äº†ä¸€äº›å¯ç”¨æ€§ï¼Œä¾‹å¦‚ï¼šåŠ å…¥ `is_distributed()` åˆ¤æ–­ï¼Œä½¿å¾—åœ¨éåˆ†å¸ƒå¼ç³»ç»Ÿä¸Šä¸ä¼šæŠ¥é”™ï¼›å¯¹éœ€è¦åˆ›å»º place holder çš„åœ°æ–¹è‡ªåŠ¨åˆ›å»ºï¼Œä¾‹å¦‚ `all_gather` ç›¸å…³æ–¹æ³•

é™¤æ­¤ä¹‹å¤–è¿˜æä¾›äº† CPU ä¸Šçš„ `collect_results` æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ˜¯åˆ›å»ºäº†ä¸€ä¸ªä¸´æ—¶æ–‡ä»¶å¤¹ï¼Œå°†å„ä¸ª rank çš„å¯¹è±¡ä¿å­˜åœ¨æ–‡ä»¶å¤¹ä¸­ï¼Œç„¶ååœ¨ rank0 ä¸Šè¿›è¡Œæ•´åˆ

## DefaultHooks

Hooks æœºåˆ¶æ˜¯åœ¨ runner çš„ç‰¹å®šèŠ‚ç‚¹æ’å…¥ä¸€äº›åŠŸèƒ½ã€‚Default hooks å…¶å®éå¸¸é‡è¦ï¼Œè´Ÿè´£äº†è®°å½•æ—¥å¿—ï¼Œæ›´æ–°å­¦ä¹ ç‡ï¼Œä¿å­˜æ¨¡å‹ç­‰ã€‚ä½†æ˜¯é€šå¸¸åœ¨é…ç½®ä¸­è¢«éšè—èµ·æ¥äº†ï¼Œåœ¨ mmengine ä¸­ default hooks æœ‰6ä¸ª

1. **IterTimerHook**ï¼Œ`after_iter`ï¼Œè®°å½•æ¯ä¸€ä¸ª iteration å®ç”¨çš„æ—¶é—´ï¼Œä¼°ç®—è®­ç»ƒæ‰€éœ€æ—¶é—´ eta

2. **LoggerHook**ï¼Œ`after_train_iter`ï¼Œæ—¥å¿—å°†æ ¹æ® interval è¿›è¡Œé‡‡æ ·ï¼Œæœ€ç»ˆè¾“å‡ºåˆ° terminalï¼Œå¹¶ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶å’Œ visualization backend ä¸­

3. **ParamSchedulerHook**ï¼Œ`after_train_iter or after_train_epoch`ï¼Œåœ¨æ¯ä¸€ä¸ª epoch or iter è¿‡åæ›´æ–°å­¦ä¹ ç‡

4. **CheckpointHook**ï¼Œ`after_train_iter or after_train_epoch & after_val_epoch & after_train`ï¼Œä¿å­˜ä¿¡æ¯ï¼šmodel, optimizer, scheduler, epoch, iter, seed, dataset meta ç­‰

5. **DistSamplerSeedHook**ï¼Œ`before_train_epoch` è®¾ç½®éšæœºç§å­ `set_epoch`

6. **RuntimeInfoHook**ï¼Œ`before_train_iter & after_train_iter & after_val_epoch` è¿™é‡Œä¼šå°†è¿è¡Œæ—¶çš„ä¿¡æ¯æ”¾å…¥ message hub å½“ä¸­ï¼ŒåŒ…æ‹¬ metaï¼Œlrï¼Œlossï¼Œmetrics


## Runner

**é‡è¦ï¼**æ¨¡å‹è®­ç»ƒæµç¨‹å›¾å¦‚ä¸‹

![image-20240115174002380](MMEngine 2.0/image-20240115174002380.png)

æˆ‘å°†ç”¨ä¼ªä»£ç çš„å½¢å¼æ¥æè¿°æ•´ä¸ª Runner æµç¨‹

```python
class Runner:
    def __init__(self, **cfgs):
        # dataloader, can be lazy init
        self.train_dataloader = self.build_dataloader(mode='train')
        self.val_dataloader = self.build_dataloader(mode='val')
        self.test_dataloader = self.build_dataloader(mode='test')
        
        # init distributed, self.setup_env(), I elaborate here
        mp.set_start_method('fork', force=True)
        torch.cuda.set_device(local_rank)
        torch.dist.init_process_group(backend='nccl')
        
        # random seed
        self.set_randomness(seed)
        
        # log and experiment_name
        self.mkdir_or_exist(work_dir)	# done before here
        self._experiment_name = ...
        self._log_dir = ...
        self.logger = self.build_logger()
        
        # message hub
        self.message_hub = self.build_message_hub()
        
        # visualizer
        self.visualizer = self.build_visualizer()
        
        # build model and dist model
        self.model = self.build_model()
        self.model = self.wrap_model(self.model)
        
        # hooks
        self.hooks = self.register_hooks(default_hooks, custom_hooks)
        
        # dump config
        self.dump_config()
        
	def train(self):
        # build loops
        self.train_loop = self.build_train_loop()	# EpochBased or IterBased
        self.val_loop = self.build_val_loop()
        
        # build optimizer and scheduler
        self.optm_wrapper = self.build_optim_wrapper(model_parameters)
        self.param_schedulers = self.build_param_scheduler(self.optim_wrapper)	# a list
        
        # init model weights
        model.init_weights()
        
        # load or resume
        self.load_or_resume()
        
        # run loop
        model = self.train_loop.run()
        
        return model 
```

æ³¨æ„ï¼Œå»ºè®®ä½¿ç”¨ `mp.set_start_method('spawn', forec=True)`ï¼Œå› ä¸ºè¿™ä¼šé€ æˆä¸€äº›æ½œåœ¨é—®é¢˜ [link](https://pytorch.org/docs/stable/notes/multiprocessing.html#cuda-in-multiprocessing) [issue](https://github.com/pytorch/pytorch/issues/40403)ï¼Œä½†æ˜¯å¦‚æœèƒ½ç¡®ä¿åœ¨ fork ä¹‹å‰æ²¡æœ‰ä»»ä½•çš„ CUDA æ“ä½œï¼ˆä¸åˆ›å»º cuda tensor, ä¸å°†æ¨¡å‹ç§»åˆ° cuda ä¸Šç­‰ï¼‰å°±æ²¡æœ‰é—®é¢˜

## å­¦ä¹ ç‚¹

1. `inspect` å¯ä»¥ç”¨äºæŸ¥çœ‹å‡½æ•°å‚æ•°ï¼Œè·å¾—ç±»çš„æˆå‘˜

2. Mixin

   Mixin æ˜¯é¢å‘å¯¹è±¡ç¨‹åºè®¾è®¡è¯­è¨€ä¸­çš„ç±»ï¼Œæä¾›äº†æ–¹æ³•çš„å®ç°ã€‚å…¶ä»–ç±»å¯ä»¥è®¿é—® mixin ç±»çš„æ–¹æ³•ã€‚ç”±äº Python å…è®¸ä½¿ç”¨å¤šé‡ç»§æ‰¿ï¼Œå› æ­¤ Mixin å°±å¯ä½¿ç”¨å¤šç»§æ‰¿å®ç°

   å¤šç»§æ‰¿ä¼šé¡ºåºç»§æ‰¿ï¼Œæ‰€ä»¥åŒåçš„æ–¹æ³•ä¼šä¼˜å…ˆä½¿ç”¨æ’åºé å‰çš„çˆ¶ç±»

   ```python
   class A:
       def print(self): print("A")
   
   class B:
       def print(self): print("B")
   
   class C(B, A): pass
   class D(A, B): pass
   
   C().print() # B
   D().print() # A
   ```

   å¤šç»§æ‰¿çš„åˆå§‹åŒ–è¿˜éœ€è¦æ³¨æ„ Method Resolution Order, MRO çš„é¡ºåºï¼Œå‚è€ƒ [CSDN](https://blog.csdn.net/uncle_ll/article/details/88900521)ï¼Œä¸¾ä¸€ä¸ªç®€å•ä¾‹å­

   ```python
   class A(object):
       def __init__(self):
           print('init a')
           super().__init__()
   
   class B(object):
       def __init__(self):
           print('init b')
           super().__init__()  # comment this line, you will see no A is initialized
   
   class C(B, A):
       def __init__(self):
           B.__init__(self)
   
   C()
   print(C.mro())
   
   # init b
   # init a
   # [<class '__main__.C'>, <class '__main__.B'>, <class '__main__.A'>, <class 'object'>]
   ```

   å¯ä»¥çœ‹åˆ°æˆ‘ä»¬åªæ˜¯è°ƒç”¨äº† `B.__init__` ä½†æ˜¯ A ä»ç„¶è¢«åˆå§‹åŒ–äº†ï¼Œè¿™æ˜¯å› ä¸ºåœ¨ B çš„åˆå§‹åŒ–æ–¹æ³•ä¸­æˆ‘ä»¬ç”¨äº† `super().__init__`ï¼Œè¿™é‡Œéœ€è¦ç†è§£ super çš„ä½œç”¨

   å®é™…ä¸Šè¿™é‡Œ super çš„å‚æ•°æ˜¯çœç•¥çš„ï¼Œå®Œæ•´çš„å‚æ•°ä¸ºï¼š

   ```python
   super(B, self).__init__()
   # super(cls, instance).__init__()
   ```

   å…¶ä½œç”¨å°±æ˜¯åœ¨ mro åˆ—è¡¨ä¸­è·å¾—ä¸‹ä¸€ä¸ªå¯¹è±¡ï¼Œé€šå¸¸æ¥è¯´å°±æ˜¯çˆ¶ç±»

   å¦ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡æ˜¯ï¼Œä½ æ— æ³•å®ç°ä¸‹é¢çš„ä»£ç ï¼Œå› ä¸ºæ— æ³•ç”Ÿæˆ MRO åˆ—è¡¨

   ```python
   # Wrong
   class C(A, B):
       def __init__(self):
           B.__init__(self)
   # Correct
   class C(B, A):
       def __init__(self):
           B.__init__(self)
   ```

   å¦‚æœå¯ä»¥ç”Ÿæˆ MRO çš„è¯ä¼šæ˜¯è¿™æ ·å­ï¼š`C->A->B->A`ï¼Œé‚£ä¹ˆ A å¯ä»¥è¦†ç›– Bï¼Œè€Œ B ä¹Ÿå¯ä»¥è¦†ç›– Aï¼Œè¿™å°±çŸ›ç›¾äº†ï¼Œåˆ°åº•è°è¦†ç›–è°ï¼Ÿ

3. **ç†è§£åŸå§‹ optimizer ç»„ä»¶**

   ä¸‹æ–¹ä»£ç æ˜¯ä¸€ä¸ªç®€å•çš„ Optimizer çš„å®éªŒï¼Œä»ç»“æœå¯ä»¥çŸ¥é“ï¼š

   1. `state.keys() & param_groups[0]['params'] & model parameters` çš„é•¿åº¦éƒ½æ˜¯ä¸€æ ·çš„ï¼ˆå‰ææ˜¯åªæœ‰ä¸€ä¸ª param groupsï¼Œå¹¶ä¸”ä»–ä»¬éƒ½éœ€è¦è¿›è¡Œæ¢¯åº¦è®¡ç®— `requires_grad`ï¼‰ã€‚å®é™…ä¸Šæ˜¯å› ä¸ºä»–ä»¬éƒ½æŒ‡å‘çš„åŒä¸€ä¸ªäº‹ç‰©ï¼šæ¨¡å‹çš„å¯å­¦ä¹ å‚æ•°
   2. `state_dict` åŒ…å« `state & param_groups` ä¸¤ä¸ªéƒ¨åˆ†
      - `param_groups` åŒ…å«äº†å½“å‰çš„å­¦ä¹ ç‡ã€æƒé‡è¡°å‡ç­‰ç­‰
      - `state` åŒ…å«äº†å½“å‰çš„ä¸­é—´çŠ¶æ€ï¼Œä¾‹å¦‚æ¢¯åº¦çš„ä¸€é˜¶å¹³å‡å’ŒäºŒé˜¶å¹³å‡
   3. `defaults` ä»£è¡¨è¾“å…¥æœªç»è¿‡ scheduler çš„ä¼˜åŒ–å™¨å‚æ•°

   ```python
   # simple optimizer test
   import torch
   from torch.optim.adamw import AdamW
   from torch.optim.lr_scheduler import CosineAnnealingLR
   from torchvision.models import resnet50
   model = resnet50(pretrained=False)
   opt = AdamW(model.parameters(), lr=0.01)
   # multi groups
   # params = list(model.parameters())
   # opt = AdamW([{'params': params[0], 'lr': 0.01},
   #              {'params': params[1:], 'lr': 0.001}])
   lr_scheduler = CosineAnnealingLR(opt, T_max=10)
   
   img = torch.randn(1, 3, 224, 224)
   target = torch.randint(0, 1000, (1,))
   
   output = model(img)
   loss = torch.nn.CrossEntropyLoss()(output, target)
   loss.backward()
   opt.step()
   lr_scheduler.step()
   
   print(len(opt.state))
   print(len(opt.param_groups[0]['params']))
   print(len(list(model.parameters())))
   print(opt.state_dict().keys())
   print(opt.param_groups[0].keys())
   print(opt.param_groups[0]['lr'])
   print(opt.defaults)
   
   # 161
   # 161
   # 161
   # dict_keys(['state', 'param_groups'])
   # dict_keys(['params', 'lr', 'betas', 'eps', 'weight_decay', 'amsgrad', 'foreach', 'maximize', 'capturable', 'differentiable', 'fused', 'initial_lr'])
   # 0.009755282581475769
   # {'lr': 0.01, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.01, 'amsgrad': False, 'foreach': None, 'maximize': False, 'capturable': False, 'differentiable': False, 'fused': None}
   ```

4. Amp Training

   æ··åˆç²¾åº¦è®­ç»ƒå…¶å®æ˜¯è‡ªåŠ¨åœ°å°†æŸäº›èŠ‚ç‚¹è½¬æ¢ä¸º fp16 è¿›è¡Œè®¡ç®—ï¼Œä½†æ˜¯å¤§å¤šæ•°èŠ‚ç‚¹ä»ç„¶è¿˜æ˜¯ fp32 è®¡ç®—ï¼Œä¾‹å¦‚ layer normï¼Œè¿™å¯¹ transformer æ¥è¯´å°±ä¸å¤ªå‹å¥½ã€‚ç»è¿‡æµ‹è¯•çº¯å·ç§¯ç¡®å®èƒ½å¤ŸèŠ‚çœæ¥è¿‘ä¸€åŠçš„æ˜¾å­˜ ~40%ï¼Œè€Œ transformer block èƒ½èŠ‚çœ ~10% çš„æ˜¾å­˜ï¼Œåœ¨ä¸€äº›æè‡´åœºæ™¯ä¸‹ï¼Œè¿™10%å¯èƒ½å°±æ˜¯ä½ éœ€è¦çš„ã€‚ä¸‹é¢æ˜¯æµ‹è¯•çš„ç®€å•è„šæœ¬

   ```python
   # a script to test amp training
   import torch
   import torch.nn as nn
   import torch.optim as optim
   from torch.cuda.amp import GradScaler, autocast
   
   device = "cuda"
   
   model = nn.Sequential(
       nn.Conv2d(256, 256, 3, 1, 1),
       nn.ReLU(inplace=True),  # inplace=True saves more memory
       nn.Conv2d(256, 256, 3, 1, 1),
       nn.ReLU(inplace=True)
   )
   
   model.to(device)
   
   # input_data = torch.randn(1, 256, 1024, 1024, device=device)
   # target = torch.randn(1, 256, 1024, 1024, device=device)
   input_data = torch.randn(8, 1024, 1024, device=device)
   target = torch.randn(8, 1024, 1024, device=device)
   loss_fn = nn.MSELoss(reduction='sum')
   optimizer = optim.SGD(model.parameters(), lr=1e-3)
   
   do_amp = False
   # Runs forward and backward passes with autocasting.
   for _ in range(1000):
       if do_amp:
           with autocast():
               output = model(input_data)
               loss = loss_fn(output, target)
   
           # Scales loss.  Calls backward() on scaled loss to create scaled gradients.
           # Backward passes under autocast are not recommended.
           scaler = GradScaler()
           scaler.scale(loss).backward()
   
           # scaler.step() first unscales the gradients of the optimizer's assigned params.
           # If these gradients do not contain infs or NaNs,# optimizer.step() is then called,
           # otherwise, optimizer.step() is skipped.
           scaler.step(optimizer)
   
           # Updates the scale for next iteration.
           scaler.update()
           optimizer.zero_grad()
   
       else:
           output = model(input_data)
           loss = loss_fn(output, target)
           loss.backward()
           optimizer.step()
           optimizer.zero_grad()
   ```

5. æ¢¯åº¦è£å‰ª

   å¯¹äºæ¢¯åº¦çš„äºŒèŒƒæ•°å€¼è¶…è¿‡é˜ˆå€¼çš„æƒ…å†µè¿›è¡Œç¼©æ”¾ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„æµ‹è¯•ä»£ç 

   ```python
   import torch.nn as nn
   import torch
   
   clip_func = nn.utils.clip_grad_norm_
   
   a = torch.tensor([2., 5.], requires_grad=True)
   y = (a ** 2).sum()
   y.backward()
   print('tensor_grad before clip:', a.grad)
   print('grad norm before clip:', torch.norm(a.grad))
   total_norm = clip_func([a], 10, norm_type=2)
   print('tensor_grad after clip:', a.grad)
   print('grad norm after clip:', torch.norm(a.grad))
   print('total_norm is the same as grad norm before clip:', total_norm)
   
   # tensor_grad before clip: tensor([ 4., 10.])
   # grad norm before clip: tensor(10.7703)
   # tensor_grad after clip: tensor([3.7139, 9.2848])
   # grad norm after clip: tensor(10.0000)
   # total_norm is the same as grad norm before clip: tensor(10.7703)
   ```

6. åœ¨å¯¹ç±»æ–¹æ³•ä½¿ç”¨ `partial` æ–¹æ³•æ—¶ï¼Œæ˜¯ä¼šæŠ¥é”™çš„ï¼Œå› ä¸º `self` ä¸ä¼šè¢«è‡ªåŠ¨ä¼ å…¥ï¼Œå¿…é¡»ä½¿ç”¨ `functools.partialmethod`

7. ç½‘ç»œå¦‚æœå­˜åœ¨ä¸€äº›ä¸ç¨³å®šç®—å­ï¼Œå³ä½¿å›ºå®šäº†éšæœºç§å­ä¹Ÿæ²¡åŠæ³•ï¼Œå¯ä»¥æŸ¥çœ‹å…¶ä¸­çš„ä¾‹å­ [torch.use_deterministic_algorithms](https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html)ï¼Œä¾‹å¦‚æ’å€¼ä¹‹ç±»çš„æ“ä½œéƒ½æ— æ³•ä¿è¯å¯å¤ç°æ€§ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„ `F.grid_sample` çš„éšæœºæ€§å±•ç¤ºï¼Œä¹Ÿå¯ä»¥æŸ¥çœ‹ [pytorch discuss](https://discuss.pytorch.org/t/f-grid-sample-non-deterministic-backward-results/27566/5)ï¼Œå¯ä»¥çœ‹åˆ°å›ºå®šéšæœºç§å­çš„æ¢¯åº¦æ˜¯æœ‰å¾®å°å·®å¼‚çš„

   ```python
   # test on F.grid_sample
   import torch
   import torch.nn as nn
   import torch.nn.functional as F
   from mmengine.runner.utils import set_random_seed
   from torch.optim.sgd import SGD
   
   set_random_seed(0)
   class GridSample(torch.nn.Module):
       def __init__(self):
           super().__init__()
           self.conv = nn.Sequential(
               nn.Conv2d(3, 256, 3, padding=1),
               nn.BatchNorm2d(256),
               nn.ReLU(),
               nn.Conv2d(256, 256, 3, padding=1),
               nn.BatchNorm2d(256),
               nn.ReLU(),
               nn.Conv2d(256, 256, 3, padding=1),
           )
   
       def forward(self, x, grid):
           x = self.conv(x)
           return F.grid_sample(x, grid, mode='bilinear', padding_mode='zeros', align_corners=True)
   
   grid_sample = GridSample().cuda()
   x = torch.randn(4, 3, 10, 10).cuda()
   optim = SGD(grid_sample.parameters(), lr=0.0001)
   
   for i in range(10):
       grid = torch.randn(4, 10, 10, 2).cuda()
       out = grid_sample(x, grid)
       tgt = torch.randn(4, 256, 10, 10).cuda()
       loss = F.mse_loss(out, tgt, reduction='sum')
       print(loss.item())
       loss.backward()
       optim.step()
       optim.zero_grad()
   ```

   è¿™ä¸ªéšæœºæ€§åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¸¦æ¥çš„è¡¨ç°ä¸ºï¼šresume è¿‡ååªèƒ½å¯¹ä¸Šç¬¬ä¸€ä¸ª iteration çš„ loss å€¼ï¼Œä¹‹åå°±å¯¹ä¸ä¸Šäº†ï¼Œå› ä¸ºæ¢¯åº¦å˜äº†
   
8. è¿˜æœ‰ä¸€ä¸ªçœæ˜¾å­˜çš„ä½¿ç”¨æ–¹æ³•ï¼šactivation checkpointingï¼Œå¯ä»¥åœ¨ mmengine ä¸­æ–¹ä¾¿é…ç½®

## Question

1. æœ€åçš„ `PackxxxInputs` ç°åœ¨æ˜¯å¦èƒ½å¤Ÿé€šè¿‡ `@dataclass` ç®€å•å¤„ç†æ‰

2. Logger å†™å¾—è¿˜æ˜¯æ¯”è¾ƒç¹çï¼Œèƒ½ä¸èƒ½å†™å¾—æ›´ç®€æ´ä¸€äº›ï¼Œå°¤å…¶æ˜¯ LogProcessorï¼Œä¸è¿‡ç”¨æ¥ä½¿ç”¨çš„è¯ï¼Œå®Œå…¨æ²¡é—®é¢˜

3. CheckpointHook ä¸­ä¿å­˜ best model æ˜¯æœ‰ bug çš„

4. Fileio å†™å¾—ä¹Ÿæ¯”è¾ƒå¤æ‚ï¼Œå¯ä»¥è€ƒè™‘ç®€åŒ–ã€‚åˆ°åº•ä»€ä¹ˆæ˜¯ FileClient?

5. Evaluator ä¸­åŠ å…¥äº† å…¶è®¾è®¡çš„ BaseDataElement é€»è¾‘ï¼Œä¸å¥½ç”¨

6. Optimizer ä¸­åŠ å…¥äº† `base_param_settings`ï¼Œè¿™ä¼¼ä¹æ˜¯ä¸€ä¸ªæ— ç”¨è®¾è®¡ï¼Œè€ƒè™‘å»é™¤

7. ä¸ºä»€ä¹ˆ pytorch é€šå¸¸éœ€è¦é€’å½’å­æ¨¡å—ï¼Ÿææ¸…æ¥šå…¶ä¸­åŒºåˆ« [modules vs children](https://www.cnblogs.com/marsggbo/p/11512242.html#nnmodules-vs-nnchildren)ï¼Œmodules æ˜¯å®Œå…¨é€’å½’ï¼Œæˆ‘ä»¬é€šå¸¸ä¸å¸Œæœ›è¿™æ ·åšï¼Œè€Œ children åˆ™æ˜¯å‘ä¸‹é€’å½’ä¸€å±‚ï¼Œè€Œ `parameters & named_parameters` å°±æ˜¯è¿”å›æ¯ä¸€ä¸ªå¯å­¦ä¹ çš„ç½‘ç»œå±‚å‚æ•°

8. BaseModel çš„æ¦‚å¿µä¸å¤Ÿå¥½ï¼Œè€Œæ˜¯æ›´åº”è¯¥ç†è§£ä¸º xxx æ¨¡å‹

9. Hook çš„æ ‡è¯†ä¸å¤Ÿæ¸…æ™°ã€‚æ³¨å†Œäº†å“ªäº› default hookï¼Œæœ‰ä»€ä¹ˆç”¨ï¼Œè¿™äº›å¯¹äºé˜…è¯»ä»£ç çš„äººæ¥è¯´ä¸å‹å¥½

10. AMP åœ¨ä»€ä¹ˆæ—¶å€™èƒ½å¤ŸèŠ‚çœæ˜¾å­˜ï¼Ÿæˆ‘å°è¯•è¿‡è‡ªå·±çš„ä»£ç ï¼Œå¹¶ä¸æ€ä¹ˆçœæ˜¾å­˜ï¼Œæ˜¯ nn.Embedding çš„é—®é¢˜ï¼Ÿ[issue](https://github.com/pytorch/pytorch/issues/98814) å®é™…ä¸Šä¹Ÿæ²¡åŠæ³•æ‰‹åŠ¨è½¬æ¢ï¼Œæ¯”è¾ƒéº»çƒ¦

11. `MMDistributedDataParallel` æœ‰æ²¡æœ‰æ›¿ä»£æ–¹å¼ï¼Œä¸å¤Ÿä¼˜é›…

12. `data_preprocessor` å†™å¾—æ¯”è¾ƒéšæ™¦ï¼Œæœ‰æ—¶å€™ä¼šå¿½ç•¥æ‰

13. num workers æ˜¯çœŸçš„å¾ˆæœ‰ç”¨ï¼Œå°¤å…¶æ˜¯å½“è·å– data çš„æ—¶é—´å¤§äºè®¡ç®—æ—¶é—´çš„æ—¶å€™ï¼Œåœ¨ cpu ä¸Šè¿›è¡Œæ•°æ®å¢å¼ºè¿˜æ˜¯æ¯”è¾ƒè€—æ—¶çš„...

14. amp çš„è®­ç»ƒä¼¼ä¹å‡ºç°äº†é—®é¢˜ï¼Œå³ä½¿æ˜¯æµ®ç‚¹è®­ç»ƒï¼Œloss ä¹Ÿä¸‹é™æ²¡é‚£ä¹ˆå¿«ï¼Œæ˜¯ä¸æ˜¯è¦æ£€æŸ¥ä¸‹ loss çš„è¾“å‡ºæ˜¯å¦æœ‰å¹³æ»‘

    æ˜¯çš„ loss æ˜¯ç”± log processor è¿›è¡Œäº†å¹³æ»‘å¤„ç†ï¼Œwindow size = 10ã€‚è€Œä¸” mmengine çš„ amp optim wrapper é»˜è®¤æ˜¯å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œè‡ªåŠ¨åŠç²¾åº¦å¤„ç†ï¼Œä½†é€šå¸¸åœ¨ head éƒ¨åˆ†ä¸ä¼šä½¿ç”¨åŠç²¾åº¦ã€‚æ‰€ä»¥æˆ‘å»é™¤äº† autocast çš„ contextï¼Œé€‰æ‹©ä½¿ç”¨ mmcv ä¸­çš„ `auto_fp16` çš„è£…é¥°å™¨å½¢å¼ï¼Œåªå¯¹æ¨¡å‹çš„ backbone éƒ¨åˆ†è¿›è¡ŒåŠç²¾åº¦ï¼Œç„¶åå°† `AmpOptimWrapper` çš„ `optim_context` æ”¹ä¸ºé»˜è®¤ç‰ˆæœ¬ï¼Œä¸ä½¿ç”¨ `autocast` çš„ä¸Šä¸‹æ–‡

    `clip_grad` å¯¹äºè®­ç»ƒçš„æ”¶æ•›ä¹Ÿæ˜¯éå¸¸é‡è¦çš„ï¼Œå¦‚æœä¸è¿›è¡Œæ¢¯åº¦å‰ªè£çš„è¯ï¼Œloss åœ¨åˆæœŸä¼¼ä¹ä¸‹é™æ›´æ…¢ï¼Œæ²¡æœ‰åšè¿‡å®Œæ•´çš„å®éªŒ

15. åœ¨æ¨¡å‹ä¸­çš„éšæœºè¿‡ç¨‹ä¹Ÿå¿…é¡»ç¡®å®šä¸‹æ¥ï¼Œè¿™æ ·æ‰èƒ½å®Œå…¨çš„ resumeï¼

16. revised keys æ²¡æœ‰åŠæ³•ä¼ åˆ° runner å½“ä¸­

17. random seed åˆ°åº•èƒ½æ§åˆ¶åˆ°å“ªäº›å±‚é¢ï¼Ÿ

    worker ä¸­çš„éšæœºç§å­æ˜¯ç‹¬ç«‹æ§åˆ¶çš„

18. å¯¹ä¸Šäº† cosine learning rate and warm by epoch

19. logger æ²¡åŠæ³•çœ‹åˆ°æ˜¯ä»å“ªä¸ªæ–‡ä»¶è®°å½•çš„ï¼Œä¸å¥½ debug

20. config çš„ä»£ç ä¹Ÿå¤ªé‡äº†ï¼Œæˆ‘ä¸éœ€è¦è¿™ä¹ˆå¤šçš„åŠŸèƒ½ï¼Œéœ€è¦åˆ ç¹å°±ç®€ï¼Œä½†å¦‚æœä½¿ç”¨ LazyConfig çš„æ ¼å¼ï¼Œregistry å°±ä¼šæˆä¸ºå†å²

21. Pytorch çš„ fp16 åœ¨ ViT å½“ä¸­çš„è®¡ç®—è¿‡ç¨‹

22. ä¸å¥½æ›¿æ¢ collate functionï¼Œæ˜¯ä½¿ç”¨çš„ `get` æ–¹æ³•ï¼Œæ‰¾ä¸åˆ°ä½ æ³¨å†Œçš„ collate functionï¼Œå¿…é¡»è¦åœ¨ config é‡Œé¢ä½¿ç”¨å®Œæ•´è·¯å¾„

23. freeze weights åº”è¯¥åœ¨ build çš„æ—¶å€™å°±å®Œæˆï¼Œæ›´å‡†ç¡®åœ°è¯´éœ€è¦åœ¨ DDP ä¹‹å‰å®Œæˆï¼Œå¦åˆ™ DDP å°†ä¸ä¼šè¯†åˆ« `requires_grad=False`ï¼Œå¹¶å°è¯•æ›´æ–°æ¢¯åº¦ï¼Œè¿™å°±ä¼šå¯¼è‡´æŠ¥é”™ [github issue](https://github.com/pytorch/pytorch/issues/22049#issuecomment-505617666)

    ä¸€ä¸ªå…¸å‹çš„æŠ¥é”™ï¼š

    - `RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. `
    - `Parameter indices which did not receive grad for rank 0`

24. mmengine worker init fn ä¼šè®©æ¯ä¸€ä¸ª epoch çš„éšæœºæ€§ç›¸åŒï¼Œå‚è€ƒ [zhihu](https://zhuanlan.zhihu.com/p/618639620)

    ```python
    from torch.utils.data import Dataset, DataLoader
    from mmengine.dataset import worker_init_fn as default_worker_init_fn
    from mmengine.dataset.sampler import  DefaultSampler
    from mmengine.runner.utils import set_random_seed
    import numpy as np
    set_random_seed(1)
    class TestDataset(Dataset):
        def __init__(self, num):
            self.data = [i for i in range(num)]
    
        def __getitem__(self, idx):
            ori_data = self.data[idx]
            random_aug = np.random.randint(0, 100)
            return (ori_data, random_aug)
    
        def __len__(self):
            return len(self.data)
    from functools import partial
    init_fn = partial(
        default_worker_init_fn,
        num_workers=8,
        rank=0,
        seed=0,
        disable_subprocess_warning=True)
    
    dataset = TestDataset(20)
    samper = DefaultSampler(dataset, shuffle=True)
    dataloader = DataLoader(dataset, sampler=samper, batch_size=10, worker_init_fn=init_fn, num_workers=8)
    data_iter = iter(dataloader)
    epoch = 3
    for epoch_id in range(0, epoch):
        # set epoch
        samper.set_epoch(epoch_id)
        for idx, data in enumerate(dataloader):
            print(f'epoch {epoch_id}, iter {idx}, data {data}')
        print('-----------------------------------------')
    ```

    è§£å†³æ–¹æ³•æ˜¯åŠ å…¥ `torch.initial_seed()`

    ```python
    worker_seed = (num_workers * rank + worker_id + seed + torch.initial_seed()) % 2**32
    ```

25. mmengine ä¸­æ‰€äº§ç”Ÿçš„ json file å®é™…ä¸Šæ˜¯ line jsonï¼Œç›´æ¥ä½¿ç”¨ `json.loads` è¯»å–ä¼šäº§ç”Ÿé—®é¢˜ï¼Œéœ€è¦é€è¡Œè¯»å–

    ```python
    for line in f:
        data = json.loads(line)
    ```

    è¿™è¿˜æ˜¯åœ¨ hugging face å­¦çš„