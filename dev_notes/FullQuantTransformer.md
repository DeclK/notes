# FullQuantTransformer

ğŸ¤— **FullQuantTransformer (FQT) æ˜¯ä¸€ä¸ªç®€æ´ã€é«˜æ•ˆã€å¼ºå¤§çš„é‡åŒ–å·¥å…·ç®±**

## HightLight

1. Easy to hackï¼šå°‘æŠ½è±¡ï¼Œæ˜“ç†è§£ï¼Œè½»é‡åŒ–ï¼ˆ~1000è¡Œæ ¸å¿ƒé‡åŒ–ä»£ç ï¼‰
2. Easy to useï¼šåªéœ€è¦~4è¡Œæ ¸å¿ƒä»£ç å³å¯å®Œæˆé‡åŒ–
3. Flex DataRecorderï¼šå¿«é€Ÿé€‚é…ä»»ä½•é¡¹ç›®ä»¥æ„å»º calibration set
4. æ”¯æŒå¤šç§é‡åŒ–æ–¹æ³•ä»¥åŠå¤šç§ç²¾åº¦å®ç°ï¼šGPTQ, AWQ, SmoothQuant, W4/W8/A4/A8/A16
5. æ”¯æŒå„ç§ä¸åŒæ¨¡å‹çš„é‡åŒ–ï¼šQwen Dense, MoE, ViT
6. æ”¯æŒ w4a16 æ¨¡å‹å‹ç¼©ï¼Œå…¼å®¹äº‘ç«¯ serving æ¡†æ¶ï¼ˆvllm & sglangï¼‰
7. æ”¯æŒé‡åŒ–åŠ é€Ÿç®—å­ï¼šw8a8 scaled mm (int8 & fp8)
8. å·¥å…·é½å…¨ï¼šæ¿€æ´»å¯è§†åŒ–ã€æ—¥å¿—ç³»ç»Ÿã€io æ¥å£ã€timer benchmark

## Code Structure Overview

TODOï¼šæ€ç»´å¯¼å›¾

## Code Design

æ¥ä¸‹æ¥å°†æ›´å…·ä½“åœ°ä»‹ç»ä¸€äº›æ ¸å¿ƒæ¨¡å—çš„åŠŸèƒ½ä»¥åŠå…¶ä½¿ç”¨æ–¹å¼ï¼Œå¦‚æœæœ‰æ¯”è¾ƒç‰¹åˆ«çš„è®¾è®¡æ€è·¯ä¹Ÿå°†è¿›è¡Œæè¿°

### Data

#### DataRecorder

åœ¨é‡åŒ–æ¡†æ¶ä¸­ï¼Œæ ¡å‡†é›†çš„æ”¶é›†æ˜¯å¿…ç»ä¹‹è·¯ã€‚åœ¨é‡åŒ–ç®—æ³•ä¸­ï¼Œæ ¡å‡†é›†é€šå¸¸å°±æ˜¯æ¨¡å‹å‰å‘è¿‡ç¨‹ä¸­çš„è¾“å…¥ or è¾“å‡ºã€‚è¦è·å¾—è¿™äº›è¾“å…¥ or è¾“å‡ºï¼Œå¯ä»¥åˆ©ç”¨ pytorch æä¾›çš„ `register_forward_hook` æ¥å£ï¼Œè¯¥æ¥å£ä¼šåœ¨ `nn.Module `è¿è¡Œå…¶ `forward` æ–¹æ³•ä¹‹å‰ï¼Œè°ƒç”¨æ‰€æ³¨å†Œçš„ hook å‡½æ•°ï¼Œå¹¶ä¸”ç»™ hook å‡½æ•°ä¼ å…¥å…¶ `forward` ä¸­çš„è¾“å…¥å‚æ•°ã€‚

é‚£ä¹ˆ `DataRecorder` çš„è®¾è®¡æ€è·¯å°±å¾ˆç®€å•äº†ï¼šåªéœ€è¦æŠŠ hook å‡½æ•°è®¾ç½®ä¸ºä¿å­˜è¿™äº›è¾“å…¥å‚æ•°å³å¯ï¼Œhook å‡½æ•°å’Œä¿å­˜çš„å†…å®¹äº¤ç”± `DataRecorder` è¿›è¡Œç®¡ç†

`DataRecorder` çš„**æœ€å…¸å‹**ç”¨æ³•å¦‚ä¸‹ï¼š

```python
model = Qwen2ForCausalLM(config)

# create data recorder, set target module & saving device
dc = DataRecorder(model=model.model.layers[0],
                  device="cpu")

dc.register_input_hooks()		# register input hooks
dc.register_pos_embed_hooks()	# register pos emb hooks

for i, prompt in enumerate(dataset): # model evaluation loop
	model.generate(prompts)
    dc.save_inputs(f"{i}.pkl")	# save inputs
    dc.clear_inputs()			# clear inputs for next round conversation

dc.remove_input_hooks()			# remove hooks
```

åœ¨ä¸Šè¿°ä¾‹å­ä¸­ï¼Œä½¿ç”¨äº† `register_input_hooks` æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ä¼šä¿å­˜æ‰€æŒ‡å®šæ¨¡å‹çš„ç¬¬ä¸€ä¸ª positional argumentï¼Œè¯¥ argument å¯¹äº transformer layers æ¥è¯´å°±æ˜¯æ‰€è¾“å…¥çš„ hidden states

```python
class Qwen2DecoderLayer(nn.Module):
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        ...
        **kwargs: Unpack[FlashAttentionKwargs],
    ):
```

ä¿å­˜çš„ `i.pkl` æ˜¯ä¸€ä¸ª dict of listï¼Œå…¶ä¿å­˜äº†ä¸€ä¸ª generate è¿‡ç¨‹ä¸­æ¯æ¬¡ forward æ‰€ä¿å­˜æ‰€ input hidden states & position embeddings

```python
{
    "model_input": [hs_prefill, hs_decode_0, hs_decode_1, ...],
    "model_pos_embed": [pos_prefill, pos_decode_0, pos_decode_1, ...]
}
```

é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰å…¶ä»–çš„ hooks å¯ç”¨ï¼Œå¯æŒ‰éœ€å–ç”¨ï¼š

1. `register_linear_input_hooks`ï¼Œä¼šè®°å½•è¯¥æ¨¡å‹ä¸­æ‰€æœ‰ linear å±‚çš„ inputï¼Œä¿å­˜å…³é”®å­—ä¸º linear å±‚çš„åå­—
2. `register_input_ids_hooks`ï¼Œä¼šè®°å½•æ¨¡å‹è¾“å…¥ä¸­çš„ `input_ids`ï¼Œä¿å­˜å…³é”®å­—ä¸º `model_input_ids`
3. `register_output_hooks`ï¼Œä¼šè®°å½•æ¨¡å‹è¾“å‡ºä¸­çš„æ‰€æœ‰ outputï¼Œä¿å­˜å…³é”®å­—ä¸º `model_output`

#### DefaultDataset

`DefaultDataset` ç”¨äºé…åˆ `DataRecorder`ï¼Œå¯¹ä¿å­˜çš„æ•°æ®åš concat å¤„ç†ï¼šå› ä¸ºä¸€æ¬¡ generate æ•°æ®ä¼šä¿å­˜ prefill + decode çš„æ‰€æœ‰æ¿€æ´»å€¼ï¼Œéœ€è¦è¿›è¡Œ concat å¤„ç†ä»¥æ–¹ä¾¿è®¡ç®—ã€‚`DefaultDataset` ä¸­æ¯ä¸€ä¸ª item ä¸ºä¸€ä¸ª dictï¼š

1. `input_tensor`ï¼Œå³ä¸º `hidden_states (B, N, C)` 
2. `kwargs`ï¼šä¸€ä¸ª dictï¼ŒåŒ…å« `position_ids (B, N)`  & `position_embeddings (B, N, C)`

åœ¨ `dataset.py` ä¸­è¿˜æœ‰ä¸€ä¸ª `build_calib_datset_inputs` å‡½æ•°ï¼Œè¯¥å‡½æ•°å°±æ˜¯å°† dataset ä¸­çš„éšæœº `num_samples` ä¸ªæ•°æ®è¿›è¡Œæ‰“åŒ…ï¼Œæ„å»ºç¬¦åˆé‡åŒ–ç®—æ³•çš„ inputs

### Quantizer

ä¸ºäº†å®ç°å¤šç§é‡åŒ–ç®—æ³•ï¼ŒFQT å®ç°äº†ä¸€ä¸ªçµæ´»çš„ quantizerï¼Œå…¶èƒ½å¤Ÿå®ç°ä»¥ä¸‹**å¯¹ç§° int é‡åŒ–**

1. per-channelï¼Œé’ˆå¯¹äºäºŒç»´å¼ é‡ `(N, K)`ï¼Œè®¡ç®—å‡ºä¸€ä¸ª `(1, K)` çš„ scale
2. per-tokenï¼Œé’ˆå¯¹äºå¤šç»´å¼ é‡ `(.., M, K)`ï¼Œè®¡ç®—å‡ºä¸€ä¸ª `(..., M, 1)` çš„ scale
3. per-tensorï¼Œé’ˆå¯¹äºå¤šç»´å¼ é‡ï¼Œè®¡ç®—å…¶æœ€å¤§å€¼ä¸º scale
4. per-blockï¼Œé’ˆå¯¹äºå¤šç»´å¼ é‡ `(..., M, K)`ï¼Œè®¡ç®—å‡ºä¸€ä¸ª `(..., M, G)` çš„ scaleï¼Œå…¶ä¸­ `G = K // group size`

æ„å»º `Quantizer` éœ€è¦ç¡®å®šä»¥ä¸‹å‚æ•°

```python
class Quantizer(nn.Module):
    def __init__(self, 
                 bits: int, 
                 method: str, 
                 group_size: int = None):
```

1. bitsï¼Œé‡åŒ–æ¯”ç‰¹ï¼Œæ— ä»»ä½•é™åˆ¶
2. methodï¼Œé‡åŒ–æ–¹æ³•ï¼Œå››é€‰ä¸€ `["per-tensor", "per-token", "per-channel", "per-block"]`
3. group sizeï¼Œä¸“é—¨ä¸º `per-block` é‡åŒ–æ–¹æ³•æ‰€æä¾›çš„å‚æ•°ï¼Œå¯é€‰

`Quantizer` æœ€é‡è¦çš„ä¸¤ä¸ªæ–¹æ³•ï¼š

1. `find_scale(x)`ï¼Œè®¡ç®—è¾“å…¥ `x` çš„ scaleï¼Œä¿å­˜åˆ° `self.scale` å½“ä¸­
2. `quantize(x)`ï¼Œè®¡ç®—è¾“å…¥ `x` çš„é‡åŒ–ç»“æœï¼ŒåŒ…å«äº† quantize & dequantize è¿‡ç¨‹ï¼Œæ•…è¿”å›å€¼å¹¶éæ•´æ•°ï¼Œè€Œæ˜¯æµ®ç‚¹

#### FakeQuantLinear

æœ‰äº†çµæ´»çš„ `Quantizer`ï¼ŒFQT æ„å»ºä¸€ä¸ª `FakeQuantLinear` å°±å˜å¾—ç®€å•èµ·æ¥äº†ï¼Œä¸€ä¸‹æ˜¯æ„å»º `FakeQuantLinear` æ‰€éœ€è¦çš„å‚æ•°

```python
class FakeQuantLinear(nn.Module):
    def __init__(
        self,
        in_features,
        out_features,
        bias=True,
        w_bits=8,
        a_bits=8,
        w_method="per-token",
        a_method="per-token",
        w_group_size=None,
        a_group_size=None,
    ):
```

å¯ä»¥çœ‹åˆ°ï¼Œé™¤äº†æ™®é€š linear æ‰€éœ€è¦çš„å‚æ•°å¤–ï¼Œè¿˜æœ‰å’Œ weight & activation quantizer ç›¸å…³çš„å‚æ•°ã€‚æ‰€ä»¥è¯¥ `FakeQuantLinear` æ”¯æŒä»»æ„ç²¾åº¦ã€ä»»æ„é‡åŒ–æ–¹æ³•çš„ä¼ªé‡åŒ–çº¿æ€§å±‚ã€‚é€šå¸¸ï¼Œæ„å»º `FakeQuantLinear` çš„æƒé‡é€šå¸¸æ¥è‡ªäºåŸæ¨¡å‹çš„ linearï¼Œæ‰€ä»¥ `FakeQuantLinear` è¿˜æä¾›äº†ä¸€ä¸ª `from_module` çš„æ–¹æ³•ï¼Œç”¨äºä»å·²å­˜åœ¨çš„ linear åˆ›å»º

```python
FakeQuantLinear.from_module(linear, w_bits=8, a_bits=8)
```

åŒæ—¶ FQT è¿˜é¢„è®¾äº†ä¸€äº›å¸¸ç”¨çš„ä¼ªé‡åŒ–çº¿æ€§å±‚ï¼Œå¯ç›´æ¥é€šè¿‡ `from_module` å¿«é€Ÿç”Ÿæˆå¯¹åº”ä¼ªé‡åŒ–çº¿æ€§å±‚

```python
class W8A8FakeLinear(FakeQuantLinear):
    @classmethod
    def from_module(cls, module, w_method="per-token", a_method="per-token"):
        return FakeQuantLinear.from_module(module, 8, 8, w_method=w_method, a_method=a_method)


class W4A16FakeLinear(FakeQuantLinear):
    @classmethod
    def from_module(cls, module, w_method="per-token", a_method="per-token"):
        return FakeQuantLinear.from_module(module, 4, 16, w_method=w_method, a_method=a_method)
```

#### QuantLinear

`FakeQuantLinear` æ˜¯ä½œä¸ºä¼ªé‡åŒ–çº¿æ€§å±‚ï¼Œå…¶æƒé‡ä»ç„¶æ˜¯æµ®ç‚¹ï¼Œè€ŒçœŸæ­£çš„é‡åŒ–çº¿æ€§å±‚å…¶æƒé‡ä¸º intï¼Œåœ¨ FQT ä¸­å¯¹ `QuantLinear` çš„æ”¯æŒæ¯”è¾ƒæœ‰é™ï¼Œå…¶ä½œç”¨ä»…ä¸ºå¯¹æƒé‡è¿›è¡Œå‹ç¼©ï¼Œæ— æ³•ä½¿ç”¨å‹ç¼©æƒé‡è¿›è¡Œ forward è®¡ç®—

æ„å»ºä¸€ä¸ª `QuantLinear` çš„æ–¹å¼ä¹Ÿæ¨èä½¿ç”¨ `from_module` æ–¹æ³•

```python
QuantLinear.from_module(linear, bits=4, method="per-block", group_size=64)
```

åœ¨ FQT ä¸­å®ç°äº†å¯¹ w4a16 çš„é‡åŒ–å‹ç¼©æ¨¡å‹çš„å¯¼å‡ºï¼Œå…¶å¯¼å‡ºæ ¼å¼é€‚é…äº vLLM & SGLang æ¡†æ¶

### LayerForwardManager

åœ¨é‡åŒ–ç®—æ³•ä¸­ï¼Œé€šå¸¸éœ€è¦å¯¹å„ç§å„æ ·çš„ module è¿è¡Œå…¶ forward æ–¹æ³•ã€‚è¿™äº› forward æ–¹æ³•æœ‰ä¸åŒçš„è¾“å…¥å‚æ•°ï¼Œå…¶è¾“å‡ºä¹Ÿå¯èƒ½äº”èŠ±å…«é—¨ã€‚FQT é’ˆå¯¹äºé‡åŒ–åœºæ™¯ï¼Œå®ç°äº†é€šç”¨çš„ `LayerForwardManager`ï¼Œå…¶ä½¿ç”¨åŒä¸€æ¥å£ï¼Œå¯è¿è¡Œä¸åŒ module çš„å‰å‘è¿‡ç¨‹

è¯¥ `LayerForwardManager` å¯¹è¿™äº› module çš„è¾“å…¥å‚æ•°æœ‰å¦‚ä¸‹å‡è®¾ï¼š**æœ‰ä¸”åªæœ‰ä¸€ä¸ª positional argumentsï¼Œé€šå¸¸ä¸º input hidden statesï¼Œå‰©ä½™çš„å‚æ•°å‡ä¸º kwargs å‚æ•°**ã€‚è¯¥å‡è®¾å¯¹äº linear å’Œ transformer decoder layer & attention & mlp å‡æˆç«‹

`LayerForwardManager` çš„ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹

```python
# import global forward_manager
from fqt.quantization.utils import forward_manager

def qwen_forward(layer: Qwen2DecoderLayer, input_tensor, **kwargs):
    output = layer(input_tensor, **kwargs)
    return output[0]

forward_manager.register("Qwen2DecoderLayer", qwen_forward)
```

æ­¤æ—¶å°±ç±»åä¸º `Qwen2DecoderLayer` çš„ moduleï¼Œæ³¨å†Œåˆ° `forward_manager` å½“ä¸­ï¼Œé€šè¿‡å¦‚ä¸‹æ–¹æ³•è¿è¡Œå‰å‘ï¼š

```python
from fqt.quantization.utils import forward_manager
def forward_layer(layer, input_tensor, **kwargs):
    return forward_manager.forward(layer, input_tensor, **kwargs)
```

å¦‚æœæ£€æµ‹åˆ°è¯¥ layer æ˜¯ `Qwen2DecoderLayer`ï¼Œé‚£ä¹ˆå°±ä¼šè°ƒç”¨ `qwen_forward`

å¦‚æœä¼ å…¥äº†æŸä¸ªæœªæ³¨å†Œçš„ layerï¼Œæ—¥å¿—ç³»ç»Ÿå°†ä¼šå‘å‡º warningï¼Œç„¶åé‡‡ç”¨ `default_forward` æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é€šå¸¸ä¸ä¼šæˆåŠŸ

```python
    def default_forward(self, layer, input_tensor, **kwargs):
        return layer(input_tensor, **kwargs)
```

### GPTQ

GPTQ ç®—æ³•æ˜¯ç›®å‰æœ€å¸¸ç”¨çš„é‡åŒ–ç®—æ³•ï¼ŒFQT æ„å»ºäº† `run_gptq_for_sequential_layers` API å‡½æ•°ï¼Œç”¨äºå¿«é€Ÿé‡åŒ– sequential æ¨¡å‹ï¼Œå…¶è¾“å…¥å‚æ•°ä¸ºï¼š

```python
def run_gptq_for_sequential_layers(layers, 
                                   inputs, 
                                   cfg: GPTQConfig):
```

1. layersï¼Œé€šå¸¸ä¸º transformer blocks (ModuleList or Sequential)

2. inputsï¼Œç”± `build_calib_datset_inputs` æ‰€æ„å»ºçš„ inputsï¼ŒåŒ…å« input hidden states & position embeddings

3. cfgï¼Œ`GPTQConfig` æ˜¯ä¸€ä¸ªç®€å•çš„ dataclassï¼Œä»¥é…ç½® GPTQ ç®—æ³•å‚æ•°

   ```python
   @dataclass
   class GPTQConfig:
       bits: int = 4
       group_size: int = 64
       method: str = "per-block"
       damp: float = 0.01
       blocksize: int = 128
   ```

### Unify AWQ & SmoothQuant

åœ¨ FQT ä¸­ï¼Œå°† SmoothQuant å’Œ AWQ ç»Ÿä¸€èµ·æ¥ï¼ŒäºŒè€…æœ¬è´¨ä¸ŠåŸç†æ˜¯ç›¸åŒçš„ï¼šè®¡ç®— scale å¯¹ activation è¿›è¡Œ smoothï¼ŒåŒæ—¶å°† scale èå…¥åˆ° weight å½“ä¸­ä»¥ä¿è¯è®¡ç®—ç»“æœä¸å˜ã€‚é€šè¿‡ä¸æ–­åœ°è®¡ç®—ä¸åŒ scale çš„é‡åŒ–æŸå¤±ï¼Œå¯»æ‰¾åˆ°ä¸€ä¸ªæœ€ä¼˜çš„ scaleã€‚äºŒè€…çš„åŒºåˆ«åœ¨äºï¼šåœ¨è®¡ç®—é‡åŒ–æŸå¤±æ—¶ï¼ŒSmoothQuant ä¼šå°†æƒé‡å’Œæ¿€æ´»éƒ½è¿›è¡Œé‡åŒ–ï¼Œè€Œ AWQ ä»…ä¼šå°†æƒé‡è¿›è¡Œé‡åŒ–ã€‚è¯¥åŒºåˆ«å¯ä»¥é€šè¿‡é…ç½® `Quantizer` æ–¹ä¾¿å®Œæˆ

FQT ä¸­ä½¿ç”¨ `run_smooth_quant_for_sequential_layers` API å‡½æ•°ï¼Œç”¨äºé‡åŒ– sequential æ¨¡å‹ï¼Œå…¶è¾“å…¥å‚æ•°å’Œ GPTQ ç±»ä¼¼

```python
def run_smooth_quant_for_sequential_layers(layers, 
                                           inputs, 
                                           cfg: SmoothQuantConfig):
```

1. layersï¼Œé€šå¸¸ä¸º transformer blocks (ModuleList or Sequential)

2. inputsï¼Œç”± `build_calib_datset_inputs` æ‰€æ„å»ºçš„ inputsï¼ŒåŒ…å« input hidden states & position embeddings

3. cfgï¼Œ`SmoothQuantConfig` æ˜¯ä¸€ä¸ªç®€å•çš„ dataclassï¼Œä»¥é…ç½® SmoothQuant/AWQ ç®—æ³•å‚æ•°

   ```python
   @dataclass
   class SmoothQuantConfig:
       w_method: str = "per-tensor"
       a_method: str = "per-tensor"
       q_linear_type: FakeQuantLinear = W8A8FakeLinear
       group_size: int = None
       duo_scale: bool = False
   ```

   å…¶ä¸­ `w_mehtod & a_method` ä½œä¸ºå‚æ•°ï¼Œç”¨äºå®ä¾‹åŒ–ä¸€ä¸ªå…·ä½“çš„ `FakeQuantLinear` ç±»æˆ–è€… fp8 linearï¼Œå¯é€‰çš„ç±»æœ‰ï¼š

   1. `W8A8FakeLinear`
   2. `W4A16FakeLinear`
   3. `FP8Linear`

### Other support

1. Custom op support

   ä¸ºäº†è®©é‡åŒ–çœŸæ­£å‘æŒ¥åŠ é€Ÿä½œç”¨ï¼Œéœ€è¦å®šåˆ¶çš„é‡åŒ–ç®—å­ã€‚åœ¨ FQT ä¸­é›†æˆäº† int8 & fp8 çš„ scaled mm å¯ç”¨äºåŠ é€Ÿ linear è®¡ç®—ã€‚å…¶ä¸­ fp8 scaled mm ç®—å­å·²ç»ä½¿ç”¨ `FP8Linear` è¿›è¡Œé›†æˆï¼Œ`Int8Linear` å¾…å¼€å‘

2. Logging

   FQT æ„å»ºäº†ä¸€ä¸ªç®€å•çš„ logger é…ç½®ï¼Œä¼šå°†é‡åŒ–è¿‡ç¨‹ä¸­çš„é‡åŒ–è¯¯å·®è¿›è¡Œè®°å½•ã€‚åªéœ€è¦å†ä»£ç è¿è¡Œå‰è°ƒç”¨ `setup_logging` å³å¯

   ```python
   from fqt.utils.logger import setup_logging
   setup_logging("path/to/quant.log")
   ```

3. IO

   FQT æ„å»ºäº†ç®€å•çš„ io ç³»ç»Ÿï¼Œå¯ä»¥é€šè¿‡ç»Ÿä¸€çš„ `load & dump` å¯¹ pickle & json & bin & pth æ–‡ä»¶è¿›è¡Œè¯»å–

   ```python
   from fqt.utils.io import load, dump
   ```

4. Visualization

   æä¾›äº†å¯¹äºŒç»´å¼ é‡å¯è§†åŒ–çš„è„šæœ¬ï¼Œå¯ä»¥é€šè¿‡ 3D surface ç›´è§‚æŸ¥çœ‹æ¿€æ´»åˆ†å¸ƒ

   ```python
   from fqt.utils.visualization import plot_3D_tensor
   plot_3D_tensor("layer_name", tensor, "save_name.png")
   ```

5. benchmark

   æä¾›äº†ä¸€äº›æµ‹é‡å»¶æ—¶å’Œç²¾åº¦çš„å°å·¥å…·

   ```python
   from fqt.utils.benchmark import benchmark, calculate_errors, Timer
   # a simple benchmark tool
   benchmark(func, inputs, num_warmup=10, num_repeats=100, desc="function")
   # check errors
   calculate_errors(actuals, predictions , desc="")
   # a context timer
   with Timer("name"):
       run()
   Timer.stats()
   ```

## User Guide

### Install

å¦‚æœä¸éœ€è¦ custom op supportï¼Œå³ä¸éœ€è¦ç¼–è¯‘ç®—å­ï¼Œé‚£ä¹ˆå¯ä»¥ç›´æ¥ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œå®‰è£…

```shell
ENABLE_NO_CSRC=1 pip install -e . --no-build-isolation --config-settings editable-mode=compat
```

å…¶ä¸­ `--config-settings` æ˜¯ä¸ºäº†è®© pylance èƒ½å¤Ÿåœ¨å…¶ä»–åº“ä¹Ÿè§£æåˆ° FQT

å¦‚æœè¦ç¼–è¯‘ç®—å­åˆ™ç›´æ¥å»æ‰å‰é¢çš„ç¯å¢ƒå˜é‡å³å¯

```shell
pip install -e . --no-build-isolation --config-settings editable-mode=compat
```

### Build calibration

æ„å»ºæ ¡å‡†é›†çš„æ–¹å¼å¯ä»¥ç›´æ¥ä½¿ç”¨ DataRecorder å°èŠ‚ä¸­çš„æœ€å…¸å‹ç”¨æ³•ã€‚è¯¥ç”¨æ³•èƒ½å¤Ÿä»¥è¾ƒå°çš„ä»£ä»·ï¼Œé€‚é…ä¸åŒçš„é¡¹ç›®å’Œæ¨¡å‹ï¼šå¤§å¤šæ•°é¡¹ç›®éƒ½ä¼šæä¾›æ¨¡å‹çš„è¯„æµ‹/æ¨ç†ä»£ç ï¼Œé€šè¿‡åœ¨è¯„æµ‹ä»£ç ä¸­æ’å…¥ `DataRecorder` å°±å¯ä»¥è·å¾—æ ¡å‡†é›†

```python
model = Qwen2ForCausalLM(config)

# create data recorder, set target module & saving device
dc = DataRecorder(model=model.model.layers[0],
                  device="cpu")

dc.register_input_hooks()		# register input hooks
dc.register_pos_embed_hooks()	# register pos emb hooks

for i, prompt in enumerate(dataset): # model evaluation loop
	model.generate(prompts)
    dc.save_inputs(f"{i}.pkl")	# save inputs
    dc.clear_inputs()			# clear inputs for next round conversation

dc.remove_input_hooks()			# remove hooks
```

### Quantize

åœ¨ FQT é¡¹ç›®ä¸­æä¾›äº†é‡åŒ– `Qwen2MoeForCausalLM` çš„è„šæœ¬ç¤ºä¾‹ `projects/quantize_moe.py`ï¼Œæ•´ä¸ªä»£ç ~30è¡Œï¼Œæ ¸å¿ƒä»£ç åªæœ‰ä¸‹æ–¹4è¡Œ

```python
# create dataset
dataset = DefaultDataset(root_dir=args.dataset_path)
# inputs for quantization algorithm
inputs = build_calib_datset_inputs(dataset, num_samples=num_samples, seed=seed, device=device)
# create model
model: Qwen2MoeForCausalLM = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(device)
# quantize and save
run_gptq_for_sequential_layers(model.model.layers, inputs, GPTQConfig())
```

### Compress Model

åœ¨ FQT é¡¹ç›®ä¸­æä¾›äº†å‹ç¼© `Qwen2ForCausalLM` ä¸º int4 æ¨¡å‹çš„è„šæœ¬ç¤ºä¾‹ `projects/compress_qwen.py`ï¼Œæ•´ä¸ªä»£ç  ~20è¡Œã€‚å…¶æ ¸å¿ƒåœ¨äºå°†æ¨¡å‹ä¸­çš„çº¿æ€§å±‚æ›¿æ¢ä¸º `QuantLinear`ï¼Œç„¶åè¿›è¡Œä¿å­˜ã€‚å¹¶ä¸”ä¸ºäº†è®© vLLM or SGLang è¿™æ ·çš„æ¡†æ¶èƒ½å¤Ÿä½¿ç”¨ï¼Œè¿˜éœ€è¦ä¿å­˜ç¬¦åˆ transformers åº“è§„èŒƒçš„é‡åŒ– config

```python
config = GPTQConfig(bits=4, group_size=128, method="per-block")

# replace with QuantLinear
replace_linear_with_custom(
    model.model.layers,
    QuantLinear,
    bits=config.bits,
    method=config.method,
    group_size=config.group_size
)

# add `quantization_config` key 
save_config = build_vllm_compat_gptq_config(config)
model.config.quantization_config = save_config
model.config.save_pretrained(qmodel_path)
```

## Future Work

1. w4a4 method integration
2. w4a16 & other fused kernel
3. int8/fp8 model compression
4. attention quantization
5. GPTQ static weight re-order