# SmoothQuant

[arxiv](https://arxiv.org/abs/2211.10438) [github](https://github.com/mit-han-lab/smoothquant)

smooth quant æ˜¯ç›®å‰ dynamic quantization çš„ä¸»æµæ–¹æ³•ï¼Œåœ¨ W8A8 ä¸Šèƒ½å¤Ÿæœ‰æ•ˆä¿ç•™æ¨¡å‹ç²¾åº¦ï¼Œä¹‹å‰åªæ˜¯å¬åˆ«äººè®²è§£è¿‡ï¼Œç°åœ¨éœ€è¦è‡ªå·±æ¥æ·±å…¥ç†è§£ä¸‹

## Paper

- SmoothQuant is a PTQ solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs

  ä¹‹å‰ä¸€ç›´ä»¥ä¸º smooth quant æ˜¯ä¸€ç§ online quantizationï¼Œçœ‹æ¥è¯¯ä¼šå¤§äº†

- Why it is hard to do activation quantization

  When we scale up LLMs beyond 6.7B parameters, systematic outliers with **large magnitude will emerge in activations** (Dettmers et al., 2022), leading to large quantization errors and accuracy degradation.

  ä¹‹å‰çš„æ–¹æ³•è¦ä¹ˆåªå¯¹å°æ¨¡å‹å¥æ•ˆï¼Œè¦ä¹ˆæ²¡æœ‰åŠ é€Ÿçš„ kernel å®ç°

- Key observation

  Even if activations are much harder to quantize than weights due to the presence of outliers (Dettmers et al., 2022), **different tokens exhibit similar variations across their channels.**

- Intuition of SmoothQuant

  <img src="SmoothQuant/image-20250108175203878.png" alt="image-20250108175203878" style="zoom:67%;" />

  SmoothQuant proposes a mathematically equivalent per-channel scaling transformation that significantly smooths the magnitude across the channels, making the model quantization-friendly

- Per-tensor & per-token & per-channel quantization

  æ‰€è°“é‡åŒ–å°±æ˜¯å°†é«˜æ¯”ç‰¹çš„æµ®ç‚¹å€¼å‹ç¼©ä¸ºä¸€ä¸ªä½æ¯”ç‰¹å€¼ï¼Œè¿™ä¸ªå€¼å¯ä»¥æ˜¯æµ®ç‚¹ä¹Ÿå¯ä»¥æ˜¯ intã€‚æˆ‘ä»¬é€šå¸¸åœ¨å®è·µä¸­ä½¿ç”¨çš„é‡åŒ–å…¬å¼å¦‚ä¸‹
  $$
  Q(W) = \DeltaÂ·\text{Round}(\frac{W}{\Delta}) \\ \Delta = \frac{\max{(|W|)}}{2^{N-1}}
  \\ \text{most cases we use: } \Delta = \frac{\max{(|W|)}}{2^{N-1}-1}
  $$
  å…¶ä¸­ $\Delta$ å°±æ˜¯æˆ‘ä»¬å¸¸è¯´çš„ scaleï¼Œè€Œ $Round(\frac{W}{\Delta})$ å°±æ˜¯æˆ‘ä»¬éœ€è¦ä¿å­˜çš„é‡åŒ–æƒé‡ã€‚æˆ‘ä»¬åœ¨è®¡ç®— $\Delta$ çš„æ—¶å€™æœ‰å¥½å‡ ç§ç²’åº¦ï¼Œè®ºæ–‡æœ‰æ¸…æ¥šçš„å›¾ç¤ºæ¥è¡¨ç¤ºè¿™ä¸‰ç§ç²’åº¦çš„é‡åŒ–æ–¹å¼
  
  <img src="SmoothQuant/image-20250110101442534.png" alt="image-20250110101442534" style="zoom:80%;" />
  
  - Per-tensor é‡åŒ–å°±æ˜¯æœ€ç®€å•ç²—æš´çš„ï¼Œä¸€ä¸ª tensor åªä½¿ç”¨ä¸€ä¸ª scale
  - Per-token é‡åŒ–é’ˆå¯¹äº X (actication) é‡åŒ–ï¼Œä¸€ä¸ª tensor æœ‰ `num_tokens` ä¸ª scale
  - Per-channel é‡åŒ–é’ˆå¯¹äº W (weight) é‡åŒ–ï¼Œä¸€ä¸ª tensor æœ‰ `num_out_channels` ä¸ª scale
  
  å…¶å®å¯ä»¥å°† per-token å’Œ per-channel ç»Ÿä¸€åœ°çœ‹å¾…ï¼šä¸€ä¸ª MNK çš„çŸ©é˜µä¹˜ï¼ŒX shape (M, K) W shape (N, K) ä»–ä»¬çš„ reduction dimension æ˜¯ K ç»´åº¦ï¼Œåœ¨é‡åŒ–çš„æ—¶å€™ç»Ÿè®¡ scale çš„ä¹Ÿæ˜¯ K ç»´åº¦
  
  åˆ©ç”¨ä¸‹é¢çš„å‡½æ•°å°±å¯ä»¥è®¡ç®—å¾—åˆ°å¯¹ç§°é‡åŒ–ï¼ˆper-tensor & per-channelï¼‰çš„ scale ç»“æœ
  
  ```python
      def find_params(self, x: torch.Tensor, weight=True):
          """ Args:
              - x: weight of linear, shaped in (N, K) i.e. (output_dim, input_dim)
              - weight: always True, this is for compatibility with Quantizer
          """
          if x.dim() == 3:
              # x should shaped in (N, K, group_size)
              assert x.shape[-1] == self.group_size
              # view back to 2D
              x = rearrange(x, "N G C -> N (G C)")
          
          assert x.dim() == 2
          out_dim , in_dim = x.shape
          group_size = self.group_size
  
          # reshape x to (out_dim, num_groups, group_size)
          grouped_x = x.view(out_dim, -1, group_size)
  
          # get scale
          max_abs = torch.max(torch.abs(grouped_x), dim=-1, keepdim=True)[0]
          self.scale = max_abs / self.max_q   # (out_dim, num_groups, 1)
  ```
  
- Dynamic quantization & static quantization

  è¿™åªé’ˆå¯¹äº activation çš„é‡åŒ–ã€‚weight æ˜¯ä¸€æˆä¸å˜çš„ï¼Œè€Œ activation æ˜¯å˜åŒ–çš„ã€‚è®ºæ–‡è§£é‡Šå¦‚ä¸‹

  > We can calculate âˆ† offline with the activations of some calibration samples, what we call static quantization. We can also use the runtime statistics of activations to get âˆ†, what we call dynamic quantization.

- ä¸ºä»€ä¹ˆé‡åŒ– activation æ˜¯å›°éš¾çš„ï¼Œé‡åŒ– weight æ˜¯ç®€å•çš„

  - weight distribution is quite uniform and flat, even with INT4 does not degrade accuracy

  - åœ¨ activation ä¸­å­˜åœ¨ç€ outliersï¼Œè¿™äº› outliers æ˜¯å…¶ä»–å€¼çš„ 100xï¼Œè¿™å°±å¯¼è‡´äº†å…¶ä»–å€¼çš„ç²¾åº¦ä¼šè¢«æŠ¹å»

  - Outliers persist in fixed channelsï¼Œè®ºæ–‡æŒ‡å‡º outliers ä¼šå‡ºç°åœ¨ä¸€å°éƒ¨åˆ†çš„ channel ä¸­ï¼Œå³ï¼šå¦‚æœæŸä¸ª channel æœ‰ oulierï¼Œé‚£ä¹ˆåœ¨æ‰€æœ‰ token çš„å¯¹åº” channel ä¸­ï¼Œä¼šæŒç»­å‡ºç° outlier

    <img src="SmoothQuant/image-20250110105447199.png" alt="image-20250110105447199" style="zoom:80%;" />

    è®ºæ–‡ä¹Ÿåšäº†å®éªŒï¼Œå¯¹æ¯”äº† per-token & per-channel å¯¹æ¨¡å‹ç²¾åº¦çš„å½±å“

    <img src="SmoothQuant/image-20250110105806716.png" alt="image-20250110105806716" style="zoom:80%;" />

    å¯ä»¥çœ‹åˆ° per-channel çš„ç¡®å‡ ä¹æ— é€€åŒ–ï¼Œä½†æ˜¯å…‰æœ‰ per-channel é‡åŒ–æ˜¯æ²¡åŠæ³•åš INT8 gemm æ¥è¾¾åˆ°åŠ é€Ÿç›®çš„çš„ğŸ˜å¿…é¡»è¦å°† activation ä¸€èµ·é‡åŒ–æ‰è¡Œ

- Migrate the quantization difficulty from activations to weights

  è¿™é‡Œå°±æ˜¯è®¨è®ºäº†ï¼Œå¦‚ä½•å°† activation çš„é‡åŒ–éš¾åº¦åˆ†ä¸€éƒ¨åˆ†ç»™ weightï¼Œä»è€Œæœ‰æ•ˆé™ä½é‡åŒ–è¯¯å·®
  $$
  Y=(XÂ·diag(s)^{-1})Â·(diag(s)W)=\hat{X}\hat{W}
  $$
  æ¥ä¸‹æ¥çš„é—®é¢˜å°±æ˜¯è¿™ä¸ª scale $s$ åº”è¯¥å¦‚ä½•è®¡ç®—äº†

  è®ºæ–‡æå‡ºäº†ä¸€ä¸ª migration strength $\alpha$ çš„è¶…å‚æ•°ï¼Œè¿™ä¸ªå½¢å¼ AWQ éƒ½æ²¡æœ‰è¿›è¡Œæ”¹å˜
  $$
  s_j = \frac{max(X_j)^{\alpha}}{max(W_j)^{1-\alpha}}
  $$

- Lossless performance

  ç»è¿‡å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†éªŒè¯ï¼ŒW8A8 çš„ SmoothQuant åŸºæœ¬ä¸Šèƒ½å¤Ÿåšåˆ°ç²¾åº¦æ— æŸæˆ–è€…å¾ˆä½çš„æŸå¤±

  <img src="SmoothQuant/image-20250113164059597.png" alt="image-20250113164059597" style="zoom:80%;" />

## Coding

- How to calculate W8A8

- è¾¾åˆ°è®ºæ–‡ä¸­å£°ç§°çš„ 1.5x speed upï¼Œå…¶ä¸­çš„é‡åŒ–å’Œåé‡åŒ–éƒ½æ˜¯åœ¨å“ªäº›åœ°æ–¹å®Œæˆçš„ï¼Ÿ

  åœ¨ä¸‹é¢è¿™å¼ å›¾ä¸­ï¼Œè¿ç»­çš„ int8 é‡åŒ–æ˜¯å¦æœ‰åé‡åŒ–çš„å‘ç”Ÿï¼Ÿå¦åˆ™æ˜¯å¦‚ä½•å®ç°ä»…æœ‰ INT8 çš„å­˜åœ¨çš„ï¼Œä»–ä»¬çš„ scales æ˜¯éœ€è¦è¿›è¡Œä¼ é€’çš„å—ï¼Ÿ

  <img src="SmoothQuant/image-20250211143734355.png" alt="image-20250211143734355" style="zoom:80%;" />

- å¦‚ä½•è¡¡é‡ç”±äº quant & dequant æ‰€å¸¦æ¥çš„é¢å¤–è®¡ç®—é‡ï¼Œåº”è¯¥æœ‰ä¸€ä¸ªä¸´ç•Œç‚¹ï¼Œå½“è®¡ç®—é‡å¾ˆå°çš„æ—¶å€™ï¼Œquant & dequant æ‰€å¸¦æ¥çš„æ”¶ç›Šå°†å°äº fp16 è®¡ç®—

- accumulator å¾ˆé‡è¦...ä¸å¯èƒ½ç”¨ int8 å»å­˜å‚¨ int8 gemm ä¸­çš„ä¸œè¥¿

## ImageNet

- Download dataset

  

## Question

- å°† activation çš„é‡åŒ–éš¾åº¦è½¬ç§»åˆ° weightï¼Œæœ‰ç‚¹åƒ AWQ çš„æ€è·¯ï¼Œå…¶æ˜¯æ ¹æ® activation çš„å¤§å°æ¥å¯¹æƒé‡è¿›è¡Œç¼©æ”¾ã€‚äºŒè€…ä¼¼ä¹æ˜¯å¹¶è¡Œçš„æ–¹æ³•ï¼Œå¦‚æœè¿›è¡Œå åŠ ä¼šæœ‰æ„ä¹‰å—ï¼Ÿ

- AWQ çš„æ€æƒ³åŸºæœ¬ä¸Šå°±æ˜¯ SmoothQuant åªç”¨åˆ°æƒé‡ä¸Šï¼Œä¸€ä¸ªå¥—è·¯è®²ä¸¤ä¸ªæ•…äº‹ï¼Œè¿™å¯çœŸèƒ½æ°´å•ŠğŸ¤£ä¸è¿‡å¥½æ¶ˆæ¯å°±æ˜¯ï¼Œæˆ‘èƒ½å¤Ÿç›´æ¥åº”ç”¨ AWQ çš„é‡åŒ–æ¡†æ¶æ¥è®¡ç®— SmoothQuant æ‰€éœ€è¦çš„é‡åŒ–å‚æ•°ï¼Œä¸éœ€è¦ä»»ä½•çš„æ”¹åŠ¨

  ä¸‹é¢çš„å…¬å¼æ˜¯ AWQ çš„é‡åŒ–è¯¯å·®ï¼Œå…¶ä¸­æ²¡æœ‰ activation çš„é‡åŒ–
  $$
  Err(Q(wÂ·s)\frac{x}{s}) = \DeltaÂ·\text{RoundErr}(\frac{wÂ·s}{\Delta})Â·xÂ·\frac{1}{s}
  $$

- ä¸ºä»€ä¹ˆ int8 éœ€è¦è®¡ç®—é‡æ¯”è¾ƒå¤§çš„æ—¶å€™æ‰èƒ½å‘æŒ¥ä¼˜åŠ¿ï¼Ÿèƒ½å¦é€šè¿‡å‚æ•°ç›´æ¥æ¨å¯¼å‡ºè¿™ä¸ªä¸´ç•Œè®¡ç®—é‡ï¼Ÿ

  ```shell
  # small amount of computation
  SEQ_LEN =  1024
  C1 =  896
  C2 =  896
  w8a8b8o8:
  Average inference time: 0.03092975997924805 ms
  fp16:
  Average inference time: 0.03977686309814453 ms
  
  # medium amount of computation
  SEQ_LEN =  1024
  C1 =  1024
  C2 =  4096
  w8a8b8o8:
  Average inference time: 0.07712290954589844 ms
  fp16:
  Average inference time: 0.17219020080566405 ms
  
  # large amount of computation
  SEQ_LEN =  1024
  C1 =  2048
  C2 =  8192
  w8a8b8o8:
  Average inference time: 0.2231982421875 ms
  fp16:
  Average inference time: 0.6151588134765625 ms
  ```

  ä»¥ä¸Šä¸º [torch-int](https://github.com/Guangxuan-Xiao/torch-int) åœ¨ 3080 ä¸Šçš„ profile ç»“æœï¼Œè¿™ä¸ªé¡¹ç›®å·²ç»æ˜¯3å¹´å‰çš„ repoï¼ˆnow I'm in 2025ï¼‰ï¼Œèƒ½å¤Ÿè·‘èµ·æ¥å·²ç»ä¸é”™äº†ï¼Œå¯èƒ½ç›®å‰æœ‰æ›´å¥½çš„ kernel è¿›è¡ŒåŠ é€Ÿã€‚ä¸è¿‡è¿™è¯å®äº†ç”¨ int8 åŠ é€Ÿ ViT-Large å®Œå…¨æ²¡é—®é¢˜çš„