# SmoothQuant

[arxiv](https://arxiv.org/abs/2211.10438) [github](https://github.com/mit-han-lab/smoothquant)

smooth quant æ˜¯ç›®å‰ dynamic quantization çš„ä¸»æµæ–¹æ³•ï¼Œåœ¨ W8A8 ä¸Šèƒ½å¤Ÿæœ‰æ•ˆä¿ç•™æ¨¡å‹ç²¾åº¦ï¼Œä¹‹å‰åªæ˜¯å¬åˆ«äººè®²è§£è¿‡ï¼Œç°åœ¨éœ€è¦è‡ªå·±æ¥æ·±å…¥ç†è§£ä¸‹

## Concept

- SmoothQuant is a PTQ solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs

  ä¹‹å‰ä¸€ç›´ä»¥ä¸º smooth quant æ˜¯ä¸€ç§ online quantizationï¼Œçœ‹æ¥è¯¯ä¼šå¤§äº†

- Why it is hard to do activation quantization

  When we scale up LLMs beyond 6.7B parameters, systematic outliers with **large magnitude will emerge in activations** (Dettmers et al., 2022), leading to large quantization errors and accuracy degradation.

  ä¹‹å‰çš„æ–¹æ³•è¦ä¹ˆåªå¯¹å°æ¨¡å‹å¥æ•ˆï¼Œè¦ä¹ˆæ²¡æœ‰åŠ é€Ÿçš„ kernel å®ç°

- Key observation

  Even if activations are much harder to quantize than weights due to the presence of outliers (Dettmers et al., 2022), **different tokens exhibit similar variations across their channels.**

- Intuition of SmoothQuant

  <img src="SmoothQuant/image-20250108175203878.png" alt="image-20250108175203878" style="zoom:67%;" />

  SmoothQuant proposes a mathematically equivalent per-channel scaling transformation that significantly smooths the magnitude across the channels, making the model quantization-friendly

- Per-tensor & per-token & per-channel quantization

  æ‰€è°“é‡åŒ–å°±æ˜¯å°†é«˜æ¯”ç‰¹çš„æµ®ç‚¹å€¼å‹ç¼©ä¸ºä¸€ä¸ªä½æ¯”ç‰¹å€¼ï¼Œè¿™ä¸ªå€¼å¯ä»¥æ˜¯æµ®ç‚¹ä¹Ÿå¯ä»¥æ˜¯ intã€‚æˆ‘ä»¬é€šå¸¸åœ¨å®è·µä¸­ä½¿ç”¨çš„é‡åŒ–å…¬å¼å¦‚ä¸‹
  $$
  Q(W) = \DeltaÂ·\text{Round}(\frac{W}{\Delta}) \\ \Delta = \frac{\max{(|W|)}}{2^{N-1}}
  $$
  å…¶ä¸­ $\Delta$ å°±æ˜¯æˆ‘ä»¬å¸¸è¯´çš„ scaleï¼Œè€Œ $Round(\frac{W}{\Delta})$ å°±æ˜¯æˆ‘ä»¬éœ€è¦ä¿å­˜çš„é‡åŒ–æƒé‡ã€‚æˆ‘ä»¬åœ¨è®¡ç®— $\Delta$ çš„æ—¶å€™æœ‰å¥½å‡ ç§ç²’åº¦ï¼Œè®ºæ–‡æœ‰æ¸…æ¥šçš„å›¾ç¤ºæ¥è¡¨ç¤ºè¿™ä¸‰ç§ç²’åº¦çš„é‡åŒ–æ–¹å¼
  
  <img src="SmoothQuant/image-20250110101442534.png" alt="image-20250110101442534" style="zoom:80%;" />
  
  - Per-tensor é‡åŒ–å°±æ˜¯æœ€ç®€å•ç²—æš´çš„ï¼Œä¸€ä¸ª tensor åªä½¿ç”¨ä¸€ä¸ª scale
  - Per-token é‡åŒ–é’ˆå¯¹äº X (actication) é‡åŒ–ï¼Œä¸€ä¸ª tensor æœ‰ `num_tokens` ä¸ª scale
  - Per-channel é‡åŒ–é’ˆå¯¹äº W (weight) é‡åŒ–ï¼Œä¸€ä¸ª tensor æœ‰ `num_out_channels` ä¸ª scale
  
  å…¶å®å¯ä»¥å°† per-token å’Œ per-channel ç»Ÿä¸€åœ°çœ‹å¾…ï¼šä¸€ä¸ª MNK çš„çŸ©é˜µä¹˜ï¼ŒX shape (M, K) W shape (N, K) ä»–ä»¬çš„ reduction dimension æ˜¯ K ç»´åº¦ï¼Œåœ¨é‡åŒ–çš„æ—¶å€™ç»Ÿè®¡ scale çš„ä¹Ÿæ˜¯ K ç»´åº¦
  
  åˆ©ç”¨ä¸‹é¢çš„å‡½æ•°å°±å¯ä»¥è®¡ç®—å¾—åˆ°å¯¹ç§°é‡åŒ–ï¼ˆper-tensor & per-channelï¼‰çš„ scale ç»“æœ
  
  ```python
      def find_params(self, x: torch.Tensor, weight=True):
          """ Args:
              - x: weight of linear, shaped in (N, K) i.e. (output_dim, input_dim)
              - weight: always True, this is for compatibility with Quantizer
          """
          if x.dim() == 3:
              # x should shaped in (N, K, group_size)
              assert x.shape[-1] == self.group_size
              # view back to 2D
              x = rearrange(x, "N G C -> N (G C)")
          
          assert x.dim() == 2
          out_dim , in_dim = x.shape
          group_size = self.group_size
  
          # reshape x to (out_dim, num_groups, group_size)
          grouped_x = x.view(out_dim, -1, group_size)
  
          # get scale
          max_abs = torch.max(torch.abs(grouped_x), dim=-1, keepdim=True)[0]
          self.scale = max_abs / self.max_q   # (out_dim, num_groups, 1)
  ```
  
- Dynamic quantization & static quantization

  è¿™åªé’ˆå¯¹äº activation çš„é‡åŒ–ã€‚weight æ˜¯ä¸€æˆä¸å˜çš„ï¼Œè€Œ activation æ˜¯å˜åŒ–çš„ã€‚è®ºæ–‡è§£é‡Šå¦‚ä¸‹

  > We can calculate âˆ† offline with the activations of some calibration samples, what we call static quantization. We can also use the runtime statistics of activations to get âˆ†, what we call dynamic quantization.

- ä¸ºä»€ä¹ˆé‡åŒ– activation æ˜¯å›°éš¾çš„ï¼Œé‡åŒ– weight æ˜¯ç®€å•çš„

  - weight distribution is quite uniform and flat, even with INT4 does not degrade accuracy

  - åœ¨ activation ä¸­å­˜åœ¨ç€ outliersï¼Œè¿™äº› outliers æ˜¯å…¶ä»–å€¼çš„ 100xï¼Œè¿™å°±å¯¼è‡´äº†å…¶ä»–å€¼çš„ç²¾åº¦ä¼šè¢«æŠ¹å»

  - Outliers persist in fixed channelsï¼Œè®ºæ–‡æŒ‡å‡º outliers ä¼šå‡ºç°åœ¨ä¸€å°éƒ¨åˆ†çš„ channel ä¸­ï¼Œå³ï¼šå¦‚æœæŸä¸ª channel æœ‰ oulierï¼Œé‚£ä¹ˆåœ¨æ‰€æœ‰ token çš„å¯¹åº” channel ä¸­ï¼Œä¼šæŒç»­å‡ºç° outlier

    <img src="SmoothQuant/image-20250110105447199.png" alt="image-20250110105447199" style="zoom:80%;" />

    è®ºæ–‡ä¹Ÿåšäº†å®éªŒï¼Œå¯¹æ¯”äº† per-token & per-channel å¯¹æ¨¡å‹ç²¾åº¦çš„å½±å“

    <img src="SmoothQuant/image-20250110105806716.png" alt="image-20250110105806716" style="zoom:80%;" />

    å¯ä»¥çœ‹åˆ° per-channel çš„ç¡®å‡ ä¹æ— é€€åŒ–ï¼Œä½†æ˜¯å…‰æœ‰ per-channel é‡åŒ–æ˜¯æ²¡åŠæ³•åš INT8 gemm æ¥è¾¾åˆ°åŠ é€Ÿç›®çš„çš„ğŸ˜å¿…é¡»è¦å°† activation ä¸€èµ·é‡åŒ–æ‰è¡Œ

- Migrate the quantization difficulty from activations to weights

  è¿™é‡Œå°±æ˜¯è®¨è®ºäº†ï¼Œå¦‚ä½•å°† activation çš„é‡åŒ–éš¾åº¦åˆ†ä¸€éƒ¨åˆ†ç»™ weightï¼Œä»è€Œæœ‰æ•ˆé™ä½é‡åŒ–è¯¯å·®
  $$
  Y=(XÂ·diag(s)^{-1})Â·(diag(s)W)=\hat{X}\hat{W}
  $$
  æ¥ä¸‹æ¥çš„é—®é¢˜å°±æ˜¯è¿™ä¸ª scale $s$ åº”è¯¥å¦‚ä½•è®¡ç®—äº†

  è®ºæ–‡æå‡ºäº†ä¸€ä¸ª migration strength $\alpha$ çš„è¶…å‚æ•°ï¼Œè¿™ä¸ªå½¢å¼ AWQ éƒ½æ²¡æœ‰è¿›è¡Œæ”¹å˜
  $$
  s_j = \frac{max(X_j)^{\alpha}}{max(W_j)^{1-\alpha}}
  $$

- Lossless performance

  ç»è¿‡å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†éªŒè¯ï¼ŒW8A8 çš„ SmoothQuant åŸºæœ¬ä¸Šèƒ½å¤Ÿåšåˆ°ç²¾åº¦æ— æŸæˆ–è€…å¾ˆä½çš„æŸå¤±

  <img src="SmoothQuant/image-20250113164059597.png" alt="image-20250113164059597" style="zoom:80%;" />

## Question

- å°† activation çš„é‡åŒ–éš¾åº¦è½¬ç§»åˆ° weightï¼Œæœ‰ç‚¹åƒ AWQ çš„æ€è·¯ï¼Œå…¶æ˜¯æ ¹æ® activation çš„å¤§å°æ¥å¯¹æƒé‡è¿›è¡Œç¼©æ”¾ã€‚äºŒè€…ä¼¼ä¹æ˜¯å¹¶è¡Œçš„æ–¹æ³•ï¼Œå¦‚æœè¿›è¡Œå åŠ ä¼šæœ‰æ„ä¹‰å—ï¼Ÿ

- AWQ çš„æ€æƒ³åŸºæœ¬ä¸Šå°±æ˜¯ SmoothQuant åªç”¨åˆ°æƒé‡ä¸Šï¼Œä¸€ä¸ªå¥—è·¯è®²ä¸¤ä¸ªæ•…äº‹ï¼Œè¿™å¯çœŸèƒ½æ°´å•ŠğŸ¤£ä¸è¿‡å¥½æ¶ˆæ¯å°±æ˜¯ï¼Œæˆ‘èƒ½å¤Ÿç›´æ¥åº”ç”¨ AWQ çš„é‡åŒ–æ¡†æ¶æ¥è®¡ç®— SmoothQuant æ‰€éœ€è¦çš„é‡åŒ–å‚æ•°ï¼Œä¸éœ€è¦ä»»ä½•çš„æ”¹åŠ¨

  ä¸‹é¢çš„å…¬å¼æ˜¯ AWQ çš„é‡åŒ–è¯¯å·®ï¼Œå…¶ä¸­æ²¡æœ‰ activation çš„é‡åŒ–
  $$
  Err(Q(wÂ·s)\frac{x}{s}) = \DeltaÂ·\text{RoundErr}(\frac{wÂ·s}{\Delta})Â·xÂ·\frac{1}{s}
  $$
  