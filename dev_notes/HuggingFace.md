# Hugging Face

[Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/chapter1/1) å­¦ä¹ ç¬”è®°ï¼Œè¿™ä¸ªè¯¾ç¨‹æ¯”è¾ƒè€äº†ï¼Œå¤§æ¦‚æ›´æ–°åˆ°äº† 2021 å¹´çš„ä¸€äº›æ¨¡å‹

## ç¬¬ä¸€ç«  Intro

1. è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡

   - **Classifying sentences or words in sentences**
   - **Generating text content**
   - **Extracting an answer from a text**
   - **Generating a new sentence from an input text**ï¼Œæœ€åä¸€ä¸ªä»»åŠ¡å°±æ¯”è¾ƒå®½æ³›äº†ï¼ŒåŒ…å« chat, translate ç­‰

2. `pipeline` in transformers

   èƒ½å¤Ÿåˆ›å»ºä¸€ä¸ª NLP ä»»åŠ¡æµç¨‹ï¼ŒåŒ…å«é¢„å¤„ç†ï¼Œæ¨¡å‹å‰å‘è·¯å¾„ï¼Œåå¤„ç†ã€‚åšåˆ°è¾“å…¥æ–‡å­—ï¼Œè¿”å›è¾“å‡º

   ![image-20240125141156830](HuggingFace/image-20240125141156830.png)

3. Transformer -2021 çš„ä¸€äº›é‡è¦æ¨¡å‹

   - GPT series, auto-regressive transformer models
   - BERT & DistillBERT, auto-encoding transformer models
   - BART & T5, sequence-to-sequence transformer models

   ä¼¼ä¹ç°åœ¨æ˜¯ GPT åœ¨å¼•é¢†æ½®æµ

## ç¬¬äºŒç«  Transformers Lib

transformers æ˜¯ huggingface å¼€æºçš„ç¬¬ä¸€ä¸ªåº“ï¼Œä¹Ÿæ˜¯å…¶æœ€å—æ¬¢è¿çš„åº“ï¼Œåœ¨ github ä¸Šå·²ç»æ”¶è·äº† 100k â­ï¸

å…¶åˆ›å»ºçš„ç›®çš„æ˜¯ä¸ºäº†è§£å†³å¦‚ä¸‹é—®é¢˜ï¼š1. è®­ç»ƒå’Œä½¿ç”¨å¼€æºæ¨¡å‹å›°éš¾ï¼›2. ç›¸åŒæ¨¡å‹åœ¨å„ä¸ªæ¡†æ¶ä¸‹å®ç°ä¸åŒã€‚ä¸ºäº†è§£å†³è¿™ä¸¤ä¸ªéš¾é¢˜ï¼Œhuggingface transformers çš„è®¾è®¡å°±æœ‰å¦‚ä¸‹ç‰¹æ€§ï¼š

1. Ease of useï¼Œèƒ½å¤Ÿè½»æ¾ä¸‹è½½å’Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
2. Flexibilityï¼Œä¸ä½¿ç”¨å…¶ä»–åŸºç¡€åº“ï¼Œç”±åŸºæœ¬çš„ pytorch ä»£ç ç»„æˆ
3. Simplicityï¼Œæ²¡æœ‰ä»»ä½•çš„æŠ½è±¡å’Œç»„ä»¶ï¼Œæ‰€æœ‰çš„æ¨¡å‹éƒ½åœ¨ä¸€ä¸ªæ–‡ä»¶é‡Œï¼ŒçœŸæ­£åšåˆ°äº† **all in one file**ï¼Œä¸€ä¸ªæ–‡ä»¶å³ä¸€ä¸ªäº§å“ã€‚è¿™ä¸€ç‚¹çœŸçš„å¾ˆæ£’ï¼Œå¦‚æœä½ çŸ¥é“æˆ‘åœ¨è¯´å“ªå®¶çš„æŠ½è±¡å¾ˆå·®çš„è¯ğŸ¤¨

ä¸‹è½½ transformer library

```shell
pip install transformers
```

### Tokenizer

åŸºæœ¬ä¸Šè¿™ä¸€æ•´ä¸ªç« èŠ‚éƒ½æ˜¯åœ¨è®² tokenizer çš„äº‹æƒ…

tokenizer å®é™…ä¸Šå°±æ˜¯é¢„å¤„ç†ï¼Œå°†æ–‡å­—è½¬åŒ–ä¸ºæ•°å­—

æ‰€è°“ tokenï¼Œç‹­éš˜çš„ç†è§£å°±æ˜¯ sub-wordï¼Œè€Œ vocabulary å°±æ˜¯ä¸€å¼ å¤§è¡¨ï¼Œåœ¨è¿™ä¸ªè¡¨ä¸Šæ¯ä¸€ä¸ª sub-word éƒ½æœ‰è‡ªå·± idï¼Œè¿™ä¸ª id ä¹Ÿå« token idã€‚è€Œ tokenizer å°±æ˜¯è¿™ä¸ªè¿‡ç¨‹ï¼šword -> sub-word -> token id

tokenizer éœ€è¦åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶ä¿æŒä¸€è‡´ï¼Œæ‰€ä»¥å¯ä»¥é€šè¿‡ `.from_pretrained(checkpoint)` æ–¹æ³•è·å¾—è®­ç»ƒä¿¡æ¯

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
```

`AutoTokenizer` çš„ `Auto` åœ¨äºè¯†åˆ« checkpoint ä¸­çš„æ¨¡å‹ç±»åˆ«ï¼Œç„¶åå†ç”Ÿæˆå¯¹åº”çš„ tokenizerï¼Œä¾‹å¦‚ `BertTokenizer`

é™¤äº† `AutoTokenizer` ä¹‹å¤–ï¼Œtransformers è¿˜æä¾› `AutoModel`ï¼Œä¹Ÿå¯ä»¥é€šè¿‡å­—ç¬¦ä¸²çš„æ–¹å¼è·å¾—é¢„è®­ç»ƒæ¨¡å‹

```python
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)

outputs = model(**inputs)
print(outputs.last_hidden_state.shape)
# torch.Size([2, 16, 768])
```

ä½†éœ€è¦æ³¨æ„çš„æ˜¯ `AutoModel` çš„è¾“å‡ºåªæ˜¯ logtisï¼Œä¹Ÿå°±æ˜¯æ²¡æœ‰ç»è¿‡é¢„æµ‹å¤´çš„è¾“å‡ºã€‚å¦‚æœæƒ³è¦ä½¿ç”¨é¢„æµ‹å¤´ï¼Œè¿˜å¾—ä½¿ç”¨å…·ä½“çš„ `Autoxxx`

```python
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
# torch.Size([2, 2])
```

åœ¨ transformers ä¸­ä¹Ÿå¯ä»¥ä½¿ç”¨é…ç½®æ¥åˆ›å»ºæ¨¡å‹ï¼Œæ¯ä¸€ä¸ªæ¨¡å‹éƒ½æœ‰ä¸€ä¸ªè‡ªå·±çš„é…ç½®ç±»

```python
from transformers import BertConfig, BertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = BertModel(config)

# Building from pretrain
model = BertModel.from_pretrained("bert-base-cased")
```

é…ç½®ç±»åŒ…å«äº†å¾ˆå¤šåˆ›å»ºæ¨¡å‹çš„å±æ€§

åœ¨ transformers ä¸­ä¿å­˜æ¨¡å‹å°†ç”Ÿæˆä¸¤ä¸ªæ–‡ä»¶

```python
model.save_pretrained("dir_path")
```

ä¸€ä¸ªæ˜¯ `config.json` å…¶ä¿å­˜äº†æ¨¡å‹çš„ç»“æ„ç›¸å…³çš„å±æ€§ï¼Œå¦ä¸€ä¸ªå°±æ˜¯ `pytorch_model.bin` å…¶ä¿å­˜äº†æ¨¡å‹æƒé‡ï¼Œç›¸å½“äº `state_dict`

åˆ†è¯ï¼ˆtokenizationï¼‰

1. word-based tokenization

   è‹±æ–‡å•è¯çš„æ•°é‡æœ‰ 5000,000 å¤šä¸ªï¼Œè¿™å°†æ˜¯ä¸€ä¸ªæ¯”è¾ƒå¤§æŸ¥æ‰¾è¡¨ã€‚å¹¶ä¸”æŒ‰ç…§ word åˆ’åˆ†ä¹Ÿæ— æ³•è·å¾—è¯ä¸è¯ä¹‹é—´çš„å…±æ€§ï¼Œä¾‹å¦‚ dog å’Œ dogs å°†åˆ†ä¸ºä¸¤ä¸ªè¯

2. character-based tokenization

   è™½ç„¶è¿™æ ·åˆ†è¯æ˜¯æœ€ç®€å•çš„ï¼Œæ¯•ç«Ÿåªæœ‰26ä¸ªå­—æ¯ï¼Œä½†è¿™ä¼šè®©æ¯ä¸€ä¸ª token çš„æ„ä¹‰æ²¡æœ‰é‚£ä¹ˆæ˜æ˜¾

3. subword tokenization

   å‰ä¸¤è€…çš„ç»“åˆ

tokenizer çš„åŠ è½½å’Œä¿å­˜ä¸æ¨¡å‹ä¸€æ ·ï¼Œä¿å­˜ä¸‹ config & weightï¼Œè¿™é‡Œçš„ weight ä¹Ÿè¢«ç§°ä¸º vocabularyï¼Œå°†æ–‡å­—è½¬åŒ–ä¸º token id

```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
tokenizer.save_pretrained("dir_path")
```

tokenizer çš„ä½¿ç”¨æ–¹æ³•

1. `tokenizer(sequence, return_tensors='pt')`ï¼Œå°†æ–‡å­—è½¬ä¸º batched token ids
2. `tokenizer.tokenize(sequence)`ï¼Œå°†æ–‡å­—è½¬ä¸º token ids
3. `tokenizer.decode(sequence)`ï¼Œå°† token ids è½¬ä¸ºæ–‡å­—

tokenizer èƒ½å¤Ÿå¤„ç† batched sequencesï¼Œå¹¶ä¸”æ¨¡å‹ä¹Ÿå¿…é¡»æ¥å— batched inputsã€‚ä¸ºäº†å¤„ç† batched sequencesï¼Œéœ€è¦å¯¹ä¸åŒé•¿åº¦çš„åºåˆ—è¿›è¡Œ paddingï¼Œå¹¶ä¸”ä½¿ç”¨ attention mask æ¥æ§åˆ¶æ³¨æ„åŠ›èŒƒå›´

ä¸€ä¸ªå®Œæ•´çš„è°ƒç”¨ä»£ç 

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)
```

## ç¬¬ä¸‰ç«  Finetune a Pretrained Model

Loading a dataest from hub

å®‰è£… datasets åº“

```shell
pip install datasets
```

ä»¥  MRPC (Microsoft Research Paraphrase Corpus) dataset ä¸ºä¾‹

cache åœ°å€å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡ `HF_HOME` æ¥è®¾ç½®ï¼Œå¦åˆ™å°†å­˜å‚¨åˆ°é»˜è®¤çš„ `~/.cache/huggingface`

åŠ è½½æ•°æ®é›†

```python
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")

# DatasetDict({
#     train: Dataset({
#         features: ['sentence1', 'sentence2', 'label', 'idx'],
#         num_rows: 3668
#     })
#     validation: Dataset({
#         features: ['sentence1', 'sentence2', 'label', 'idx'],
#         num_rows: 408
#     })
#     test: Dataset({
#         features: ['sentence1', 'sentence2', 'label', 'idx'],
#         num_rows: 1725
#     })
# })
```

è¿™ä¸ª raw dataset æ˜¯ä¸€ä¸ª `DatasetDict` ç±»ï¼Œä»¥ train split ä¸ºä¾‹ï¼šè¯¥æ•°æ®é›†çš„ train split ä¸€å…±æœ‰ 3668 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸€ä¸ªæ ·æœ¬çš„ç‰¹å¾æœ‰4ä¸ªï¼Œåˆ†åˆ«æ˜¯ `sentence1 & sentence2 & label & idx`

å’Œ pytorch dataset ä¸€æ ·ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ index æ¥è·å¾—æ¯ä¸€ä¸ª itemã€‚ä¸ Pytorch dataset ä¸ä¸€æ ·çš„æ˜¯è¿˜å¯ä»¥é€šè¿‡ dict çš„æ–¹å¼æ¥è·å¾—æ•´ä¸ªæ•°æ®çš„æŸä¸ªç‰¹å¾

```python
raw_train_dataset = raw_datasets["train"]
sentence1 = raw_train_dataset["sentence1"]
print(type(sentence1), len(sentence1))
# <class 'list'> 3668
```

dataset æä¾›äº† `map` æ–¹æ³•æ¥å¯¹ samples è¿›è¡Œé¢å¤–çš„å¤„ç†

```python
def tokenize_function(example):
    # tokenize your example, or just use tokenizer
    return tokenized_example

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
```

ç›¸å½“äºæ‰€æœ‰çš„æ ·æœ¬éƒ½ç»è¿‡äº† `tokenize_function` çš„å¤„ç†ï¼Œå¹¶ä¸”ä½¿ç”¨ batched å‚æ•°å¯ä»¥å¹¶è¡Œå¤„ç†ï¼Œèµ·åˆ°åŠ é€Ÿçš„æ•ˆæœã€‚tokenizer ä½¿ç”¨ Tokenizer libraryï¼Œæ˜¯ç”¨ Rust å®ç°çš„ï¼Œæ‰€ä»¥å·²ç»ä½¿ç”¨äº†å¤šçº¿ç¨‹è¿›è¡ŒåŠ é€Ÿ

dynamic padding: å¯¹æ¯ä¸€ä¸ª batchï¼Œéƒ½ pad åˆ°å½“å‰ batch ä¸­æœ€é•¿çš„é•¿åº¦ã€‚ç›¸å¯¹çš„ï¼Œnot dynamic padding æ˜¯æ‰€æœ‰æ ·æœ¬ padding åˆ°æŒ‡å®šé•¿åº¦ï¼Œé€šå¸¸ä¸ºæ•´ä¸ªæ•°æ®é›†ä¸­æœ€é•¿çš„æ ·æœ¬

å®ç° dynamic padding æ˜¯ç”± `DataCollatorWithPadding` å®ç°ï¼Œå®é™…ç­‰ä»·äº pytorch dataloader ä¸­çš„ collate function

```python
from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

samples = tokenized_datasets["train"][:8]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}

batch = data_collator(samples)
print({k: v.shape for k, v in batch.items()})
# {'input_ids': torch.Size([8, 67]),
#  'token_type_ids': torch.Size([8, 67]),
#  'attention_mask': torch.Size([8, 67]),
#  'labels': torch.Size([8])}
```

Transformers æä¾›äº†ä¸€ä¸ª `Trainer` ç±»æ¥åšç®€å•æ¨¡å‹è®­ç»ƒï¼Œè¿™é‡Œä¸åšè¿‡å¤šä»‹ç»ï¼Œæ¯•ç«Ÿè¿™ä¸æ˜¯è¯¥åº“çš„æ ¸å¿ƒ

## ç¬¬å››ç«  Huggingface Hub

huggingface model hubï¼Œæ¯ä¸€ä¸ªæ¨¡å‹åœ¨ huggingface éƒ½æˆç«‹ä¸€ä¸ª github repo å¯ç”¨äºç‰ˆæœ¬æ§åˆ¶å’Œä¸‹è½½ï¼Œè¿™ä¸ª Repo ä¹Ÿå«åš model card

huggingface è¿˜æ˜¯æ¨èä¸Šé¢ä»‹ç»çš„æ–¹å¼ `.from_pretrained` çš„æ–¹æ³•æ¥è·å¾—æ¨¡å‹ï¼Œè€Œè¿™ä¹Ÿæ˜¯å¤§å¤šæ•° model card åšçš„ï¼Œå¯ä»¥æ‰“å¼€ä¸€ä¸ª [CodeLlma](https://huggingface.co/codellama/CodeLlama-70b-Instruct-hf) æ¥çœ‹ä¸‹

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

import torch

model_id = "codellama/CodeLlama-70b-Instruct-hf"

tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
   model_id,
   torch_dtype=torch.float16,
   device_map="auto",
)
```

æŠŠæ¨¡å‹æ¨é€åˆ° huggingface ä¸Šï¼Œåªéœ€è¦ä½¿ç”¨ `.push_to_hub` å³å¯ï¼Œå‰ææ˜¯ä½ æœ‰ huggingface è´¦å·ï¼Œè´¦å·ç™»å½•æœ‰ä¸¤ç§æ–¹æ³•

```python
# first
from huggingface_hub import notebook_login

notebook_login()

# commandline
huggingface-cli login
```

ç™»å½•è¿‡åä½¿ç”¨ api å³å¯

```python
model.push_to_hub("dummy-model")
tokenizer.push_to_hub("dummy-model")
tokenizer.push_to_hub("dummy-model", organization="huggingface")
tokenizer.push_to_hub("dummy-model", organization="huggingface", use_auth_token="<TOKEN>")M
```

Model Card ç•Œé¢æ˜¯ç”± README.md ç”Ÿæˆçš„ï¼Œå¸Œæœ›ä½ èƒ½åŒ…å«å¦‚ä¸‹å†…å®¹ï¼š

- Model description
- Intended uses & limitations
- How to use
- Limitations and bias
- Training data
- Training procedure
- Evaluation results

è¿™æ ·æ‰èƒ½ä½¿å¾—æ¨¡å‹å…·æœ‰æ˜“ç”¨æ€§ï¼Œå¹¶ä¸”å¸®åŠ©åˆ«äººå¤ç°ä½ çš„å·¥ä½œ

## ç¬¬äº”ç«  Datasets Lib

datasets ç”¨äºå¤„ç†è¡¨æ ¼æ•°æ®éå¸¸æ–¹ä¾¿ï¼Œä¾‹å¦‚ä»¥ä¸‹æ ¼å¼ï¼š

| Data format        | Loading script | Example                                                 |
| ------------------ | -------------- | ------------------------------------------------------- |
| CSV & TSV          | `csv`          | `load_dataset("csv", data_files="my_file.csv")`         |
| Text files         | `text`         | `load_dataset("text", data_files="my_file.txt")`        |
| JSON & JSON Lines  | `json`         | `load_dataset("json", data_files="my_file.jsonl")`      |
| Pickled DataFrames | `pandas`       | `load_dataset("pandas", data_files="my_dataframe.pkl")` |

æ—¢å¯ä»¥ä»æœ¬åœ°ï¼Œä¹Ÿå¯ä»¥ä» urlï¼›æ—¢å¯ä»¥ load ä¸€ä¸ªï¼Œä¹Ÿå¯ä»¥ load å¤šä¸ª split

```python
from datasets import load_dataset

squad_it_dataset = load_dataset("json", data_files="SQuAD_it-train.json", field="data")

data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
squad_it_dataset

url = "https://github.com/crux82/squad-it/raw/master/"
data_files = {
    "train": url + "SQuAD_it-train.json.gz",
    "test": url + "SQuAD_it-test.json.gz",
}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

æ¥ä¸‹æ¥ä»‹ç»äº†ä¸‹ datasets çš„å¸¸è§„åŠŸèƒ½

1. filter
2. **map**
3. sort

å…¶ä¸­ map æ˜¯æœ€é‡è¦çš„åŠŸèƒ½ï¼Œè¿˜å¯ä»¥ä½¿ç”¨ `batched=True` æ¥å¼€å¯å¤šçº¿ç¨‹åŠ é€Ÿï¼Œé…åˆ fast tokenizer é€Ÿåº¦æ›´å¿«ï¼Œä½†ä»€ä¹ˆæ˜¯ fast tokenizer è¿˜æ²¡ç»†è¯´ã€‚è¿˜å¯ä»¥æ˜¯ç”¨ `num_proc` æ¥å¼€å¯å¤šè¿›ç¨‹ï¼Œè¿›ä¸€æ­¥åŠ é€Ÿ

å¯ä»¥ä½¿ç”¨ `datasets.save_to_disk & to_csv & to_json` æ¥ä¿å­˜æ•°æ®é›†

`save_to_dist` å°†é‡‡ç”¨ arrow formatï¼Œä»€ä¹ˆæ˜¯ arrow format ä¹Ÿä¸æ¸…æ¥š



å¯¹äºè¶…å¤§å‹çš„æ•°æ®é›†ï¼Œæ˜¯ä¸å¯èƒ½å°†æ‰€æœ‰æ•°æ®è½½å…¥åˆ°å†…å­˜å½“ä¸­çš„ï¼Œhuggingface dataset å¯ä»¥é€šè¿‡ memory-map çš„æ–¹å¼è§£å†³ï¼Œä½¿ç”¨ memory-map å°±éœ€è¦ä½¿ç”¨ arrow format

memory-map è¡¨ç¤ºåœ¨ RAM å’Œ filesystem storage ä¹‹é—´æä¾›ä¸€ä¸ªæ˜ å°„ï¼Œè¿™æ ·å°±èƒ½è·å–å’Œæ“ä½œæ•°æ®é›†ï¼Œä»è€Œé¿å…å®Œå…¨å°†æ•°æ®é›†è£…å…¥å†…å­˜

huggingface è¿˜æä¾›äº† streaming datasetsï¼Œè¿™æ ·å°±ä¸éœ€è¦å°†æ•°æ®é›†å®Œæ•´åœ°ä¸‹è½½åˆ°æœ¬åœ°ï¼Œè€Œæ˜¯å®æ—¶è·å–ï¼Œéšå–éšç”¨



ä¸‹é¢ä¸¤ç« èŠ‚ä½¿ç”¨äº† request å»çˆ¬ä¸€äº› github issues ä½œä¸ºæ•°æ®ï¼Œç®€å•åˆ›å»ºäº†ä¸€ä¸ªæ•°æ®é›†ï¼Œç„¶åä½¿ç”¨ä¸€ä¸ª auto tokenizer & model æ¥è¿›è¡Œæ–‡å­—ç¼–ç ï¼Œä½¿ç”¨ FAISS ç®—æ³•æ¥è¿›è¡Œè¯­ä¹‰æœç´¢

## ç¬¬å…­ç«  Tokenizer Lib

å½“ corpus (è¯­æ–™åº“) ä¹‹é—´çš„å·®å¼‚å¤ªå¤§äº†ï¼Œå¯¹åº”çš„ tokenizer æ˜¾ç„¶å·®å¼‚ä¹Ÿå¾ˆå¤§ï¼Œä¾‹å¦‚è‹±æ–‡è¯­æ–™åº“å’Œæ—¥æ–‡è¯­æ–™åº“ï¼Œé‡Œé¢å¯¹åº”çš„åˆ†è¯è‚¯å®šä¸ä¸€æ ·

ä¸ºäº†è®©æ‰€æœ‰äººéƒ½çœ‹æ‡‚ï¼Œè¿™ä¸€ç« ä»¥ English & Python ä¸¤ä¸ªè¯­è¨€ä½œä¸ºå¯¹æ¯”

ç›´æ¥æ‹¿ GPT2 çš„ tokenizer è¿‡æ¥è®­

```python
from transformers import AutoTokenizer

old_tokenizer = AutoTokenizer.from_pretrained("gpt2")
```

tokenizer åˆ°åº•åŒ…å«ä»€ä¹ˆå‘¢ï¼Ÿä¸ºä»€ä¹ˆéœ€è¦è®­ç»ƒï¼Ÿå¯èƒ½å®ƒçš„å‚æ•°ä¸åƒç¥ç»ç½‘ç»œçš„æƒé‡ä¸€æ ·ï¼Œä½†ä¹Ÿæ˜¯éœ€è¦è®­ç»ƒçš„

å¦‚æœç”¨ GPT2 çš„ tokenzier å»å¯¹ Python code è¿›è¡Œåˆ†è¯ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ

```python
example = '''def add_numbers(a, b):
    """Add the two numbers `a` and `b`."""
    return a + b'''

tokens = old_tokenizer.tokenize(example)
print(tokens)

['def', 'Ä add', '_', 'n', 'umbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä """', 'Add', 'Ä the', 'Ä two',
 'Ä numbers', 'Ä `', 'a', '`', 'Ä and', 'Ä `', 'b', '`', '."', '""', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']
```

å…¶ä¸­çš„ C, G éƒ½æ˜¯ä»£è¡¨äº†ç©ºæ ¼å’Œæ¢è¡Œã€‚å¯ä»¥çœ‹åˆ°åˆ†è¯ç»“æœéå¸¸å·®ï¼ŒæŠŠ number éƒ½åˆ†æˆäº† n å’Œ umberï¼ŒåŒæ—¶ç©ºæ ¼éå¸¸å¤š

å¯ä»¥é€šè¿‡ä¸‹é¢çš„æ¥å£è®­ç»ƒ

```python
tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)
```

è®­ç»ƒä¹‹åçš„åˆ†è¯ç»“æœ

```python
['def', 'Ä add', '_', 'numbers', '(', 'a', ',', 'Ä b', '):', 'ÄŠÄ Ä Ä ', 'Ä """', 'Add', 'Ä the', 'Ä two', 'Ä numbers', 'Ä `',
 'a', '`', 'Ä and', 'Ä `', 'b', '`."""', 'ÄŠÄ Ä Ä ', 'Ä return', 'Ä a', 'Ä +', 'Ä b']
```

å¯ä»¥çœ‹åˆ°æŠŠ indent éƒ½åˆåœ¨ä¸€èµ·äº†ï¼Œnumbers ä¹Ÿæ­£å¸¸äº†

è®­ç»ƒå®Œè¿‡åè¿˜æ˜¯é€šè¿‡ `.save_pretrained` æ¥å£ä¿å­˜ tokenizer



tokenizer çš„è¾“å‡ºä¸æ˜¯æ™®é€šçš„ python dictï¼Œè€Œæ˜¯ä¸€ä¸ª `BatchEncoding` å®ä¾‹ï¼Œè¯¥ç±»æœ‰ä¸€äº›æ–¹æ³•

1. `is_fast`ï¼ŒæŸ¥çœ‹æ˜¯å¦æ˜¯ fast tokenizer

2. `tokens()`ï¼ŒæŸ¥çœ‹æ‰€æœ‰çš„åˆ†è¯ç»“æœ

   ```python
   ['[CLS]', 'My', 'name', 'is', 'S', '##yl', '##va', '##in', 'and', 'I', 'work', 'at', 'Hu', '##gging', 'Face', 'in',
    'Brooklyn', '.', '[SEP]']
   ```

3. `word_ids()`ï¼Œä¸æ˜¯ token idsï¼Œæ˜¯ç”¨äºæŸ¥çœ‹ token åœ¨è¯¥åºåˆ—å½“ä¸­çš„å“ªä¸ªå•è¯ï¼Œé€šå¸¸æŸ¥çœ‹è¿™ä¸ªå•è¯æ˜¯ä¸æ˜¯é¦–ä½ï¼Œæˆ–è€…ä¸¤ä¸ªå•è¯æ˜¯å¦è¿ç»­

   ```python
   [None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]
   ```

4. `sentence_ids()`ï¼ŒæŸ¥çœ‹ token å±äºå“ªä¸ª sentence



`offset_mapping` é€šå¸¸ç”¨äºæŸ¥æ‰¾ token ä¸åŸæ–‡æœ¬çš„å¯¹åº”ï¼Œåœ¨ NER (Name Entity Recognition) or QA (Question Answering) ä¸­ç»å¸¸ä½¿ç”¨

` "Hello, world!"` çš„ token å¦‚æœç»“æœä¸º `["Hello", ",", "world", "!"]`,  `offset_mapping` å¤§æ¦‚å°±æ˜¯è¿™æ · `[(0, 5), (5, 6), (7, 12), (12, 13)]`ï¼Œå¯èƒ½è¿˜è¦è€ƒè™‘ special tokenï¼Œä¾‹å¦‚ CLS tokenï¼Œè¿™äº› token çš„ offset mapping å°±æ˜¯æ²¡æœ‰æ„ä¹‰çš„ï¼Œä¾‹å¦‚ `(0,0)`

é•¿å¥å­è¿™é‡Œæä¾›äº†ä¸€ä¸ªç¬¨åŠæ³•ï¼šä½¿ç”¨ä¸€ä¸ªæ»‘åŠ¨çª—å£ï¼Œå»å•ç‹¬å¤„ç†æ¯ä¸€ä¸ªçª—å£



å¸¸è§çš„åˆ†è¯æ–¹æ³•ï¼š

1. BPEï¼ŒByte Pari Encoding
2. WordPiece
3. Unigram



normalizationï¼Œä¸€èˆ¬æ˜¯ preprocess çš„å…¶ä¸­ä¸€ä¸ªè¿‡ç¨‹ï¼ŒåŒ…å«å‡ ä¸ªå¯èƒ½æ“ä½œï¼š

1. Lowercasingï¼Œè½¬æ¢å¤§å°å†™
2. Unicode normalizationï¼Œå…¨éƒ¨ä½¿ç”¨åŒä¸€ä¸ªç¼–ç 
3. Removing accentsï¼Œå»é™¤å£éŸ³ã€å£°è°ƒ
4. Expanding contractionsï¼Œå±•å¼€ä¸€äº›ç¼©å†™ï¼Œä¾‹å¦‚ don't å±•å¼€ä¸º do not

pre-tokenizationï¼Œå°±æ˜¯å°† input åˆ†æˆä¸€ä¸ª words listï¼Œè¿™é‡Œè¿˜ä¸æ˜¯ token

tokenze algorithmï¼Œè¿›ä¸€æ­¥å°† word åˆ†ä¸º sub-wordï¼Œè¿™é‡Œå°±æ˜¯ token äº†ï¼Œä¸€èˆ¬ä½¿ç”¨ BPE è¿™äº›ç®—æ³•

postprocessorï¼ŒåŠ å…¥ä¸€äº›ç‰¹æ®Š tokenï¼Œä¾‹å¦‚ CLS, SEP ç­‰ç­‰

![image-20240201160243336](HuggingFace/image-20240201160243336.png)

## Question

1. huggingface ä¸Šçš„æ¨¡å‹åŸºæœ¬ä¸Šéƒ½éœ€è¦ VPNï¼Œä¸‹è½½æ¨¡å‹å’Œæ•°æ®é›†éƒ½ä¸æ–¹ä¾¿ï¼Œå¦‚æœåªæ˜¯å­¦ä¹ çš„è¯å¯ä»¥é€‰æ‹©ä½¿ç”¨ Colabï¼Œèƒ½å¤Ÿåšä¸€äº›ç®€å•çš„æµ‹è¯•
2. Dataset çš„å¤šè¿›ç¨‹æ˜¯å¦‚ä½•å®ç°çš„ï¼Ÿ
