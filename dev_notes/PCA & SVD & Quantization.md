# PCA & SVD & Quantization

From Linear Algebra to the Essence of Eigen

最近在研究一些量化问题，最后回到了对 SVD 的本质理解上。在学习过程中对 SVD 所展现的一些性质能够理解，但是对特征向量的本质仍然无法理解：

1. 什么是特征向量？
2. 为什么会存在特征向量？
3. 特征向量与基之间有什么关系？
4. 特征向量在实际应用中会有什么用处？

能否从特征向量出发，完成对 SVD 的代数&几何理解。然后从 PCA 出发，发现 SVD 与特征的实际含义。最后再利用 SVD 的特性来说明其在模型量化当中的应用 [SVDQuant](https://arxiv.org/abs/2411.05007)

参考材料：[科学空间](https://www.spaces.ac.cn/archives/10407) [白板机器学习](https://www.bilibili.com/video/BV1aE411o7qd) [线性代数的本质](https://www.3blue1brown.com/topics/linear-algebra)

## 线性代数的本质

在 3b1b 的视频中，对特征值和特征向量的介绍放在了非常靠后的章节。其开头就说了：如果你不理解特征值和特征向量，估计就是对线性代数中的基本概念不理解。所以，我应该需要把他之前章节中的核心进行掌握，再来看特征值和特征向量的本质

> "There is hardly any theory wnich is more elementary than linear algebra, in spite of the fact that generations of professors and textbook writers have obscured its simplicity by preposterous calculations with matrices."	- Jean Dieudonné

对于这种成熟的概念以及总结性质的问题，交给 AI 来做再好不过了，而且好消息是这些教程不仅仅以视频的形式存在，还被整理成为了网站，我只需要把网站交给 Kimi 就好了！

### Vectors, what even are they?

**向量的三种视角**

| 视角               | 定义                             | 特点                                                |
| ------------------ | -------------------------------- | --------------------------------------------------- |
| **物理视角**       | 空间中的箭头                     | 由长度和方向定义，可平移，二维或三维                |
| **计算机科学视角** | 有序的数字列表                   | 每个位置代表一个维度，顺序很重要，如 `[面积, 价格]` |
| **数学视角**       | 任何可以进行“加法”和“数乘”的对象 | 抽象，强调运算规则而非具体形式                      |

线性代数的核心在于**向量加法**和**数乘**这两种操作。无论是将向量看作空间中的箭头，还是数字列表，真正重要的是**在这两种视角之间自由切换**的能力：

- 对数据分析者：将高维数据可视化，发现模式。
- 对物理学家或图形程序员：用数字精确描述空间与变换。

另外注意到：在教程中的向量都是竖着表示的

### Linear combination, span and basis vectors

1. **基向量（Basis Vectors）**

   - **标准基**：二维空间中，单位向量 î（x 方向）和 ĵ（y 方向）构成标准基。

   - **广义的基**：任何两个**不共线**的向量都可以作为一组基，构建新的坐标系。

   - **基的用途**：通过缩放基向量再相加，可以表示平面中的任意向量。

2. **线性组合（Linear Combination）**

   - 定义：对两个向量 **v** 和 **w**，任意实数 a 和 b，表达式 **a·v + b·w** 就是一个线性组合。

   - 几何意义：线性组合是通过缩放两个向量再相加，得到新的向量。

3. **张成空间（Span）**

   - 定义：一组向量的**所有可能线性组合**构成的集合，称为这些向量的**张成空间**。

4. **线性相关与线性无关**

   - **线性相关**：一组向量中，至少有一个向量可以被其他向量的线性组合表示（即“多余”）。
   - **线性无关**：每个向量都贡献了一个新的维度，移除任何一个都会减少张成空间的维度。

5. **空间的基向量**

   一组向量构成一个空间的**基**，当且仅当：

   1. 它们是**线性无关**的；
   2. 它们的**张成空间是整个空间**。

### Linear transformations and matrices

Space is fixed, it is the ultimate playground, what can be changed is the basis and vector

Keep in mind, it is what we do to the vector, not what we do to the space

1. **什么是线性变换？**

   - **变换**本质上是函数，输入向量，输出另一个向量。

   - **线性变换**在视觉上满足两个条件：
     1. 所有直线变换后仍是直线（不弯曲）；
     2. 原点保持不动（即零向量映射到零向量）。

2. **矩阵与线性变换的关系**

   **矩阵的列**就是基向量变换后的坐标

$$
\begin{bmatrix}
  a & b \\
  c & d
  \end{bmatrix}
  \begin{bmatrix}
  x \\
  y
  \end{bmatrix}
  = x \begin{bmatrix}
  a \\
  c
  \end{bmatrix} + y \begin{bmatrix}
  b \\
  d
  \end{bmatrix}
  = \begin{bmatrix}
  ax + by \\
  cx + dy
  \end{bmatrix}
$$

3. **线性变换的数学性质**

   **线性变换**满足两个数学性质： 

   1. 可加性：$ T(u + v) = T(u) + T(v) $
   2. 齐次性：$ T(ku) = kT(u) $

### Matrix multiplication as composition

1. 矩阵乘法的几何意义：变换的复合

   连续施加两个线性变换（例如先旋转再剪切）的结果，仍然是一个线性变换。

2. 重要性质

   **非交换性**：矩阵乘法不满足交换律，\(AB \neq BA\)，因为变换的顺序会影响最终结果。

   **结合性**：矩阵乘法满足结合律，\((AB)C = A(BC)\)，从变换的角度看是显然的：无论怎么加括号，变换的顺序始终是 C → B → A。

### The determinant

1. **行列式的几何意义：面积/体积的缩放因子**

   行列式衡量一个线性变换对空间**面积（2D）或体积（3D）**的缩放程度。我认为这里的“空间”应当理解为“有限空间”，而不是理解为整个 space。举一个例子，有限空间可以是 2D space 中的一个单位正方形，其面积为1，我们将该单位正方形中的所有向量（点）进行线性变换 $A$ 过后，会形成一个新的区域。该区域的面积就是行列式的大小，即其面积变为 $\det(A)$

2. **行列式为 0 的含义**

   如果行列式为 0，说明整个空间被压缩到更低维度（如 2D 压缩成一条线，或 3D 压缩成一个平面甚至点）。这也意味着矩阵的列向量是**线性相关**的。

   **负行列式的含义：空间方向被翻转**

3. **矩阵乘积的行列式**

   两个矩阵相乘后，其行列式等于各自行列式的乘积：
   $$
   \det(M_1 M_2) = \det(M_1) \cdot \det(M_2)
   $$
   这是因为行列式本质上是变换对空间的**缩放因子**，连续变换的缩放因子等于各步缩放因子的乘积。

### Inverse matrices, column space, and null space

1. **线性方程组的几何意义**

    一个线性方程组可以表示为矩阵与向量的乘法：  
   $$
   Ax=v
   $$
   其中 A 是系数矩阵，x 是未知向量，v 是常数向量。**几何上，这相当于寻找一个向量 x，使得线性变换 A 将其映射到向量 v。**

2. **逆矩阵（Inverse Matrix）**

   当变换 A 没有“压缩空间”（即行列式不为零），A 是可逆的，存在逆矩阵 A⁻¹。解方程组的方法：**x = A⁻¹ v**，即“倒带”变换，找到原向量 x。

3. **不可逆的情况（行列式为零）**

   如果 A 的行列式为零，意味着它将空间压缩到更低维度（如平面压缩成线）。此时 A 没有逆矩阵，因为无法从压缩后的结果恢复原始空间。 但方程组仍可能有解，只要向量 v 落在 A 的“输出空间”中。

4. **列空间（Column Space）与秩（rank）**

   列空间是矩阵 A 的所有可能输出向量构成的空间。它的维度称为“秩（rank）”，表示变换后空间的维度。如果 v 不在列空间中，方程组无解。（个人理解：列空间其实就代表了解空间）

5. **零空间（Null Space 或 Kernel）**

   零空间是所有被 A 映射到零向量的输入向量构成的集合。用线性方程组来表示，零空间的向量满足
   $$
   Ax=0
   $$
   如果 A 是“压缩”变换，零空间是非零的，表示多个输入对应同一个输出。反之，如果 A 是一个满秩矩阵，那么仅有全零的情况满足

附注：Nonsquare matrices as transformations between dimensions，非方阵用于维度之间的转换（2D <-> 3D）

### Dot Product

在这一小节中探讨了点积与投机之间的关系，从对偶性和可视化的角度描述了二者之间的等价。不过我觉得这还是不够本质。我在思考的过程中，最终需要得到结论：投影是一种线性变换，这样才能够比较通畅地理解。所以对于这一节的整理就不按照 3b1b 的视频来了

要理解点积和投影之间的关系，首先我们需要对二者进行定义

1. 什么是点积

   点积的定义非常简单，就是两个向量的 element wise 乘积之和
   $$
   a·b = \sum_i a_ib_i
   $$

2. 什么是投影

   对于投影的定义相对来说比较难，最终会需要对面积进行理解？从比较简单的代数角度，投影代表了一个向量 $a$ 在另一个向量 $b$ 上的分量，并按照 $b$ 的模进行缩放
   $$
   proj(a,b)=\lVert a\rVert \lVert b \rVert \cos{\theta}
   $$

3. 投影与点积

   在这里我们似乎还没有将投影与向量之间显示地联系起来，其中的关键在于**模的定义**。模的定义（在此情形下）就是将几何与代数进行联系的桥梁
   $$
   \lVert a\rVert=\sqrt{\sum_i a_i^2}
   $$
   这个公式异常的平常，以至于我忽略了其重要性。通过对模的定义，我们完全可以推到出投影的向量化形式

   根据平面几何的余弦定理
   $$
   c^2 = a^2 + b^2 − 2ab \cos θ
   $$
   余弦定理的证明由 [Euclid's Element](https://en.wikipedia.org/wiki/Law_of_cosines) 欧几里得几何原本中已经得到了证明，没有使用任何向量化语言。

   在向量的语言中用 $a,b$ 来表示向量 $c$ 非常简单（三角形法则），其模的计算用向量化形式表示为
   $$
   \rVert c \lVert = \lVert a-b\rVert = \sqrt{\sum_i (a_i-b_i)^2}=\sqrt{a^2+b^2-2a·b}
   $$
   我用了点积的定义来简化了上述式子的表达。把式子带入到余弦定理当中就可得到
   $$
   a·b =\lVert a\rVert \lVert b \rVert \cos{\theta}
   $$
   虽然我用一些简单定理将投影与点积的等价关系进行了证明，但是这些简单定理的证明也显得没有那么普通了。例如，为什么向量 $c = a - b$ 为什么成立？为什么向量的长度就是几何平均？不过由于这些结论足够平常，我们可以将其作为公理进行看待（虽然他们并不是），最后会或许深入到线性空间的定义

4. **投影是一种线性变换**

   现在更进一步，用线性变换的角度来看待投影。我们可以证明投影是一种线性变换，对向量 $x$ 投影到向量 $a$ 上
   $$
   proj_a(x) = x·a
   $$
   可以很容易证明该变换的可加性和齐次性
   $$
   proj_a(x_1+x_2)=proj_a(x_1)+proj_a(x_2)\\
   proj_a(k·x)=k·proj_a(x)
   $$
   既然投影是一种线性变换，而矩阵即可描述一个线性变换，我们就可以从线性变换的角度来重新审视投影。此时投影矩阵 $a$ 是一个“倒下”的向量，即一个一维矩阵。该矩阵其实是一个降维变换，把向量全部压缩到一维空间当中。最终的结果是 $a$ 中各个值的线性组合，我把其看做在各个坐标分量的线性组合
   $$
   a = (a_1,a_2) = (a_1, 0)+(0, a_2)\\
   x·a = x·(a_1,0)+x·(0,a_2) = x_1a_1 + x_2a_2
   $$
   相当于**将投影分解成为了在各个轴上的分量之和**，以线性组合的角度重新结构了投影

### Change of Basis

其实早在第二节的时候就引入过基向量和标准基的概念了，在这一节讨论了基的变换，并且简单引出了相似变换的本质意义

1. 问题引出

   对于同一个向量，使用不同的基向量表示也不相同。例如，使用标准基来表示 $(1, 0)$ 向量，需要只需要 1 个 $\hat i$ 向量，其坐标也是 $(1, 0)$；使用基向量 $(1, 1)$ & $(1, -1)$ 来表示，则需要两个基向量的一半，其坐标表示为 $(0.5, 0.5)$

2. 基变换矩阵

   基变换矩阵 $A$ 是一个矩阵，**其列向量代表新坐标系的基向量在当前坐标系中的坐标。**（个人觉得）不失一般性当前坐标系可认为是标准基构成的坐标系

   **用途**：用于将向量从**新坐标系**转换到**当前坐标系**。反之，则需要基变换矩阵的逆矩阵

3. 基变换矩阵与相似变换

   考虑如何在新坐标系下描述一个旋转变换。在新坐标系下的向量 $x$，首先将其转变为标准基下的向量表示，然后再使用旋转矩阵，最后再用逆矩阵把向量还原为新坐标系下的向量表示
   $$
   \text{Rotation in New Basis}=A^{-1}MA
   $$
   中间的矩阵 $M$ 表示在当前坐标系中的旋转变换，其可以延伸为任意的线性变换。此时理解相似变换就变得更加容易了：即为线性变换 $M$ 在不同坐标系下的矩阵表示，所以称为相似

   > An expression like $A^{-1}MA$ suggests a mathmatical sort of empathy
   >
   > 表达式 $A^{-1}MA$ 暗示着一种数学上的转移作用







## SVD

- 引理1：谱定理

  谱定理的几何直觉：

- 引理2：正交变换不改变矩阵范数

  直观理解很容易：旋转不改变长度，只改变方向

- 证明：SVD 的存在性

- 应用1：低秩近似，简易自编码器（最小重构代价）

- 应用2：PCA 降维（最大投影方差）

  理解在机器学习的世界里：什么是特征值？什么是特征向量？

# Question

- 为什么实对称矩阵一定可以被对角化？这其中有没有什么深层次的原因？

  询问了 Kimi & DeepSeek，二者都给出了两个过程：

  1. 证明实对称矩阵可对角化的三个步骤

  2. 引出更一般的谱定理（[Spectral Theorem](https://en.wikipedia.org/wiki/Spectral_theorem)）

     **谱定理**大致指出：**自伴算子（或矩阵）可以在某个标准正交基下被对角化**。所以，实对称矩阵可对角化是谱定理的一个直接推论。谱定理是泛函分析、量子力学等领域的基石，它将算子（或矩阵）的“结构”完全由其“谱”（即特征值的集合）来描述。

  这确实很神奇🧐