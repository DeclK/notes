# PCA & SVD & Quantization

From Linear Algebra to the Essence of Eigen

最近在研究一些量化问题，最后回到了对 SVD 的本质理解上。在学习过程中对 SVD 所展现的一些性质能够理解，但是对特征向量的本质仍然无法理解：

1. 什么是特征向量？
2. 为什么会存在特征向量？
3. 特征向量与基之间有什么关系？
4. 特征向量在实际应用中会有什么用处？

能否从特征向量出发，完成对 SVD 的代数&几何理解。然后从 PCA 出发，发现 SVD 与特征的实际含义。最后再利用 SVD 的特性来说明其在模型量化当中的应用 [SVDQuant](https://arxiv.org/abs/2411.05007)

参考材料：[科学空间](https://www.spaces.ac.cn/archives/10407) [白板机器学习](https://www.bilibili.com/video/BV1aE411o7qd) [线性代数的本质](https://www.3blue1brown.com/topics/linear-algebra)

## 线性代数的本质

在 3b1b 的视频中，对特征值和特征向量的介绍放在了非常靠后的章节。其开头就说了：如果你不理解特征值和特征向量，估计就是对线性代数中的基本概念不理解。所以，我应该需要把他之前章节中的核心进行掌握，再来看特征值和特征向量的本质

> "There is hardly any theory wnich is more elementary than linear algebra, in spite of the fact that generations of professors and textbook writers have obscured its simplicity by preposterous calculations with matrices."	- Jean Dieudonné

对于这种成熟的概念以及总结性质的问题，交给 AI 来做再好不过了，而且好消息是这些教程不仅仅以视频的形式存在，还被整理成为了网站，我只需要把网站交给 Kimi 就好了！

### Vectors, what even are they?

**向量的三种视角**

| 视角               | 定义                             | 特点                                                |
| ------------------ | -------------------------------- | --------------------------------------------------- |
| **物理视角**       | 空间中的箭头                     | 由长度和方向定义，可平移，二维或三维                |
| **计算机科学视角** | 有序的数字列表                   | 每个位置代表一个维度，顺序很重要，如 `[面积, 价格]` |
| **数学视角**       | 任何可以进行“加法”和“数乘”的对象 | 抽象，强调运算规则而非具体形式                      |

线性代数的核心在于**向量加法**和**数乘**这两种操作。无论是将向量看作空间中的箭头，还是数字列表，真正重要的是**在这两种视角之间自由切换**的能力：

- 对数据分析者：将高维数据可视化，发现模式。
- 对物理学家或图形程序员：用数字精确描述空间与变换。

另外注意到：在教程中的向量都是竖着表示的

### Linear combination, span and basis vectors

1. **基向量（Basis Vectors）**

   - **标准基**：二维空间中，单位向量 î（x 方向）和 ĵ（y 方向）构成标准基。

   - **广义的基**：任何两个**不共线**的向量都可以作为一组基，构建新的坐标系。

   - **基的用途**：通过缩放基向量再相加，可以表示平面中的任意向量。

2. **线性组合（Linear Combination）**

   - 定义：对两个向量 **v** 和 **w**，任意实数 a 和 b，表达式 **a·v + b·w** 就是一个线性组合。

   - 几何意义：线性组合是通过缩放两个向量再相加，得到新的向量。

3. **张成空间（Span）**

   - 定义：一组向量的**所有可能线性组合**构成的集合，称为这些向量的**张成空间**。

4. **线性相关与线性无关**

   - **线性相关**：一组向量中，至少有一个向量可以被其他向量的线性组合表示（即“多余”）。
   - **线性无关**：每个向量都贡献了一个新的维度，移除任何一个都会减少张成空间的维度。

5. **空间的基向量**

   一组向量构成一个空间的**基**，当且仅当：

   1. 它们是**线性无关**的；
   2. 它们的**张成空间是整个空间**。

### Linear transformations and matrices

Space is fixed, it is the ultimate playground, what can be changed is the basis and vector

Keep in mind, it is what we do to the vector, not what we do to the space

1. **什么是线性变换？**

   - **变换**本质上是函数，输入向量，输出另一个向量。

   - **线性变换**在视觉上满足两个条件：
     1. 所有直线变换后仍是直线（不弯曲）；
     2. 原点保持不动（即零向量映射到零向量）。

2. **矩阵与线性变换的关系**

   **矩阵的列**就是基向量变换后的坐标

$$
\begin{bmatrix}
  a & b \\
  c & d
  \end{bmatrix}
  \begin{bmatrix}
  x \\
  y
  \end{bmatrix}
  = x \begin{bmatrix}
  a \\
  c
  \end{bmatrix} + y \begin{bmatrix}
  b \\
  d
  \end{bmatrix}
  = \begin{bmatrix}
  ax + by \\
  cx + dy
  \end{bmatrix}
$$

3. **线性变换的数学性质**

   **线性变换**满足两个数学性质： 

   1. 可加性：$ T(u + v) = T(u) + T(v) $
   2. 齐次性：$ T(ku) = kT(u) $

### Matrix multiplication as composition

1. 矩阵乘法的几何意义：变换的复合

   连续施加两个线性变换（例如先旋转再剪切）的结果，仍然是一个线性变换。

2. 重要性质

   **非交换性**：矩阵乘法不满足交换律，\(AB \neq BA\)，因为变换的顺序会影响最终结果。

   **结合性**：矩阵乘法满足结合律，\((AB)C = A(BC)\)，从变换的角度看是显然的：无论怎么加括号，变换的顺序始终是 C → B → A。

### The determinant

1. **行列式的几何意义：面积/体积的缩放因子**

   行列式衡量一个线性变换对空间**面积（2D）或体积（3D）**的缩放程度。我认为这里的“空间”应当理解为“有限空间”，而不是理解为整个 space。举一个例子，有限空间可以是 2D space 中的一个单位正方形，其面积为1，我们将该单位正方形中的所有向量（点）进行线性变换 $A$ 过后，会形成一个新的区域。该区域的面积就是行列式的大小，即其面积变为 $\det(A)$

2. **行列式为 0 的含义**

   如果行列式为 0，说明整个空间被压缩到更低维度（如 2D 压缩成一条线，或 3D 压缩成一个平面甚至点）。这也意味着矩阵的列向量是**线性相关**的。

   **负行列式的含义：空间方向被翻转**

3. **矩阵乘积的行列式**

   两个矩阵相乘后，其行列式等于各自行列式的乘积：
   $$
   \det(M_1 M_2) = \det(M_1) \cdot \det(M_2)
   $$
   这是因为行列式本质上是变换对空间的**缩放因子**，连续变换的缩放因子等于各步缩放因子的乘积。

### Inverse matrices, column space, and null space

1. **线性方程组的几何意义**

    一个线性方程组可以表示为矩阵与向量的乘法：  
   $$
   Ax=v
   $$
   其中 A 是系数矩阵，x 是未知向量，v 是常数向量。**几何上，这相当于寻找一个向量 x，使得线性变换 A 将其映射到向量 v。**

2. **逆矩阵（Inverse Matrix）**

   当变换 A 没有“压缩空间”（即行列式不为零），A 是可逆的，存在逆矩阵 A⁻¹。解方程组的方法：**x = A⁻¹ v**，即“倒带”变换，找到原向量 x。

3. **不可逆的情况（行列式为零）**

   如果 A 的行列式为零，意味着它将空间压缩到更低维度（如平面压缩成线）。此时 A 没有逆矩阵，因为无法从压缩后的结果恢复原始空间。 但方程组仍可能有解，只要向量 v 落在 A 的“输出空间”中。

4. **列空间（Column Space）与秩（rank）**

   列空间是矩阵 A 的所有可能输出向量构成的空间。它的维度称为“秩（rank）”，表示变换后空间的维度。如果 v 不在列空间中，方程组无解。（个人理解：列空间其实就代表了解空间）

5. **零空间（Null Space 或 Kernel）**

   零空间是所有被 A 映射到零向量的输入向量构成的集合。用线性方程组来表示，零空间的向量满足
   $$
   Ax=0
   $$
   如果 A 是“压缩”变换，零空间是非零的，表示多个输入对应同一个输出。反之，如果 A 是一个满秩矩阵，那么仅有全零的情况满足

附注：Nonsquare matrices as transformations between dimensions，非方阵用于维度之间的转换（2D <-> 3D）