# Denoising Diffusion Probabilistic Models

[DDPM arxiv](https://arxiv.org/pdf/2006.11239)

[Understanding Diffusion Models: A Unified Perspective arxiv](https://arxiv.org/pdf/2208.11970)

[bilibili-æ‰©æ•£æ¨¡å‹ - Diffusion Modelã€æå®æ¯…2023ã€‘](https://www.bilibili.com/video/BV14c411J7f2) [PPT1](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/DiffusionModel%20(v2).pdf) [PPT2](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/StableDiffusion%20(v2).pdf) [PPT3](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/DDPM%20(v7).pdf) [course page](https://speech.ee.ntu.edu.tw/~hylee/ml/2023-spring.php)

æœ¬æ–‡æ¡£æ ¹æ®æå®æ¯…è§†é¢‘æ•´ç†å¾—åˆ°ï¼Œå¸Œæœ›å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

1. How to train and inference a diffusion model
2. What are the basic mathmatics behind it
3. How SOTA models improve the original diffusion model
4. What are the insights of diffusion can give us

## Notaions in Probability

- $x\sim p(Â·)$ ä»£è¡¨ç€ $x$ æ˜¯ä¸€ä¸ªéšæœºå˜é‡æœä»äºæ¦‚ç‡åˆ†å¸ƒ $p$ï¼Œåœ¨è®ºæ–‡ä¸­ï¼Œä¹Ÿå¸¸å¸¸ç”¨è¯¥ notation æ¥è¡¨ç¤ºåœ¨ $p$ åˆ†å¸ƒä¸­é‡‡æ · $x$
- $E_{x\sim p(x)}$ ä»£è¡¨ç€æ±‚è§£å˜é‡ $x$ çš„æœŸæœ›ï¼Œå¹¶ä¸”è¯¥å˜é‡æœä»æ¦‚ç‡åˆ†å¸ƒ $p(x)$ï¼Œæœ‰æ—¶å€™ä¹Ÿä¼šç”¨ $E_{p(x)}$â€‹ æ¥ç®€å†™è¯¥è¿‡ç¨‹
- $N(x;\mu, \sigma^2)$ æ¥è¡¨ç¤ºä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œå…¶ä¸­éšæœºå˜é‡ä¸º $x$ï¼Œå…¶å‡å€¼å’Œæ–¹å·®åˆ†åˆ«ä¸º $\mu, \sigma^2$ï¼Œæœ‰æ—¶å€™ä¹Ÿä¼šçœç•¥æ‰ $x$ï¼Œç›´æ¥å†™ä½œ $N(\mu, \sigma^2)$ã€‚å¯¹äºä¸€ä¸ªå¤šç»´é«˜æ–¯åˆ†å¸ƒä½¿ç”¨å¦‚ä¸‹ notation: $N(x;\mu,\Sigma)$ï¼Œå…¶ä¸­ $x,\mu$ éƒ½æ˜¯å¤šç»´å‘é‡ï¼Œè€Œ $\Sigma$ ä¸ºåæ–¹å·®çŸ©é˜µ (covariance matrix)

## An intuitive perspective of DDPM

<img src="Denoising Diffusion Probabilistic Models/image-20241206102950079.png" alt="image-20241206102950079" style="zoom:67%;" />

çœ‹å›¾è¯´è¯ï¼šDDPM å…¶å®å°±æ˜¯ä¸€ä¸ªä»éšæœºå™ªå£°å¼€å§‹ï¼Œä¸æ–­å»å™ªç›´åˆ°ç”Ÿæˆç›®æ ‡å¯¹è±¡çš„è¿‡ç¨‹ã€‚é‚£åˆ°åº•ä¼šç”Ÿæˆä»€ä¹ˆæ ·çš„å›¾åƒå‘¢ï¼Ÿæ ¹æ®ä¸Šé¢çš„ç¤ºæ„å›¾ï¼Œæˆ‘ä»¬å…¶å®å¹¶æ²¡æœ‰ç»™å»å™ªæ¨¡å‹ä»»ä½•çš„ä¿¡æ¯ï¼Œåªç»™äº†ä¸€ä¸ªåˆå§‹å™ªå£°å°±å¼€å§‹è®©å…¶è¿›è¡Œå»å™ªï¼Œæ‰€ä»¥ç†è®ºä¸Šä¼šç”Ÿæˆä»»æ„çš„å›¾åƒã€‚è€Œç°åœ¨ diffusion model åœ¨ text-to-image é¢†åŸŸåº”ç”¨éå¸¸å¤šï¼Œæ‰€ä»¥å¦‚æœæƒ³è¦ç”ŸæˆæŒ‡å®šçš„å›¾åƒï¼Œæˆ‘ä»¬éœ€è¦ç»™ diffusion model åŠ å…¥é¢å¤–çš„ä¿¡æ¯ï¼Œä¾‹å¦‚æ–‡å­—ï¼š

<img src="Denoising Diffusion Probabilistic Models/image-20241206161019438.png" alt="image-20241206161019438" style="zoom: 67%;" />

## How to inference DDPM

- Algorithm

  <img src="Denoising Diffusion Probabilistic Models/image-20241205155625447.png" alt="image-20241205155625447" style="zoom:80%;" />

  Notation of the algorithm

  - $\alpha_t, \bar{\alpha_t}$â€‹

    äºŒè€…éƒ½æ˜¯è¶…å‚æ•°ï¼Œç”±æˆ‘ä»¬è‡ªå·±å†³å®šã€‚å®é™…ä¸Šè¿™äº§ç”Ÿäºå¦ä¸€ä¸ªåºåˆ— $\beta_1,\beta_2,...\beta_t$ï¼Œè¿™ä¸ªåºåˆ—ä»£è¡¨äº†æˆ‘ä»¬æ¯ä¸€ä¸ª step ä¸­å™ªå£°æ‰€å çš„æ¯”ä¾‹ï¼Œæ˜¯ä¸€ä¸ªé€’å¢çš„åºåˆ—ï¼Œå¹¶ä¸”å±äº 0~1 ä¹‹é—´ã€‚ç®€å•æ¥è¯´ï¼Œéšç€ forward step çš„åŠ æ·±ï¼Œæˆ‘ä»¬å‘åŸå›¾ä¸­åŠ çš„å™ªå£°ä¼šè¶Šæ¥è¶Šå¤§ï¼Œå³å™ªå£°æ‰€å çš„æ¯”ä¾‹ä¼šè¶Šæ¥è¶Šå¤§
    $$
    \alpha_t = 1-\beta_t\\
    \bar{\alpha_t}=\alpha_1\alpha_2...\alpha_t
    $$

  - $\epsilon_\theta$â€‹

    å…¶å®å°±æ˜¯ neural networkï¼Œå…¶å‚æ•°ç”¨ $\theta$ è¡¨ç¤ºï¼Œè¾“å…¥ä¸º $(x_t,t)$ï¼Œè¯¥ç½‘ç»œçš„ä½œç”¨å°±æ˜¯æ ¹æ®è¾“å…¥å›¾åƒå’Œ time step tï¼Œé¢„æµ‹å‡ºåŠ å…¥åˆ°è¯¥å›¾åƒçš„å™ªå£° $\epsilon$ï¼Œç„¶åç”¨è¾“å…¥å›¾åƒå‡å»è¯¥å™ªå£°å°±èƒ½å¤Ÿè·å¾—å»å™ªå›¾åƒ

  - $\sigma_t$

    ä»ç„¶ä¹Ÿæ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œä»£è¡¨æ¯ä¸€ä¸ª timestep t é¢å¤–åŠ å…¥çš„ variance

- Algorithm ç¤ºæ„å›¾ (at time step 999)

  <img src="Denoising Diffusion Probabilistic Models/image-20241206102258158.png" alt="image-20241206102258158" style="zoom:80%;" />

## How to train DDPM

- Algorithm

  <img src="Denoising Diffusion Probabilistic Models/image-20241205155602678.png" alt="image-20241205155602678" style="zoom: 80%;" />

  DDPM çš„è®­ç»ƒç®—æ³•ä¹Ÿéå¸¸çš„ç®€æ´ï¼Œç®€å•æ¥çœ‹å°±æ˜¯ç”¨ç½‘ç»œ $\epsilon_\theta$ å»é¢„æµ‹åŠ å…¥åˆ°æ ·æœ¬ä¸­çš„å™ªå£°ï¼Œå¸Œæœ›è¿™ä¸ªé¢„æµ‹å™ªå£°å’ŒçœŸå®åŠ å…¥çš„å™ªå£°è¶³å¤Ÿçš„æ¥è¿‘ã€‚ä½†å…¶å®è¿™ä¸ªè®­ç»ƒè¿‡ç¨‹æš—è—ç„ğŸ”ï¼Œé‡Œé¢è¿™äº›ç³»æ•°åˆ°åº•æ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿä¸ºä»€ä¹ˆåœ¨æ¨ç†çš„æ—¶å€™ä¼¼ä¹æ˜¯ä¸€æ­¥ä¸€æ­¥åœ°å»å™ªï¼Œè€Œè¿™ä¸ªè¿‡ç¨‹åœ¨è®­ç»ƒé‡Œæ²¡æœ‰å‘¢ï¼Ÿè¿™äº›éƒ½éœ€è¦åœ¨ä¹‹åçš„æ•°å­¦æ¨å¯¼ä¸­å›ç­”ï¼

## SOTA diffusion framework

- Overall Framework

  ç°ä»Š SOTA çš„ text-to-image æ¨¡å‹éƒ½å¯ä»¥åˆ†ä¸º3å¤§æ­¥éª¤ï¼š

  1. ä½¿ç”¨ text encoder å°†æ–‡å­—è¿›è¡Œ encodeï¼Œç”Ÿæˆ text feature
  2. ä½¿ç”¨ generation model å¯¹å™ªå£°è¿›è¡Œå»å™ªã€‚æ­¤æ—¶éœ€è¦å°† text feature ä½œä¸ºè¾“å…¥ï¼Œç»è¿‡æ¨¡å‹åè¾“å‡ºä¸€ä¸ªä¸­é—´äº§ç‰©ï¼Œè¯¥äº§ç‰©å¯ä»¥æ˜¯ feature mapï¼Œæˆ–è€…å‹ç¼©çš„å›¾ç‰‡
  3. å¯¹ä¸­é—´äº§ç‰©è¿›è¡Œ decodeï¼Œç”Ÿæˆæœ€ç»ˆçš„å›¾åƒ

  <img src="Denoising Diffusion Probabilistic Models/image-20241206163249880.png" alt="image-20241206163249880" style="zoom:67%;" />

  è¿™ä¸‰ä¸ªç»„ä»¶ï¼štext encoder, generation model, decoder é€šå¸¸éƒ½æ˜¯åˆ†å¼€è®­ç»ƒçš„ã€‚ä½†æ˜¯ generation model æ˜¯ä¾èµ–äº decoder çš„ï¼Œå› ä¸º decoder å¿…é¡»è¦è®¤è¯† generation model æ‰€äº§ç”Ÿçš„ç‰¹å¾å›¾ã€‚è¿™å®é™…ä¸Šåœ¨è®­ç»ƒ decoder æ—¶ï¼Œæˆ‘ä»¬è¿˜è®­ç»ƒäº†ä¸€ä¸ª auto encoder æ¥äº§ç”Ÿè¿™ä¸ª latent representations (feature map)ï¼Œç„¶åé€šè¿‡å‘ latent representation åŠ å…¥ noise è®­ç»ƒ generation model

- Text encoder size å¯¹äºå›¾åƒç”Ÿæˆè´¨é‡éå¸¸é‡è¦ï¼Œè€Œ vision encoder size å½±å“è¾ƒå°

  <img src="Denoising Diffusion Probabilistic Models/image-20241206161343570.png" alt="image-20241206161343570" style="zoom: 50%;" />

  FID Frechet Inception Distance å°±æ˜¯ç”¨æ¥è¯„ä»·æ‰€ç”Ÿæˆçš„å›¾åƒé›†ä¸ç›®æ ‡å›¾åƒé›†ä¹‹é—´çš„è·ç¦»ï¼ŒFID åº”è¯¥è¶Šå°è¶Šå¥½ã€‚å¯ä»¥çœ‹åˆ°éšç€ T5 æ¨¡å‹çš„å¢åŠ ï¼ŒFID æ›²çº¿æ˜¯å‘ç€å³ä¸‹è§’ç§»åŠ¨ï¼Œè¯´æ˜ç”Ÿæˆå›¾åƒæœ‰æ˜¾è‘—æ”¹å–„

  <img src="Denoising Diffusion Probabilistic Models/image-20241206161802419.png" alt="image-20241206161802419" style="zoom: 50%;" />

- è¯¾ç¨‹è¿˜ä»‹ç»äº† Stable Diffusion & DALL-Eï¼Œæˆ‘å°±ä¸æ•´ç†äº†

## From ELBO to VAE

[ELBO wiki](https://en.wikipedia.org/wiki/Evidence_lower_bound)æˆ‘èƒ½å¤Ÿä» EM algorithm æ¯”è¾ƒé¡ºåˆ©åœ°åˆ‡å…¥åˆ° VAE å½“ä¸­

- EM algorithm

- Variational Auto Encoder

  VAE å¯ä»¥è¯´æ˜¯åœ¨æ—©æœŸçš„å›¾åƒç”Ÿæˆé¢†åŸŸä¸­å¾ˆå¸¸ç”¨çš„æ–¹æ³•ï¼Œå¦‚æœç†è§£äº† VAE ç›¸ä¿¡ç†è§£ diffusion model ä¹Ÿæ˜¯æ›´ç®€å•çš„

## From VAE to Diffusion



## Fundamental Maths

æ€»ç»“ç†è§£ VAE & Diffusion model æ‰€éœ€è¦çš„åŸºç¡€æ•°å­¦ï¼Œä¸»è¦å°±æ˜¯è´å¶æ–¯ç†è®ºï¼ˆBayesian Theoremï¼‰ä»¥åŠç›¸å…³çš„æ¦‚ç‡è®ºåŸºç¡€

- Bayesian Theorem [wiki](https://en.wikipedia.org/wiki/Bayes%27_theorem)

  > Bayes' theorem (alternatively Bayes' law or Bayes' rule, after [Thomas Bayes](https://en.wikipedia.org/wiki/Thomas_Bayes)) gives a mathematical rule for inverting [conditional probabilities](https://en.wikipedia.org/wiki/Conditional_probability), allowing one to **find the probability of a cause given its effect.**

  ä¸Šé¢å°±æ˜¯ wiki çš„ç¬¬ä¸€å¥è¯ï¼Œæ˜¯å¯¹è´å¶æ–¯ç†è®ºçš„é«˜åº¦æ€»ç»“ï¼šfind the probability of a cause given its effectï¼Œç»™å®šè¿™äº›ç°è±¡æ¥å¯»æ‰¾åŸå› ã€‚è¿™ä¸ªåŠŸèƒ½æ˜¯ä¸€ä¸ªéå¸¸å¼ºå¤§çš„åŠŸèƒ½ï¼Œä½†å…¶å…¬å¼å´ç›¸å½“çš„ç®€å•
  $$
  P(A|B) = \frac{P(B|A)P(A)}{P(B)}
  $$
  è¿™é‡Œçš„ A å’Œ B éƒ½æ˜¯ä»»æ„çš„éšæœºäº‹ä»¶ï¼Œå¹¶ä¸” P(B) ä¸ä¸º 0ã€‚è¿™é‡Œçš„ A å’Œ B çœ‹èµ·æ¥éå¸¸çš„æŠ½è±¡ï¼Œå¦‚ä½•å¯¹åº”åˆ°å®é™…åº”ç”¨å½“ä¸­ï¼Ÿåœ¨ç°å®ä¸­ï¼ŒA é€šå¸¸ç”¨æ¥è¡¨ç¤º Hypothesisï¼Œå³æˆ‘ä»¬çš„å‡è®¾ï¼ŒB é€šå¸¸ç”¨æ¥è¡¨ç¤º Evidenceï¼Œå³å‘ç”Ÿçš„ç°è±¡ï¼Œæ‰€ä»¥ä¹Ÿé€šå¸¸çœ‹åˆ°ä¸‹æ–¹çš„å­—æ¯è¡¨ç¤º
  $$
  P(H|E) = \frac{P(E|H)P(H)}{P(E)}
  $$
  ä¸ºäº†æ›´è¿›ä¸€æ­¥ç†è§£ï¼Œæˆ‘ä»¬å°†è¿™å‡ ä¸ªæ¦‚ç‡è¡¨ç¤ºçš„æ„ä¹‰å†™ä½œå¦‚ä¸‹ï¼š

  1. $P(H)$ï¼ŒPriorï¼Œå…ˆéªŒã€‚æˆ‘ä»¬é€šå¸¸ç§° hypothesis ä¸ºå…ˆéªŒ
  2. $P(E)$ï¼ŒEvidenceã€‚åç§°æ²¡æœ‰å˜åŒ–
  3. $P(E|H)$ï¼ŒLikelihoodï¼Œä¼¼ç„¶ã€‚åŸºäº hypothesis æ‰€å¾—å‡ºçš„äº‹ä»¶æ¦‚ç‡å³ä¸ºä¼¼ç„¶
  4. $P(H|E)$â€‹ï¼ŒPosteriorï¼ŒåéªŒã€‚åŸºäº evidence æ‰€æ›´æ–°çš„ hypothesis 

  æ­¤æ—¶è´å¶æ–¯å…¬å¼çš„åŠŸèƒ½å˜å¾—æ›´åŠ å…·è±¡èµ·æ¥ï¼šæ ¹æ®äº‹å®æ¥æ›´æ–°æˆ‘ä»¬çš„å‡è®¾ã€‚é‚£ä¹ˆè¿™ä¸ªå‡è®¾åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿäº‹å®åˆæ˜¯ä»€ä¹ˆï¼Ÿç”¨ä¸€ä¸ªæ›´åŠ å…·è±¡çš„ä¾‹å­è¡¨ç¤ºï¼š
  $$
  P(\theta|X) = \frac{P(X|\theta)P(\theta)}{P(X)}
  $$
  è¿™ä¸‹å­ç¬¦å·ä¼¼ä¹çœ‹èµ·æ¥æ›´ç†Ÿæ‚‰äº†ï¼š$\theta$ å°±æ˜¯æ¨¡å‹å‚æ•°ï¼Œ$X$ å°±æ˜¯æ•°æ®æ ·æœ¬ï¼Œæˆ‘ä»¬çš„ä»»åŠ¡å°±æ˜¯ç”¨æ¨¡å‹æ¥ä¼°è®¡æ ·æœ¬çš„åˆ†å¸ƒã€‚å…¶ä¸­æˆ‘ä»¬ç”¨ $\theta$ æ¥è¡¨ç¤ºäº†æˆ‘ä»¬çš„æ¨¡å‹å‚æ•°ï¼Œ**å…¶å®è¿™æ˜¯ä¸€ä¸ª over simplificationï¼Œå…¶ä¸­è¿˜åŒ…å«äº†æˆ‘ä»¬çš„å»ºæ¨¡å‡è®¾**ï¼Œä¾‹å¦‚ï¼šè¿™ä¸ªæ¨¡å‹æ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œä¸”æ•°æ®çš„åˆ†å¸ƒç¬¦åˆé«˜æ–¯åˆ†å¸ƒï¼Œé«˜æ–¯åˆ†å¸ƒçš„å‚æ•°ä¸º $\theta$ã€‚æ‰€ä»¥æˆ‘ä»¬èƒ½å¤Ÿéå¸¸è½»æ¾åœ°æ ¹æ®è¿™ä¸ªå‡è®¾è®¡ç®—å¾—åˆ° $P(X|\theta)$â€‹ï¼Œç›´æ¥æ ¹æ®é«˜æ–¯åˆ†å¸ƒæ¥ç®—å°±è¡Œäº†

  ç†è§£è¿™ä¸ªå…¬å¼æœ€å¸¸ä¸¾çš„ä¾‹å­å°±æ˜¯æŠ›ç¡¬å¸çš„ä¾‹å­ï¼š

  æˆ‘ä»¬å°†ä¸Šè¿°çš„å˜é‡éƒ½è¿›è¡Œå…·ä½“çš„å®šä¹‰

  - $\theta$ æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œå…¶å†³å®šäº†ç¡¬å¸ä¸ºæ­£é¢çš„æ¦‚ç‡
  - $X$ æ˜¯å®éªŒç»“æœï¼Œæˆ‘ä»¬çš„å®éªŒç»“æœä¸ºæŠ›10æ¬¡ç¡¬å¸ï¼Œæœ‰7æ¬¡ä¸ºæ­£é¢ï¼Œ3æ¬¡ä¸ºåé¢

  é¦–å…ˆæˆ‘ä»¬éœ€è¦æœ‰ä¸€ä¸ªåˆå§‹çŒœæµ‹ï¼š$\theta$ åˆ°åº•æ˜¯ä¸ªä»€ä¹ˆåˆ†å¸ƒï¼Ÿç”±äºä¸€å¼€å§‹æˆ‘ä»¬æ²¡æœ‰ä»»ä½•ä¿¡æ¯ï¼Œä¸å¦‚å‡è®¾ä¸ºæœ€ç®€å•çš„ uniform distribution (å‡åŒ€åˆ†å¸ƒ) $P(\theta)=1, \theta\in [0,1]$

  æˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“æ ¹æ®æˆ‘ä»¬çš„å‡è®¾è®¡ç®—å¾—åˆ°æˆ‘ä»¬çš„ likelihood 
  $$
  P(X|\theta) = C_{10}^7Â·\theta^7(1-\theta)^3
  $$
  OKï¼Œç°åœ¨æ¯”è¾ƒéš¾çš„æ˜¯æ±‚å¾— $P(X)$ï¼Œè¿™é‡Œéœ€è¦ä½¿ç”¨å…¨æ¦‚ç‡å…¬å¼ï¼Œä¸è¿‡å¥½åœ¨æˆ‘ä»¬ä¹Ÿèƒ½å¤Ÿæ±‚åˆ°
  $$
  P(X) = \int_0^1P(X|\theta)P(\theta)d\theta
  $$
  æœ€åæ±‚å¾— $P(X)â‰ˆ0.1$ï¼Œæ˜¯ä¸€ä¸ªå¸¸æ•°ã€‚æ‰€ä»¥å°†æ‰€æœ‰çš„ç»“æœå¯¼å…¥åˆ°è´å¶æ–¯å…¬å¼å½“ä¸­ï¼Œå°±å¯ä»¥å¾—åˆ°
  $$
  P(\theta|X) = \frac{C_{10}^7Â·\theta^7(1-\theta)^3Â·1}{0.1}
  $$
  å¯ä»¥çœ‹åˆ°ï¼Œç°åœ¨æˆ‘ä»¬çš„ $\theta$ è¢«æ›´æ–°ä¸ºäº†ä¸€ä¸ª [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution)ï¼Œç›¸æ¯”äºä¹‹å‰çš„ uniform distribution æ”¹åŠ¨ä¸å°ã€‚å¦‚æœå®éªŒé‡è¶³å¤Ÿå¤šï¼Œé‚£ä¹ˆæˆ‘ä»¬æ‰€ç®—å‡ºçš„ $\theta$ åº”è¯¥è¶‹è¿‘äºä¸€ä¸ª delta åˆ†å¸ƒï¼Œæ”¶æ•›åˆ° $\frac{heads}{trials}$â€‹ è¿™ä¸ªå€¼ï¼Œå¹¶ä¸”æ— è®ºä½ çš„åˆå§‹ $P(\theta)$ æ˜¯å¤šå°‘ï¼Œéƒ½ä¼šæ”¶æ•›åˆ°æœ€ç»ˆè¿™ä¸ªåˆ†å¸ƒä¸Šã€‚æ‰€ä»¥è¿™ç»™æˆ‘ä¸€ä¸ªå¯å‘ï¼š

  æ— è®ºåˆå§‹ $P(\theta)$ åˆ†å¸ƒæ˜¯æ€æ ·çš„ï¼Œæ‰€æ”¶è·åˆ°çš„ $P(X|\theta)$ æ›´æ–°éƒ½æ˜¯ä¸€æ ·çš„ï¼Œè¿™æ˜¯ç”±æˆ‘ä»¬çš„å»ºæ¨¡æ‰€å†³å®šçš„ï¼Œå³æˆ‘ä»¬çš„æ¨¡å‹å‡è®¾ï¼š$\theta$ å†³å®šäº†ç¡¬å¸ä¸ºæ­£é¢çš„æ¦‚ç‡ã€‚å¹¶ä¸”å¦‚æœæ›´æ–°çš„ likelihood è¶³å¤Ÿå¼ºï¼Œé‚£ä¹ˆå°†å®Œå…¨è¦†ç›–ä¹‹å‰çš„å…ˆéªŒï¼Œä»¥ likelihood ä¸ºåŸºå‡†

  å¦å¤–å†æä¸€ç‚¹ï¼šæˆ‘ä»¬åœ¨è®¡ç®— $P(X)$ çš„æ—¶å€™èƒ½ä½¿ç”¨è¿™ä¸ªå…¨æ¦‚ç‡å…¬å¼ï¼Œä»ç„¶æ˜¯åœ¨æˆ‘ä»¬çš„æ¨¡å‹å‡è®¾ä¹‹ä¸‹çš„ã€‚å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡å‹å‡è®¾åŸºæœ¬ä¸Šè´¯ç©¿äº†æ‰€æœ‰çš„è®¡ç®—è¿‡ç¨‹ï¼Œä¸€ä¸ªé”™è¯¯çš„æ¨¡å‹å‡è®¾ï¼Œå³ä½¿è®¡ç®—å†å¤šçš„å‚æ•°ï¼Œä¹Ÿæ— æ³•è·å¾—å¥½çš„åéªŒæ¦‚ç‡

  è¿™æ˜¯ä¸€ä¸ªéå¸¸éå¸¸ç®€å•çš„ä¾‹å­ï¼Œç®€å•åˆ°æˆ‘æƒ³é—®ï¼šä¸ºä»€ä¹ˆä¸ä¸€å¼€å§‹æˆ‘ä»¬å°±ç®— $\frac{heads}{trials}$ è¿™ä¸ªå€¼ä½œä¸ºæˆ‘ä»¬æœ€ç»ˆçš„ $\theta$ åˆ†å¸ƒå‘¢ğŸ¤” ä½†æ˜¯æœ‰å‡ ä¸ªé‡è¦çš„å¯å‘

  1. $P(X)$ is really hard to calculate
  2. Binominal distribution is too simple

  Prompt: a very weird shaped dice that you can not easily know what is the outcome when you flip it 

  <img src="Denoising Diffusion Probabilistic Models/image-20250124160132877.png" alt="image-20250124160132877" style="zoom:50%;" />

- Joint distribution and conditional distribution

  

- Marginalize

- How to understand latent variable?

  From the Understanding Diffusion Models: A Unified Perspective gives an intuitive philosophy

- Chain of rules in probability

  https://en.wikipedia.org/wiki/Chain_rule_(probability)

- Markov Chain Monte Carlo (MCMC)

- Reparameterization trick

  https://en.wikipedia.org/wiki/Reparameterization_trick

- [Lil's log on diffusion](https://lilianweng.github.io/posts/2021-07-11-diffusion-models)

- How does variational inference connected with ELBO?

  [Evidence lower bound - Wikipedia](https://en.wikipedia.org/wiki/Evidence_lower_bound#Variational_Bayesian_inference)

  These words are extremely important to anwer the question: what does these parameter is trying to model? and how to compute these values actually

  > This defines a family of joint distributions pÎ¸ over (X,Z). It is very easy to sample (x,z)âˆ¼pÎ¸: simply sample zâˆ¼p, then compute fÎ¸(z), and finally sample xâˆ¼pÎ¸(â‹…|z) using fÎ¸(z).

  In general, it's impossible to perform the integral pÎ¸(x)=âˆ«pÎ¸(x|z)p(z)dz, forcing us to perform another approximation.

## Question

- ä¸ºä»€ä¹ˆåœ¨ inference é‡‡æ ·çš„æ—¶å€™è¿˜è¦åŠ å…¥éšæœºå™ªå£°ï¼Ÿ

  é‡‡æ ·ï¼Œjust like sampling when generating tokens

- In the [material](https://arxiv.org/pdf/2208.11970), there are some p is $p_\theta(Â·)$, but some wihout $\theta$, juse $p(Â·)$, how to differenciate them?

  It seems that the $p(Â·)$ without the $\theta$â€‹ means it is a prior distribution, which means we defined it manually at the very beginning, or let's say it is out hypothesis

- Explaining the square root in the $\sqrt{\alpha_t}$ when doing linear gaussian modeling

  This is to maintain the variance structure of origianl distribution

- How to optimize the first term of VAE $E_{z\sim q_{\phi}(z|x)}[\log{p_{\theta}(x|z)}]$â€‹

  we use the network to produce the mean of of gaussian, what about variance?
  
- Why it is hard to compute the $p_\theta(x)$

  [my-chat](https://chatgpt.com/share/675726ae-8364-800a-b33d-0ed508bc3eaf)
  
- What is variational?

  å˜åˆ†è¿™ä¸ªæ¦‚å¿µä¼¼ä¹éå¸¸

- ä»€ä¹ˆæ˜¯ç”Ÿæˆæ¨¡å‹ï¼Œä»€ä¹ˆæ˜¯åˆ¤åˆ«æ¨¡å‹ï¼Œä»–ä»¬çš„æ¦‚å¿µæ˜¯ä»€ä¹ˆï¼Ÿ

- æ±‚è§£ $P(X)$ çš„æ–¹å¼é™¤äº†ä¸Šè¿°çš„å…¨æ¦‚ç‡å…¬å¼ï¼Œæ˜¯å¦è¿˜å­˜åœ¨ MCMC çš„æ–¹æ³•ï¼Ÿ
