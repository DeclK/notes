# Denoising Diffusion Probabilistic Models

[DDPM arxiv](https://arxiv.org/pdf/2006.11239)

[Understanding Diffusion Models: A Unified Perspective arxiv](https://arxiv.org/pdf/2208.11970)

[bilibili-æ‰©æ•£æ¨¡å‹ - Diffusion Modelã€æå®æ¯…2023ã€‘](https://www.bilibili.com/video/BV14c411J7f2) [PPT1](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/DiffusionModel%20(v2).pdf) [PPT2](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/StableDiffusion%20(v2).pdf) [PPT3](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/DDPM%20(v7).pdf) [course page](https://speech.ee.ntu.edu.tw/~hylee/ml/2023-spring.php)

æœ¬æ–‡æ¡£æ ¹æ®æå®æ¯…è§†é¢‘æ•´ç†å¾—åˆ°ï¼Œå¸Œæœ›å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

1. How to train and inference a diffusion model
2. What are the basic mathmatics behind it
3. How SOTA models improve the original diffusion model
4. What are the insights of diffusion can give us

## Notaions in Probability

- $x\sim p(Â·)$ ä»£è¡¨ç€ $x$ æ˜¯ä¸€ä¸ªéšæœºå˜é‡æœä»äºæ¦‚ç‡åˆ†å¸ƒ $p$ï¼Œåœ¨è®ºæ–‡ä¸­ï¼Œä¹Ÿå¸¸å¸¸ç”¨è¯¥ notation æ¥è¡¨ç¤ºåœ¨ $p$ åˆ†å¸ƒä¸­é‡‡æ · $x$
- $E_{x\sim p(x)}$ ä»£è¡¨ç€æ±‚è§£å˜é‡ $x$ çš„æœŸæœ›ï¼Œå¹¶ä¸”è¯¥å˜é‡æœä»æ¦‚ç‡åˆ†å¸ƒ $p(x)$ï¼Œæœ‰æ—¶å€™ä¹Ÿä¼šç”¨ $E_{p(x)}$â€‹ æ¥ç®€å†™è¯¥è¿‡ç¨‹
- $N(x;\mu, \sigma^2)$ æ¥è¡¨ç¤ºä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œå…¶ä¸­éšæœºå˜é‡ä¸º $x$ï¼Œå…¶å‡å€¼å’Œæ–¹å·®åˆ†åˆ«ä¸º $\mu, \sigma^2$ï¼Œæœ‰æ—¶å€™ä¹Ÿä¼šçœç•¥æ‰ $x$ï¼Œç›´æ¥å†™ä½œ $N(\mu, \sigma^2)$ã€‚å¯¹äºä¸€ä¸ªå¤šç»´é«˜æ–¯åˆ†å¸ƒä½¿ç”¨å¦‚ä¸‹ notation: $N(x;\mu,\Sigma)$ï¼Œå…¶ä¸­ $x,\mu$ éƒ½æ˜¯å¤šç»´å‘é‡ï¼Œè€Œ $\Sigma$ ä¸ºåæ–¹å·®çŸ©é˜µ (covariance matrix)

## An intuitive perspective of DDPM

<img src="Denoising Diffusion Probabilistic Models/image-20241206102950079.png" alt="image-20241206102950079" style="zoom:67%;" />

çœ‹å›¾è¯´è¯ï¼šDDPM å…¶å®å°±æ˜¯ä¸€ä¸ªä»éšæœºå™ªå£°å¼€å§‹ï¼Œä¸æ–­å»å™ªç›´åˆ°ç”Ÿæˆç›®æ ‡å¯¹è±¡çš„è¿‡ç¨‹ã€‚é‚£åˆ°åº•ä¼šç”Ÿæˆä»€ä¹ˆæ ·çš„å›¾åƒå‘¢ï¼Ÿæ ¹æ®ä¸Šé¢çš„ç¤ºæ„å›¾ï¼Œæˆ‘ä»¬å…¶å®å¹¶æ²¡æœ‰ç»™å»å™ªæ¨¡å‹ä»»ä½•çš„ä¿¡æ¯ï¼Œåªç»™äº†ä¸€ä¸ªåˆå§‹å™ªå£°å°±å¼€å§‹è®©å…¶è¿›è¡Œå»å™ªï¼Œæ‰€ä»¥ç†è®ºä¸Šä¼šç”Ÿæˆä»»æ„çš„å›¾åƒã€‚è€Œç°åœ¨ diffusion model åœ¨ text-to-image é¢†åŸŸåº”ç”¨éå¸¸å¤šï¼Œæ‰€ä»¥å¦‚æœæƒ³è¦ç”ŸæˆæŒ‡å®šçš„å›¾åƒï¼Œæˆ‘ä»¬éœ€è¦ç»™ diffusion model åŠ å…¥é¢å¤–çš„ä¿¡æ¯ï¼Œä¾‹å¦‚æ–‡å­—ï¼š

<img src="Denoising Diffusion Probabilistic Models/image-20241206161019438.png" alt="image-20241206161019438" style="zoom: 67%;" />

## How to inference DDPM

- Algorithm

  <img src="Denoising Diffusion Probabilistic Models/image-20241205155625447.png" alt="image-20241205155625447" style="zoom:80%;" />

  Notation of the algorithm

  - $\alpha_t, \bar{\alpha_t}$â€‹

    äºŒè€…éƒ½æ˜¯è¶…å‚æ•°ï¼Œç”±æˆ‘ä»¬è‡ªå·±å†³å®šã€‚å®é™…ä¸Šè¿™äº§ç”Ÿäºå¦ä¸€ä¸ªåºåˆ— $\beta_1,\beta_2,...\beta_t$ï¼Œè¿™ä¸ªåºåˆ—ä»£è¡¨äº†æˆ‘ä»¬æ¯ä¸€ä¸ª step ä¸­å™ªå£°æ‰€å çš„æ¯”ä¾‹ï¼Œæ˜¯ä¸€ä¸ªé€’å¢çš„åºåˆ—ï¼Œå¹¶ä¸”å±äº 0~1 ä¹‹é—´ã€‚ç®€å•æ¥è¯´ï¼Œéšç€ forward step çš„åŠ æ·±ï¼Œæˆ‘ä»¬å‘åŸå›¾ä¸­åŠ çš„å™ªå£°ä¼šè¶Šæ¥è¶Šå¤§ï¼Œå³å™ªå£°æ‰€å çš„æ¯”ä¾‹ä¼šè¶Šæ¥è¶Šå¤§
    $$
    \alpha_t = 1-\beta_t\\
    \bar{\alpha_t}=\alpha_1\alpha_2...\alpha_t
    $$

  - $\epsilon_\theta$â€‹

    å…¶å®å°±æ˜¯ neural networkï¼Œå…¶å‚æ•°ç”¨ $\theta$ è¡¨ç¤ºï¼Œè¾“å…¥ä¸º $(x_t,t)$ï¼Œè¯¥ç½‘ç»œçš„ä½œç”¨å°±æ˜¯æ ¹æ®è¾“å…¥å›¾åƒå’Œ time step tï¼Œé¢„æµ‹å‡ºåŠ å…¥åˆ°è¯¥å›¾åƒçš„å™ªå£° $\epsilon$ï¼Œç„¶åç”¨è¾“å…¥å›¾åƒå‡å»è¯¥å™ªå£°å°±èƒ½å¤Ÿè·å¾—å»å™ªå›¾åƒ

  - $\sigma_t$

    ä»ç„¶ä¹Ÿæ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œä»£è¡¨æ¯ä¸€ä¸ª timestep t é¢å¤–åŠ å…¥çš„ variance

- Algorithm ç¤ºæ„å›¾ (at time step 999)

  <img src="Denoising Diffusion Probabilistic Models/image-20241206102258158.png" alt="image-20241206102258158" style="zoom:80%;" />

## How to train DDPM

- Algorithm

  <img src="Denoising Diffusion Probabilistic Models/image-20241205155602678.png" alt="image-20241205155602678" style="zoom: 80%;" />

  DDPM çš„è®­ç»ƒç®—æ³•ä¹Ÿéå¸¸çš„ç®€æ´ï¼Œç®€å•æ¥çœ‹å°±æ˜¯ç”¨ç½‘ç»œ $\epsilon_\theta$ å»é¢„æµ‹åŠ å…¥åˆ°æ ·æœ¬ä¸­çš„å™ªå£°ï¼Œå¸Œæœ›è¿™ä¸ªé¢„æµ‹å™ªå£°å’ŒçœŸå®åŠ å…¥çš„å™ªå£°è¶³å¤Ÿçš„æ¥è¿‘ã€‚ä½†å…¶å®è¿™ä¸ªè®­ç»ƒè¿‡ç¨‹æš—è—ç„ğŸ”ï¼Œé‡Œé¢è¿™äº›ç³»æ•°åˆ°åº•æ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿä¸ºä»€ä¹ˆåœ¨æ¨ç†çš„æ—¶å€™ä¼¼ä¹æ˜¯ä¸€æ­¥ä¸€æ­¥åœ°å»å™ªï¼Œè€Œè¿™ä¸ªè¿‡ç¨‹åœ¨è®­ç»ƒé‡Œæ²¡æœ‰å‘¢ï¼Ÿè¿™äº›éƒ½éœ€è¦åœ¨ä¹‹åçš„æ•°å­¦æ¨å¯¼ä¸­å›ç­”ï¼

## SOTA diffusion framework

- Overall Framework

  ç°ä»Š SOTA çš„ text-to-image æ¨¡å‹éƒ½å¯ä»¥åˆ†ä¸º3å¤§æ­¥éª¤ï¼š

  1. ä½¿ç”¨ text encoder å°†æ–‡å­—è¿›è¡Œ encodeï¼Œç”Ÿæˆ text feature
  2. ä½¿ç”¨ generation model å¯¹å™ªå£°è¿›è¡Œå»å™ªã€‚æ­¤æ—¶éœ€è¦å°† text feature ä½œä¸ºè¾“å…¥ï¼Œç»è¿‡æ¨¡å‹åè¾“å‡ºä¸€ä¸ªä¸­é—´äº§ç‰©ï¼Œè¯¥äº§ç‰©å¯ä»¥æ˜¯ feature mapï¼Œæˆ–è€…å‹ç¼©çš„å›¾ç‰‡
  3. å¯¹ä¸­é—´äº§ç‰©è¿›è¡Œ decodeï¼Œç”Ÿæˆæœ€ç»ˆçš„å›¾åƒ

  <img src="Denoising Diffusion Probabilistic Models/image-20241206163249880.png" alt="image-20241206163249880" style="zoom:67%;" />

  è¿™ä¸‰ä¸ªç»„ä»¶ï¼štext encoder, generation model, decoder é€šå¸¸éƒ½æ˜¯åˆ†å¼€è®­ç»ƒçš„ã€‚ä½†æ˜¯ generation model æ˜¯ä¾èµ–äº decoder çš„ï¼Œå› ä¸º decoder å¿…é¡»è¦è®¤è¯† generation model æ‰€äº§ç”Ÿçš„ç‰¹å¾å›¾ã€‚è¿™å®é™…ä¸Šåœ¨è®­ç»ƒ decoder æ—¶ï¼Œæˆ‘ä»¬è¿˜è®­ç»ƒäº†ä¸€ä¸ª auto encoder æ¥äº§ç”Ÿè¿™ä¸ª latent representations (feature map)ï¼Œç„¶åé€šè¿‡å‘ latent representation åŠ å…¥ noise è®­ç»ƒ generation model

- Text encoder size å¯¹äºå›¾åƒç”Ÿæˆè´¨é‡éå¸¸é‡è¦ï¼Œè€Œ vision encoder size å½±å“è¾ƒå°

  <img src="Denoising Diffusion Probabilistic Models/image-20241206161343570.png" alt="image-20241206161343570" style="zoom: 50%;" />

  FID Frechet Inception Distance å°±æ˜¯ç”¨æ¥è¯„ä»·æ‰€ç”Ÿæˆçš„å›¾åƒé›†ä¸ç›®æ ‡å›¾åƒé›†ä¹‹é—´çš„è·ç¦»ï¼ŒFID åº”è¯¥è¶Šå°è¶Šå¥½ã€‚å¯ä»¥çœ‹åˆ°éšç€ T5 æ¨¡å‹çš„å¢åŠ ï¼ŒFID æ›²çº¿æ˜¯å‘ç€å³ä¸‹è§’ç§»åŠ¨ï¼Œè¯´æ˜ç”Ÿæˆå›¾åƒæœ‰æ˜¾è‘—æ”¹å–„

  <img src="Denoising Diffusion Probabilistic Models/image-20241206161802419.png" alt="image-20241206161802419" style="zoom: 50%;" />

- è¯¾ç¨‹è¿˜ä»‹ç»äº† Stable Diffusion & DALL-Eï¼Œæˆ‘å°±ä¸æ•´ç†äº†

## From ELBO to VAE

[ELBO wiki](https://en.wikipedia.org/wiki/Evidence_lower_bound) æˆ‘èƒ½å¤Ÿä» EM algorithm æ¯”è¾ƒé¡ºåˆ©åœ°åˆ‡å…¥åˆ° VAE å½“ä¸­

### Pre-request: why latent variable?

æˆ‘ä»¬ä¸ºä»€ä¹ˆè¦æå‡ºéšå˜é‡çš„æ¦‚å¿µï¼Ÿè¿™ä¸ªé—®é¢˜çš„å›ç­”æ˜¯ä»æ–‡ç«  [Understanding Diffusion Models: A Unified Perspective](https://arxiv.org/pdf/2208.11970) ä¸­å¾—åˆ°å¯å‘?

> For many modalities, we can think of the data we observe as represented or generated by an associated unseen latent variable, which we can denote by random variable z. The best intuition for expressing this idea is through Platoâ€™s Allegory of the Cave. In the allegory, a group of people are chained inside a cave their entire life and can only see the two-dimensional shadows projected onto a wall in front of them, which are generated by unseen three-dimensional objects passed before a fire. To such people, everything they observe is actually determined by higher-dimensional abstract concepts that they can never behold.
>
> Analogously, the objects that we encounter in the actual world may also be generated as a function of some higher-level representations; for example, such representations may encapsulate abstract properties such as color, size, shape, and more. **Whereas the cave people can never see (or even fully comprehend) the hidden objects, they can still reason and draw inferences about them; in a similar way, we can approximate latent representations that describe the data we observe**

åŠ ç²—çš„è¿™å¥è¯å°±æ˜¯å¯¹éšå˜é‡ç›´è§‰çš„æ€»ç»“ï¼šæˆ‘ä»¬å‡ ä¹æ°¸è¿œæ— æ³•è·å¾—äº‹ä»¶å‘ç”Ÿçš„çœŸå®æ ¹å› ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦è‡ªå·±å»ä¼°è®¡/å¯»æ‰¾/è®¡ç®—è¿™äº›æ ¹å› ï¼Œç”¨è¿™äº›æ ¹å› æ¥æè¿°æ‰€å‘ç”Ÿçš„ç°è±¡ã€‚äº‹å®ä¸Šï¼Œè¿™ä¼¼ä¹å°±æ˜¯äººç±»å®éªŒç§‘å­¦å‘å±•çš„é€”å¾„ï¼šé€šè¿‡æ‰€è§‚å¯Ÿçš„äº‹å®æ€»ç»“æˆ–éªŒè¯è§„å¾‹ã€‚è¿™äº›è§„å¾‹å°±æ˜¯æˆ‘ä»¬æ‰€å¯»æ‰¾çš„éšå˜é‡

### Variational Bayesian inference

åœ¨ä¹‹å‰æˆ‘ç®€å•æ€»ç»“äº†ä»€ä¹ˆæ˜¯ Bayesian Inferenceï¼Œç°åœ¨éœ€è¦äº†è§£ä¸€ä¸‹ä»€ä¹ˆæ˜¯å˜åˆ† variational

> FROM DeepSeek:
>
> **Variational Methods** æ˜¯ä¸€ç±»ç”¨äºè¿‘ä¼¼å¤æ‚æ•°å­¦é—®é¢˜çš„æŠ€æœ¯ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¦‚ç‡å’Œç»Ÿè®¡ä¸­ã€‚**å˜åˆ†æ–¹æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡ä¼˜åŒ–æ¥æ‰¾åˆ°ä¸€ä¸ªç®€å•çš„è¿‘ä¼¼åˆ†å¸ƒï¼Œä½¿å…¶å°½å¯èƒ½æ¥è¿‘çœŸå®çš„åéªŒåˆ†å¸ƒ**ã€‚åœ¨å˜åˆ†è´å¶æ–¯æ¨æ–­ä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªå‚æ•°åŒ–çš„åˆ†å¸ƒæ—ï¼Œå¹¶é€šè¿‡æœ€å°åŒ–æŸç§æŸå¤±å‡½æ•°ï¼ˆå¦‚KLæ•£åº¦ï¼‰æ¥æ‰¾åˆ°æœ€ä½³è¿‘ä¼¼ã€‚è¿™ç§æ–¹æ³•åœ¨å¤„ç†å¤æ‚æ¨¡å‹æˆ–é«˜ç»´æ•°æ®æ—¶ç‰¹åˆ«æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå¯ä»¥é¿å…ç›´æ¥è®¡ç®—éš¾ä»¥å¤„ç†çš„åéªŒåˆ†å¸ƒã€‚

æ‰€ä»¥è¯´å˜åˆ†å…¶å®å°±æ˜¯ä¸€ç§æ€æƒ³ï¼šä½¿ç”¨ä¼˜åŒ–çš„æ–¹å¼æ¥è·å¾—æœ€ä¼˜å‚æ•°ã€‚æˆ‘ä¸€ç›´è§‰å¾— variational è¿™ä¸ªè¯æ¯”è¾ƒæŠ½è±¡ï¼Œè¯¢é—® DeepSeek è¿‡åæ‰å‘ç°è¿™ä¸ªè¯éå¸¸å…·è±¡

> From DeepSeek:
>
> The term **"variational"** in **variational Bayesian inference** comes from **calculus of variations**, a branch of mathematics that deals with optimizing *functionals* (functions of functions). 
>
> - In calculus of variations, we study how small **variations** (tiny adjustments) to a function affect a quantity (e.g., minimizing energy or maximizing entropy).
> - It mimics the calculus of variations: instead of varying a *function* (e.g., a curve), we vary the *parameters* of $q(\theta)$ to optimize the approximation.

ä¹Ÿå°±æ˜¯è¯´ variational å˜çš„å°±æ˜¯å‚æ•° $\theta$ã€‚å¦å¤–ä¸ªäººç†è§£ï¼š$\theta$ é€šè¿‡å¾®å°çš„å˜åŒ–æ‰€å¼•èµ·çš„æŸå¤±å‡½æ•°çš„å˜åŒ–å°±èƒ½å¤Ÿå¼•å‡ºæ¢¯åº¦çš„è®¡ç®—ï¼Œä¹Ÿå°±èƒ½ä½¿ç”¨åå‘ä¼ æ’­ç®—æ³•æ¥ä¼˜åŒ–å‚æ•°

ä¸‹é¢å°†æ­£å¼å¯¹ Variational Bayesian Inference ä½œå‡ºæ•°å­¦å®šä¹‰

> Suppose we have an observable random variable $X$, and we want to find its true distribution $p$. This would allow us to generate data by sampling, and estimate probabilities of future events. In general, it is impossible to find $p^*$â€‹ exactly, forcing us to search for a good approximation.
>
> That is, we define a sufficiently large parametric family $\{p_\theta\}_{\theta \in \Theta}$ of distributions, then solve for $\min_{\theta} L(p_\theta, p^*)$ for some loss function $L$. One possible way to solve this is by considering small variation from $p_\theta$ to $p_{\theta + \delta \theta}$, and solve for $L(p_\theta, p^*) - L(p_{\theta + \delta \theta}, p^*) = 0$. This is a problem in the calculus of variations, thus it is called the **variational method**.

**é—®é¢˜å»ºæ¨¡ 1.0**

- å®šä¹‰éšå˜é‡ $Z$ï¼Œå…¶åˆ†å¸ƒ $p(z)$ ä¸ºéå¸¸ç®€å•çš„åˆ†å¸ƒï¼ˆuniform or normal distributionï¼‰
- å®šä¹‰å‡½æ•° $f_\theta$ï¼Œé€šå¸¸ä¸ºä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå…¶å‚æ•°ä¸º $\theta$ï¼Œè¾“å…¥ä¸ºéšå˜é‡ $z$ã€‚å…¶ä½œç”¨æ˜¯è¿‘ä¼¼è®¡ç®—ç»™å®šéšå˜é‡ç”Ÿæˆè§‚æµ‹æ•°æ®çš„è¿‡ç¨‹
- å®šä¹‰å‡½æ•°è¾“å‡º $f_\theta(z)$ ä¸ºæ–°çš„æ¦‚ç‡åˆ†å¸ƒçš„å‚æ•°ã€‚ä¾‹å¦‚æˆ‘ä»¬å¯ä»¥å®šä¹‰ $f_\theta(z)=(f_1(z),f_2(z))$ ä¸º normal distribution çš„å‚æ•°ï¼Œå‰è€…ä¸ºå‡å€¼ï¼Œåè€…ä¸ºæ–¹å·®

å¦‚æ­¤å»ºæ¨¡èƒ½å¤Ÿå¾ˆå®¹æ˜“åœ°é€šè¿‡é‡‡æ ·æ¥è·å¾—è”åˆåˆ†å¸ƒ $p_\theta(x,z)$

> This defines a family of joint distributions $p_\theta$ over $(X, Z)$. It is very easy to sample $(x, z) \sim p_\theta$: simply sample $z \sim p$, then compute $f_\theta(z)$, and finally sample $x \sim p_\theta(\cdot|z)$ using $f_\theta(z)$â€‹.

æ¥ç€æˆ‘ä»¬éœ€è¦è®¡ç®— $p_\theta(x)$ï¼Œå› ä¸ºæˆ‘ä»¬é€šå¸¸ä½¿ç”¨æå¤§ä¼¼ç„¶ä¼°è®¡çš„æ–¹æ³•æ¥è·å¾—æœ€ä¼˜å‚æ•°ã€‚åœ¨ä¹‹å‰çš„ Fundamental Maths ç« èŠ‚ä¸­ä»‹ç»äº†ï¼Œå¯ä»¥ä½¿ç”¨å…¨æ¦‚ç‡å…¬å¼æ¥è¿›è¡Œè®¡ç®—
$$
p _ { \theta } ( x ) = \int p _ { \theta } ( x | z ) p ( z ) d z 
$$
ä½†æ˜¯å¦‚ä¹‹å‰æ‰€åˆ†æï¼Œç”±äºé«˜ç»´åº¦éšå˜é‡ä»¥åŠå¤æ‚ç¥ç»ç½‘ç»œå»ºæ¨¡ï¼Œè¿™ä¸ªç§¯åˆ†æ˜¯æ²¡åŠæ³•è®¡ç®—çš„ğŸ˜•æ­¤æ—¶éœ€è¦å¦å¯»ä»–è·¯ï¼Œå¥½æ¶ˆæ¯æ˜¯é€šè¿‡è´å¶æ–¯æ³•åˆ™æˆ‘ä»¬è¿˜æœ‰ä¸€ä¸ªæ±‚è§£æ–¹æ³•
$$
p_\theta(x) = \frac{p_\theta(x|z) p(z)}{p_\theta(z|x)}
$$
ä½†æ˜¯æ–°çš„é—®é¢˜åˆå‡ºç°äº†ï¼šå¦‚ä½•è·å¾— $p_\theta(z|x)$ğŸ¤”æ­¤æ—¶åˆåªèƒ½æ±‚åŠ©äºè¿‘ä¼¼çš„åŠ›é‡ï¼Œå°±åƒä¹‹å‰å®šä¹‰äº†ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥è·å¾—è¿‘ä¼¼çš„ $p_\theta(x|z)$ åˆ†å¸ƒï¼Œç°åœ¨æˆ‘ä»¬éœ€è¦å†å®šä¹‰ä¸€ä¸ªæ–°çš„å‚æ•°ç»„ï¼Œæ¥è¿‘ä¼¼ï¼Œå³ï¼š $q_\phi(z|x) â‰ˆp_\theta(z|x)$â€‹ã€‚å¢åŠ å»ºæ¨¡æ¡ä»¶
$$
p_\theta(x) = \frac{p_\theta(x|z) p(z)}{q_\phi(z|x)}
$$


**é—®é¢˜å»ºæ¨¡ 2.0**

- å®šä¹‰å‡½æ•° $q_\phi$ï¼Œé€šå¸¸ä¸ºä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå…¶å‚æ•°ä¸º $\phi$ï¼Œè¾“å…¥ä¸ºè§‚æµ‹æ•°æ® $x$â€‹ã€‚å…¶ä½œç”¨æ˜¯è¿‘ä¼¼è®¡ç®—ç»™å®šè§‚æµ‹æ•°æ®ç”Ÿæˆéšå˜é‡çš„è¿‡ç¨‹

**é—®é¢˜å»ºæ¨¡ 3.0**

è¿™æ ·éšæ„çš„è¿‘ä¼¼æ˜¯åˆç†çš„å—ï¼Ÿ**æˆ‘ä»¬å…¶å®å¹¶æ²¡æœ‰è¯æ˜ä¸Šè¿°è¿‘ä¼¼çš„åˆç†æ€§**ï¼Œä½†æ˜¯éšç€æ·±å…¥åˆ†æç›®æ ‡å‡½æ•°ï¼Œå°±èƒ½ç†è§£è¿‘ä¼¼çš„åˆç†æ€§ã€‚ç°åœ¨æˆ‘ä»¬å¼•å…¥éœ€è¦ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°ï¼šexpectation of log-likelihood
$$
\mathbb{E}_{x \sim p^*(x)} [\ln p_\theta(x)] = -H(p^*) - D_{KL}(p^*(x) \| p_\theta(x))
$$
ç­‰å¼å³ä¾§çš„ç¬¬ä¸€é¡¹å°±æ˜¯çœŸå®åˆ†å¸ƒçš„ç†µï¼ˆentropyï¼‰ï¼Œç¬¬äºŒé¡¹å³ä¸ºçœŸå®åˆ†å¸ƒä¸è¿‘ä¼¼åˆ†å¸ƒçš„ KL æ•£åº¦ã€‚å¯ä»¥çœ‹åˆ°ç¬¬ä¸€é¡¹çš„ç†µæ˜¯ä¸€ä¸ªä¸åŒ…å«å‚æ•° $\theta$ çš„é¡¹ï¼ˆå¯è®¤ä¸ºæ˜¯å¸¸æ•°ï¼‰ï¼Œæ‰€ä»¥å½“æˆ‘ä»¬åœ¨æœ€å¤§åŒ–æœŸæœ›æ—¶ï¼Œç­‰ä»·äºåœ¨æœ€å°åŒ– KL æ•£åº¦ã€‚è¿›è€Œæ¨å‡ºï¼šå¦‚æœå·¦ä¾§æœŸæœ›ä¸ºæœ€å¤§å€¼ï¼Œé‚£ä¹ˆå³ä¾§çš„ KL æ•£åº¦åº”è¯¥ä¸º0ï¼ˆå¯ä»¥ç”¨åè¯æ³•è½»æ¾è¯æ˜ï¼‰ï¼Œæ­¤æ—¶ $p*=p_\theta$ï¼Œä¹Ÿå°±è¯æ˜äº†ä½¿ç”¨ç”¨ $\theta$ è¿‘ä¼¼çš„åˆç†æ€§ã€‚é‚£ä¹ˆé‡ç‚¹å°±æ˜¯ä¼˜åŒ–å·¦ä¾§çš„æœŸæœ›ç†µï¼Œä¸è¿‡æˆ‘ä»¬è¦è°ƒæ•´ä¸€ä¸‹æ±‚å¾—æœŸæœ›çš„å½¢å¼ï¼Œå› ä¸ºä¸Šè¿°å…¬å¼ä¸­çš„ $p*$ æ˜¯ä¸å¯çŸ¥çš„ã€‚æ­¤æ—¶ç›´æ¥ä½¿ç”¨ importance sampling çš„æ–¹æ³•è¿‘ä¼¼è·å¾—æœŸæœ›
$$
N \max_{\theta} \mathbb{E}_{x \sim p^*(x)} [\ln p_\theta(x)] \approx \max_{\theta} \sum_i \ln p_\theta(x_i)
$$
æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è®¤ä¸ºæ˜¯ proposal distribution å’Œ target distribution å°±æ˜¯ä¸€æ ·çš„ï¼Œå³ï¼šæ ·æœ¬å°±æ˜¯ä» $p*$ åˆ†å¸ƒä¸­äº§ç”Ÿï¼Œæ‰€ä»¥ä¸éœ€è¦ä¹˜ä»¥ ratio è°ƒæ•´

### Evidence Lower Bound (ELBO)

åœ¨ä¸Šè¿°è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å»ºæ¨¡äº†ä¸€ä¸ªç¥ç»ç½‘ç»œ $q_\phi$ æ¥è¿‘ä¼¼åéªŒåˆ†å¸ƒ $p_\theta(z|x)$ï¼Œä½†å®é™…ä¸Šæˆ‘ä»¬ä¸èƒ½å¤Ÿç›´æ¥ä½¿ç”¨è¿™ä¸ªä¼°è®¡ç½‘ç»œå¸¦å…¥åˆ°ç›®æ ‡å‡½æ•°å½“ä¸­ï¼Œå› ä¸º
$$
\ln p_{\theta}(x) \neq \ln \frac{p_{\theta}(x|z)p(z)}{q_{\phi}(z|x)}
$$
å¦‚æœæˆ‘ä»¬å»æœ€å¤§åŒ–ç­‰å¼çš„å³ä¾§ï¼Œå¹¶ä¸ä¼šå¸¦æ¥ç­‰å¼å·¦ä¾§çš„å€¼çš„ä¸Šå‡ï¼Œå³ï¼šæ­¤ä¼˜åŒ–ç®—æ³•çš„æ”¶æ•›æ€§æ˜¯æ²¡æœ‰ä¿è¯çš„âŒæ­¤æ—¶æˆ‘ä»¬é€‰æ‹©ä¼˜åŒ–çš„æ˜¯ Evidence lower bound (ELBO)

æˆ‘è¿™é‡Œç›´æ¥åˆ—å‡ºç­‰å¼
$$
\log p_\theta(\boldsymbol{x}) = \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log \frac{p_\theta(\boldsymbol{x}, \boldsymbol{z})}{q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})} \right] + D_{\mathrm{KL}}(q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x}) \| p_\theta(\boldsymbol{z}|\boldsymbol{x}))
$$
å…¶ä¸­ $q_\phi(Â·|x)$ æ˜¯ä»»æ„çš„åˆ†å¸ƒï¼ŒELBO ä¸ºç­‰å¼å³ä¾§çš„ç¬¬ä¸€é¡¹
$$
ELBO=\mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log \frac{p_\theta(\boldsymbol{x}, \boldsymbol{z})}{q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})} \right]\\
$$

ç­‰å¼æ¨å¯¼è¿‡ç¨‹å¦‚ä¸‹ï¼Œå‚è€ƒè‡ª [link](https://arxiv.org/pdf/2208.11970)
$$
\begin{aligned}
  \log p_\theta(\boldsymbol{x}) &= \log p_\theta(\boldsymbol{x}) \int q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x}) d\boldsymbol{z} \\
  &= \log p_\theta(\boldsymbol{x}) \left( \int q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x}) d\boldsymbol{z} \right) \\
  &= \int q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x}) (\log p_\theta(\boldsymbol{x})) d\boldsymbol{z} \\
  &= \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log p_\theta(\boldsymbol{x}) \right] \\
  &= \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log \frac{p_\theta(\boldsymbol{x}, \boldsymbol{z})}{p_\theta(\boldsymbol{z}|\boldsymbol{x})} \right] \\
  &= \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log \frac{p_\theta(\boldsymbol{x}, \boldsymbol{z}) q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})}{p_\theta(\boldsymbol{z}|\boldsymbol{x}) q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})} \right] \\
  &= \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log \frac{p_\theta(\boldsymbol{x}, \boldsymbol{z})}{q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})} \right] + \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log \frac{q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})}{p_\theta(\boldsymbol{z}|\boldsymbol{x})} \right] \\
  &= \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log \frac{p_\theta(\boldsymbol{x}, \boldsymbol{z})}{q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})} \right] + D_{\mathrm{KL}}(q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x}) \| p_\theta(\boldsymbol{z}|\boldsymbol{x})) \\
  &\geq \mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log \frac{p_\theta(\boldsymbol{x}, \boldsymbol{z})}{q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})} \right]
  \end{aligned}
$$
  å…¶å®æ¨å¯¼å‡º ELBO çš„æ–¹æ³•æœ‰å¾ˆå¤šï¼Œéƒ½ç¦»ä¸å¼€ä¸¤ä¸ªæ­¥éª¤ï¼š

  1. å¼•è¿›éšå˜é‡ï¼Œå¹¶ä½¿ç”¨è´å¶æ–¯å…¬å¼å°†å…¶è¿›è¡Œè½¬æ¢
     $$
     p(x)=\frac{p(x,z)}{p(z|x)}
     $$

  2. å¯¹ç­‰å¼çš„ä¸¤ä¾§ä¹˜ä»¥ $q_\phi$ (not $p_\theta$) è¿›è¡Œç§¯åˆ†ä»¥è·å¾—å…³äº $z\sim q_\phi$ çš„æœŸæœ›ã€‚è¿™ä¸€æ­¥çš„ç§¯åˆ†çœ‹èµ·æ¥åŒªå¤·æ‰€æ€ï¼Œå¦‚æœä» KL æ•£åº¦å‡ºå‘è¿›è¡Œæ¨å¯¼ï¼Œè¿™ä¸ªç§¯åˆ†æ‰ä¼šæ¯”è¾ƒè‡ªç„¶åœ°è¯ç”Ÿï¼Œ[zhihu](https://zhuanlan.zhihu.com/p/685814830) [Lilian's blog](https://lilianweng.github.io/posts/2018-08-12-vae/#loss-function-elbo)
     $$
     \text{KL}(q_\phi(z|x) \| p_\theta(z|x)) = \int_z q_\phi(z|x) \log \frac{q_\phi(z|x)}{p_\theta(z|x)} dz
     $$
     åœ¨ä¸Šè¿°åšå®¢ä¸­è¿˜ä»‹ç»äº† ELBO çš„å¦ä¸€ç§å¸¸è§å½¢å¼ï¼Œæˆ‘ä¹Ÿåˆ—åœ¨ä¸‹é¢
     $$
     \begin{aligned}
     ELBO &=\mathbb{E}_{q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})} \left[ \log \frac{p_\theta(\boldsymbol{x}, \boldsymbol{z})}{q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x})} \right] \\
     &= \log p_\theta(\boldsymbol{x|z}) - KL(q_{\boldsymbol{\phi}}(\boldsymbol{z}|\boldsymbol{x}) \parallel p_\theta(\boldsymbol{z}))
     \end {aligned}
     $$
     æœ‰äº›æ–‡ç« è®¤ä¸ºç¬¬äºŒé¡¹æ˜¯ä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œé˜²æ­¢åéªŒåˆ†å¸ƒä¸å…ˆéªŒåˆ†å¸ƒä¹‹é—´æœ‰è¿‡å¤§å·®è·
     

### Proof of ELBO

æ¥ä¸‹æ¥å°±éœ€è¦åšä¸€ä»¶é‡è¦çš„äº‹ï¼š**è¯æ˜æœ€å¤§åŒ– ELBO èƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬è·å¾—æœ€ä¼˜è§£ï¼Œå³æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶**ã€‚æœ‰äº†è¿™ä¸ªè¯æ˜æ‰èƒ½è¯´æ˜ä¸Šè¿° $q_\phi$ å»ºæ¨¡çš„åˆç†æ€§ï¼Œå¦åˆ™ä¼˜åŒ– ELBO å°±æ˜¯æ— ç”¨åŠŸã€‚è¿™ä¸ªç»“è®ºå¹¶ä¸æ˜¾ç„¶ï¼Œå› ä¸º ELBO åªæ˜¯ä¸€ä¸ªä¸‹ç•Œï¼Œå…¶ä¸çœŸå®çš„å¯¹æ•°ä¼¼ç„¶è¿˜ç›¸å·®ä¸€ä¸ª KL æ•£åº¦ã€‚å½“æˆ‘ä»¬åœ¨æœ€å¤§åŒ– ELBO çš„æ—¶å€™ï¼ŒKL æ•£åº¦ä¼šå¦‚ä½•å˜åŒ–ï¼Ÿå…¶å˜åŒ–æ˜¯å¦ä¼šå½±å“æˆ‘ä»¬çš„ä¼˜åŒ–ï¼Ÿå¯ä»¥å‚è€ƒ [bilibili-EM algorithm](https://www.bilibili.com/video/BV1qW411k7ao) ä¸­åˆ©ç”¨ï¼ˆå¹¿ä¹‰ï¼‰ EM ç®—æ³•è¯æ˜ä¼˜åŒ– ELBO æ˜¯åˆç†çš„ã€‚è¿™é‡Œæˆ‘ç»™å‡ºæˆ‘è‡ªå·±çš„ç†è§£ï¼š

1.  å¦‚æœ ELBO æ˜¯æå¤§å€¼ï¼Œé‚£ä¹ˆæ­¤æ—¶ KL æ•£åº¦å¿…å®šä¸º0ã€‚è¿™æ˜¯å› ä¸ºç­‰å¼å·¦ä¾§çš„å¯¹æ•°ä¼¼ç„¶ä¸åŒ…å«å‚æ•° $\phi$ï¼Œæ‰€ä»¥å¯¹å…¶åå¯¼ä¸º0ï¼Œå¦‚æœå›ºå®š $\theta$ï¼Œåˆ™å½“ ELBO å¢å¤§æ—¶ï¼ŒKL æ•£åº¦ä¸€å®šå‡å°ï¼›ELBO å‡å°æ—¶ï¼ŒKL æ•£åº¦å¿…å®šå¢å¤§ã€‚åˆ©ç”¨åè¯æ³•ï¼Œå¦‚æœ ELBO æ˜¯æå¤§å€¼ï¼Œä¸” KL æ•£åº¦ä¸ä¸ºé›¶ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡æ›´æ–° $\phi$ æ‰¾åˆ°æ›´å°çš„ KL æ•£åº¦ï¼Œæ­¤æ—¶ ELBO å°†ç»§ç»­å¢å¤§ï¼Œä¸ ELBO ä¸ºæå¤§å€¼çŸ›ç›¾ã€‚
2.  å¦‚æœ ELBO ä¸ºæå¤§å€¼ï¼Œåˆ™æœ‰ KL æ•£åº¦ä¸º 0ï¼Œé‚£ä¹ˆæ­¤æ—¶å¯¹æ•°ä¼¼ç„¶ç­‰äº ELBOï¼Œå³å¯¹æ•°ä¼¼ç„¶ä¹Ÿä¸ºæå¤§å€¼ï¼Œè¯æ¯•

ä»¥ä¸Šè¯æ˜ç»™ä¸ºä½•ä¼˜åŒ– ELBO æ˜¯åˆç†çš„æä¾›æ€è·¯ï¼Œå¹¶ä¸ä¸¥è°¨ï¼Œè¯¦ç»†è¯æ˜ä»éœ€è¦ç ”è¯» EM ç®—æ³•

### Misc Questions

- ä¸ºä»€ä¹ˆå»ä¼˜åŒ– ELBO è€Œä¸å»ç›´æ¥ä¼˜åŒ– $\ln p_{\theta}(x) = \ln \mathbb{E}_{z \sim q_{\phi}(\cdot|x)} \left[ \frac{p_{\theta}(x, z)}{q_{\phi}(z|x)} \right]$â€‹

  è¿™æ˜¯ä¸€ä¸ªå°æ’æ›²ï¼Œä½†æ˜¯èŠ±è´¹äº†æˆ‘å¤§é‡çš„æ—¶é—´æ¥è¿›è¡Œè¯¢é—® DeepSeekï¼Œä»¥è·å¾—æ­£ç¡®çš„ç†è§£

  ä¸»è¦åŸå› æœ‰ä¸¤ä¸ªï¼š

  1. importance weight æ‰€å¸¦æ¥çš„æå¤§æ–¹å·®
  2. MC sampling æ‰€å¸¦æ¥çš„åå·®

  è¿™ä¸¤ä¸ªåŸå› å¯¼è‡´ç›´æ¥ä¼˜åŒ–è¯¥å¼å­æ˜¯ä¸å¯è¡Œçš„ï¼Œåè€Œå»ä¼˜åŒ– ELBO æˆ‘ä»¬å°±èƒ½è·å¾—ç¨³å®šçš„æ¢¯åº¦å’Œæ— åçš„æ¢¯åº¦ä¼°è®¡

  > - **High variance** means the gradient estimates fluctuate wildly across different samples (unreliable updates).
  > - **Low variance** means the estimates are consistent across samples (stable training).

  >  The term is an **importance weight**, which can have extremely high variance:

- ä¸Šè¿°æ¨å¯¼éƒ½ä»…é™äºå•æ ·æœ¬ï¼Œä¸ºä»€ä¹ˆä¸ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†è¿›è¡Œæ¨å¯¼ï¼Ÿ

  è¯¥é—®é¢˜çš„ç­”æ¡ˆå’Œ mini-batch éšæœºæ¢¯åº¦ä¸‹é™æ˜¯ä¸€æ ·çš„ã€‚å¦‚æœä½¿ç”¨ batch å¤§å°ä¸ºæ•´ä¸ªæ•°æ®é›†ï¼Œåˆ™æ˜¯ç”¨æ•´ä¸ªæ•°æ®é›†çš„æ¢¯åº¦æ–¹å‘çš„å¹³å‡ä½œä¸ºæ›´æ–°ï¼Œæœ€ç»ˆæ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ã€‚è€Œä½¿ç”¨ mini-batch ä¸æ–­è¿›è¡Œè¿­ä»£æ±‚è§£ï¼Œä¹Ÿèƒ½æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ã€‚æ‰€ä»¥ç ”ç©¶å•æ ·æœ¬çš„æ¢¯åº¦æ¨å¯¼èƒ½å¤Ÿä½œç”¨äºæ•´ä¸ªæ•°æ®é›†çš„æ¢¯åº¦æ¨å¯¼


### Variational Auto Encoder

å…¶å®ä¸Šè¿°è¿‡ç¨‹å·²ç»æŠŠæ•´ä¸ª VAE éƒ½æ¢³ç†å¤„ç†å‡ºæ¥äº†ï¼Œç”¨ä¸‹å›¾å°±å¯ä»¥å¾ˆæ¸…æ™°åœ°çœ‹åˆ°è‡ªç¼–ç å™¨çš„ç»“æ„

<img src="Denoising Diffusion Probabilistic Models/autoencoder-architecture.png" alt="img" style="zoom:80%;" />

è¿™é‡Œæˆ‘æŠŠ VAE å’Œä¸Šè¿°çš„ ELBO åˆ†æå¯¹åº”èµ·æ¥ï¼Œå¹¶æ‹¿å‡ºæ›´å…·ä½“çš„å»ºæ¨¡å’ŒæŸå¤±å‡½æ•°ä»¥æ–¹ä¾¿ç†è§£ï¼š

- å»ºæ¨¡åéªŒåˆ†å¸ƒ $z\sim p_\phi(z|x)$ æœä»é«˜æ–¯åˆ†å¸ƒ $N(\mu_\phi, \sigma_\phi^2)$

  Encoder $g_\phi$ï¼Œç”¨äºç”Ÿæˆå‡å€¼å’Œæ–¹å·® $g_\phi(x)=(\mu_\phi,\sigma_\phi)$

- å»ºæ¨¡ä¼¼ç„¶åˆ†å¸ƒ $x\sim p_\theta(x|z)$ æœä»é«˜æ–¯åˆ†å¸ƒ $N(\mu_\phi, \sigma^2)$

  Decoder $f_\theta$ï¼Œç”¨äºç”Ÿæˆå‡å€¼ $f_\theta(z) = \mu_\theta$â€‹ï¼Œæ–¹å·®ä¸ºå¸¸é‡ï¼Œå¹¶ä¸æ˜¯æˆ‘ä»¬æ‰€å…³å¿ƒçš„

- å»ºæ¨¡å…ˆéªŒåˆ†å¸ƒ $z\sim p(z)$ æœä»é«˜æ–¯åˆ†å¸ƒ $N(0,I)$

- æŸå¤±å‡½æ•°ä¸º ELBO æŸå¤±å‡½æ•°
  $$
  \mathrm{ELBO} = \underbrace{\mathbb{E}_{q_\phi(z|x)} \left[ \log p_\theta(x|z) \right]}_{\text{reconstruction}} - \underbrace{D_{\mathrm{KL}}(q_\phi(z|x) \parallel p(z))}_{\text{regulerization}}
  $$
  ç”±äºé«˜æ–¯åˆ†å¸ƒå¸¦å…¥åˆ°å¯¹æ•°å’Œ KL æ•£åº¦ä¸­ä¼šæœ‰æå¤§çš„ç®€åŒ–ï¼ŒELBO ç¬¬ä¸€é¡¹ä¸ºé‡å»ºé¡¹ï¼Œå¯å†™ä¸º
  $$
  \log p_\theta(x|z) = -\frac{1}{2\sigma^2}  ||x - \mu_\theta(z)||^2 + C
  $$
  ç¬¬äºŒé¡¹ä¸ºæ­£åˆ™é¡¹ï¼Œä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒçš„ KL æ•£åº¦æœ‰è§£æè¡¨è¾¾å¼ï¼Œå¯å†™ä¸º
  $$
  D_{\mathrm{KL}} = \frac{1}{2} \left[ \mathrm{tr}(\Sigma) + \mu^T \mu - k - \ln |\Sigma| \right]
  $$

OKï¼Œç°åœ¨æ‰€æœ‰å…ƒç´ éƒ½åˆ°é½äº†ï¼Œå¯ä»¥æ„‰å¿«åœ°ç”¨æ¢¯åº¦ä¸‹é™è¿›è¡Œä¼˜åŒ–äº†ğŸ˜‹å¯ä»¥çœ‹åˆ°ç¬¬ä¸€ä¸ªé‡å»ºæŸå¤±å°±æ˜¯å¸¸ç”¨çš„ MSE æŸå¤±å‡½æ•°ï¼Œè¿™é‡Œå†ä¸€æ¬¡çœ‹åˆ°äº†ï¼šåœ¨é«˜æ–¯åˆ†å¸ƒå‡è®¾ä¸‹æå¤§ä¼¼ç„¶ä¼°è®¡ä¸æœ€å°äºŒä¹˜æ³•ä¹‹é—´çš„ç­‰ä»·æ€§

ä¸ªäººç†è§£ï¼šVAE = Deep Nueral Nets + ELBO + Gradient Descentï¼Œç”¨ç¥ç»ç½‘ç»œå»ºæ¨¡ encode & decoderï¼Œä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™æ–¹æ³•ä¼˜åŒ– ELBO æŸå¤±å‡½æ•°æ¥è®­ç»ƒç¥ç»ç½‘ç»œ

åœ¨ä¹‹åçš„ VAE ç ”ç©¶ä¸­ï¼Œå¯¹äºè¿™ä¸ªæ­£åˆ™åŒ–é¡¹æœ‰ä¸å°‘è®¨è®ºï¼Œä¾‹å¦‚ $\beta$-VAEï¼Œå°±æ˜¯ç»™æ•´ä¸ªæ­£åˆ™åŒ–é¡¹æ·»åŠ æƒé‡ç³»æ•° $\beta$â€‹

- $Î²=1$ï¼šæ ‡å‡† VAE
- $Î²>1$ï¼šå¼ºè°ƒéšç©ºé—´ç»“æ„åŒ–ï¼Œç‰ºç‰²é‡æ„è´¨é‡
- $Î²<1$ï¼šå¼ºè°ƒé‡æ„è´¨é‡ï¼Œéšç©ºé—´çº¦æŸæ”¾æ¾

## From VAE to Diffusion

- Inference process (q process) is known, which gives equations of $x_t, x_{t-1},x_0, \epsilon$
- Markov chain is assumed
- Optimizing ELBO -> how to model & sample & train
-  how to unify the first term (reconstruction) in ELBO with third term (denoising) 

## Fundamental Maths

æ€»ç»“ç†è§£ VAE & Diffusion model æ‰€éœ€è¦çš„åŸºç¡€æ•°å­¦ï¼Œä¸»è¦å°±æ˜¯è´å¶æ–¯ç†è®ºï¼ˆBayesian Theoremï¼‰ä»¥åŠç›¸å…³çš„æ¦‚ç‡è®ºåŸºç¡€

- Bayesian Theorem [wiki](https://en.wikipedia.org/wiki/Bayes%27_theorem) & Bayesian Inference

  > Bayes' theorem (alternatively Bayes' law or Bayes' rule, after [Thomas Bayes](https://en.wikipedia.org/wiki/Thomas_Bayes)) gives a mathematical rule for inverting [conditional probabilities](https://en.wikipedia.org/wiki/Conditional_probability), allowing one to **find the probability of a cause given its effect.**

  ä¸Šé¢å°±æ˜¯ wiki çš„ç¬¬ä¸€å¥è¯ï¼Œæ˜¯å¯¹è´å¶æ–¯ç†è®ºçš„é«˜åº¦æ€»ç»“ï¼šfind the probability of a cause given its effectï¼Œç»™å®šè¿™äº›ç°è±¡æ¥å¯»æ‰¾åŸå› ã€‚è¿™ä¸ªåŠŸèƒ½æ˜¯ä¸€ä¸ªéå¸¸å¼ºå¤§çš„åŠŸèƒ½ï¼Œä½†å…¶å…¬å¼å´ç›¸å½“çš„ç®€å•
  $$
  P(A|B) = \frac{P(B|A)P(A)}{P(B)}
  $$
  è¿™é‡Œçš„ A å’Œ B éƒ½æ˜¯ä»»æ„çš„éšæœºäº‹ä»¶ï¼Œå¹¶ä¸” P(B) ä¸ä¸º 0ã€‚è¿™é‡Œçš„ A å’Œ B çœ‹èµ·æ¥éå¸¸çš„æŠ½è±¡ï¼Œå¦‚ä½•å¯¹åº”åˆ°å®é™…åº”ç”¨å½“ä¸­ï¼Ÿåœ¨ç°å®ä¸­ï¼ŒA é€šå¸¸ç”¨æ¥è¡¨ç¤º Hypothesisï¼Œå³æˆ‘ä»¬çš„å‡è®¾ï¼ŒB é€šå¸¸ç”¨æ¥è¡¨ç¤º Evidenceï¼Œå³å‘ç”Ÿçš„ç°è±¡ï¼Œæ‰€ä»¥ä¹Ÿé€šå¸¸çœ‹åˆ°ä¸‹æ–¹çš„å­—æ¯è¡¨ç¤º
  $$
  P(H|E) = \frac{P(E|H)P(H)}{P(E)}
  $$
  ä¸ºäº†æ›´è¿›ä¸€æ­¥ç†è§£ï¼Œæˆ‘ä»¬å°†è¿™å‡ ä¸ªæ¦‚ç‡è¡¨ç¤ºçš„æ„ä¹‰å†™ä½œå¦‚ä¸‹ï¼š

  1. $P(H)$ï¼ŒPriorï¼Œå…ˆéªŒã€‚æˆ‘ä»¬é€šå¸¸ç§° hypothesis ä¸ºå…ˆéªŒ
  2. $P(E)$ï¼ŒEvidenceã€‚åç§°æ²¡æœ‰å˜åŒ–
  3. $P(E|H)$ï¼ŒLikelihoodï¼Œä¼¼ç„¶ã€‚åŸºäº hypothesis æ‰€å¾—å‡ºçš„äº‹ä»¶æ¦‚ç‡å³ä¸ºä¼¼ç„¶
  4. $P(H|E)$â€‹ï¼ŒPosteriorï¼ŒåéªŒã€‚åŸºäº evidence æ‰€æ›´æ–°çš„ hypothesis 

  æ­¤æ—¶è´å¶æ–¯å…¬å¼çš„åŠŸèƒ½å˜å¾—æ›´åŠ å…·è±¡èµ·æ¥ï¼šæ ¹æ®äº‹å®æ¥æ›´æ–°æˆ‘ä»¬çš„å‡è®¾ã€‚é‚£ä¹ˆè¿™ä¸ªå‡è®¾åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿäº‹å®åˆæ˜¯ä»€ä¹ˆï¼Ÿç”¨ä¸€ä¸ªæ›´åŠ å…·è±¡çš„ä¾‹å­è¡¨ç¤ºï¼š
  $$
  P(\theta|X) = \frac{P(X|\theta)P(\theta)}{P(X)}
  $$
  è¿™ä¸‹å­ç¬¦å·ä¼¼ä¹çœ‹èµ·æ¥æ›´ç†Ÿæ‚‰äº†ï¼š$\theta$ å°±æ˜¯æ¨¡å‹å‚æ•°ï¼Œ$X$ å°±æ˜¯æ•°æ®æ ·æœ¬ï¼Œæˆ‘ä»¬çš„ä»»åŠ¡å°±æ˜¯ç”¨æ¨¡å‹æ¥ä¼°è®¡æ ·æœ¬çš„åˆ†å¸ƒã€‚å…¶ä¸­æˆ‘ä»¬ç”¨ $\theta$ æ¥è¡¨ç¤ºäº†æˆ‘ä»¬çš„æ¨¡å‹å‚æ•°ï¼Œ**å…¶å®è¿™æ˜¯ä¸€ä¸ª over simplificationï¼Œå…¶ä¸­è¿˜åŒ…å«äº†æˆ‘ä»¬çš„å»ºæ¨¡å‡è®¾**ï¼Œä¾‹å¦‚ï¼šè¿™ä¸ªæ¨¡å‹æ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œä¸”æ•°æ®çš„åˆ†å¸ƒç¬¦åˆé«˜æ–¯åˆ†å¸ƒï¼Œé«˜æ–¯åˆ†å¸ƒçš„å‚æ•°ä¸º $\theta$ã€‚æ‰€ä»¥æˆ‘ä»¬èƒ½å¤Ÿéå¸¸è½»æ¾åœ°æ ¹æ®è¿™ä¸ªå‡è®¾è®¡ç®—å¾—åˆ° $P(X|\theta)$â€‹ï¼Œç›´æ¥æ ¹æ®é«˜æ–¯åˆ†å¸ƒæ¥ç®—å°±è¡Œäº†

  ç†è§£è¿™ä¸ªå…¬å¼æœ€å¸¸ä¸¾çš„ä¾‹å­å°±æ˜¯æŠ›ç¡¬å¸çš„ä¾‹å­ï¼š

  æˆ‘ä»¬å°†ä¸Šè¿°çš„å˜é‡éƒ½è¿›è¡Œå…·ä½“çš„å®šä¹‰

  - $\theta$ æ˜¯ä¸€ä¸ªéšæœºå˜é‡ï¼Œå…¶å†³å®šäº†ç¡¬å¸ä¸ºæ­£é¢çš„æ¦‚ç‡
  - $X$ æ˜¯å®éªŒç»“æœï¼Œæˆ‘ä»¬çš„å®éªŒç»“æœä¸ºæŠ›10æ¬¡ç¡¬å¸ï¼Œæœ‰7æ¬¡ä¸ºæ­£é¢ï¼Œ3æ¬¡ä¸ºåé¢

  é¦–å…ˆæˆ‘ä»¬éœ€è¦æœ‰ä¸€ä¸ªåˆå§‹çŒœæµ‹ï¼š$\theta$ åˆ°åº•æ˜¯ä¸ªä»€ä¹ˆåˆ†å¸ƒï¼Ÿç”±äºä¸€å¼€å§‹æˆ‘ä»¬æ²¡æœ‰ä»»ä½•ä¿¡æ¯ï¼Œä¸å¦‚å‡è®¾ä¸ºæœ€ç®€å•çš„ uniform distribution (å‡åŒ€åˆ†å¸ƒ) $P(\theta)=1, \theta\in [0,1]$

  æˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“æ ¹æ®æˆ‘ä»¬çš„å‡è®¾è®¡ç®—å¾—åˆ°æˆ‘ä»¬çš„ likelihood 
  $$
  P(X|\theta) = C_{10}^7Â·\theta^7(1-\theta)^3
  $$
  OKï¼Œç°åœ¨æ¯”è¾ƒéš¾çš„æ˜¯æ±‚å¾— $P(X)$ï¼Œè¿™é‡Œéœ€è¦ä½¿ç”¨å…¨æ¦‚ç‡å…¬å¼ï¼Œä¸è¿‡å¥½åœ¨æˆ‘ä»¬ä¹Ÿèƒ½å¤Ÿæ±‚åˆ°
  $$
  P(X) = \int_0^1P(X|\theta)P(\theta)d\theta
  $$
  æœ€åæ±‚å¾— $P(X)â‰ˆ0.1$ï¼Œæ˜¯ä¸€ä¸ªå¸¸æ•°ã€‚æ‰€ä»¥å°†æ‰€æœ‰çš„ç»“æœå¯¼å…¥åˆ°è´å¶æ–¯å…¬å¼å½“ä¸­ï¼Œå°±å¯ä»¥å¾—åˆ°
  $$
  P(\theta|X) = \frac{C_{10}^7Â·\theta^7(1-\theta)^3Â·1}{0.1}
  $$
  å¯ä»¥çœ‹åˆ°ï¼Œç°åœ¨æˆ‘ä»¬çš„ $\theta$ è¢«æ›´æ–°ä¸ºäº†ä¸€ä¸ª [beta distribution](https://en.wikipedia.org/wiki/Beta_distribution)ï¼Œç›¸æ¯”äºä¹‹å‰çš„ uniform distribution æ”¹åŠ¨ä¸å°ã€‚å¦‚æœå®éªŒé‡è¶³å¤Ÿå¤šï¼Œé‚£ä¹ˆæˆ‘ä»¬æ‰€ç®—å‡ºçš„ $\theta$ åº”è¯¥è¶‹è¿‘äºä¸€ä¸ª delta åˆ†å¸ƒï¼Œæ”¶æ•›åˆ° $\frac{heads}{trials}$â€‹ è¿™ä¸ªå€¼ï¼Œå¹¶ä¸”æ— è®ºä½ çš„åˆå§‹ $P(\theta)$ æ˜¯å¤šå°‘ï¼Œéƒ½ä¼šæ”¶æ•›åˆ°æœ€ç»ˆè¿™ä¸ªåˆ†å¸ƒä¸Šã€‚æ‰€ä»¥è¿™ç»™æˆ‘ä¸€ä¸ªå¯å‘ï¼š

  æ— è®ºåˆå§‹ $P(\theta)$ åˆ†å¸ƒæ˜¯æ€æ ·çš„ï¼Œæ‰€æ”¶è·åˆ°çš„ $P(X|\theta)$ æ›´æ–°éƒ½æ˜¯ä¸€æ ·çš„ï¼Œè¿™æ˜¯ç”±æˆ‘ä»¬çš„å»ºæ¨¡æ‰€å†³å®šçš„ï¼Œå³æˆ‘ä»¬çš„æ¨¡å‹å‡è®¾ï¼š$\theta$ å†³å®šäº†ç¡¬å¸ä¸ºæ­£é¢çš„æ¦‚ç‡ã€‚å¹¶ä¸”å¦‚æœæ›´æ–°çš„ likelihood è¶³å¤Ÿå¼ºï¼Œé‚£ä¹ˆå°†å®Œå…¨è¦†ç›–ä¹‹å‰çš„å…ˆéªŒï¼Œä»¥ likelihood ä¸ºåŸºå‡†

  å¦å¤–å†æä¸€ç‚¹ï¼šæˆ‘ä»¬åœ¨è®¡ç®— $P(X)$ çš„æ—¶å€™èƒ½ä½¿ç”¨è¿™ä¸ªå…¨æ¦‚ç‡å…¬å¼ï¼Œä»ç„¶æ˜¯åœ¨æˆ‘ä»¬çš„æ¨¡å‹å‡è®¾ä¹‹ä¸‹çš„ã€‚å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡å‹å‡è®¾è´¯ç©¿äº†æ‰€æœ‰çš„è®¡ç®—è¿‡ç¨‹ï¼Œä¸€ä¸ªé”™è¯¯çš„æ¨¡å‹å‡è®¾ï¼Œå³ä½¿è®¡ç®—å†å¤šçš„å‚æ•°ï¼Œä¹Ÿæ— æ³•è·å¾—å¥½çš„åéªŒæ¦‚ç‡

  è¿™æ˜¯ä¸€ä¸ªéå¸¸éå¸¸ç®€å•çš„ä¾‹å­ï¼Œç®€å•åˆ°é€šå¸¸ä¼šç›´æ¥ç®— $\frac{heads}{trials}$ ä½œä¸ºç¡¬å¸ä¸ºæ­£çš„æ¦‚ç‡ã€‚è¿‡äºç®€å•çš„ä¾‹å­å°†æ©ç›–æ‰ä¸¤ä¸ªé—®é¢˜

  1. **$P(X|\theta)$ is actually really hard to model**
  2. **$P(X)$ is actually really hard to calculate**

  æˆ‘è¯¢é—®äº† DeepSeekï¼Œå¸Œæœ›å…¶ä¸¾ä¸€ä¸ªä¾‹å­æ¥è¯´æ˜ä¸ºä»€ä¹ˆè¿™ä¸¤ä¸ªé—®é¢˜åœ¨å®é™…åº”ç”¨ä¸­éå¸¸éš¾è§£ã€‚DeepSeek ç»™å‡ºçš„ä¾‹å­æ˜¯å¼•å…¥äº†éšå˜é‡ï¼Œè®©é—®é¢˜å˜å¾—æ›´åŠ å¤æ‚

  > å‡è®¾æˆ‘ä»¬æœ‰ä¸¤ç§ç¡¬å¸ï¼Œ**ç¡¬å¸1**çš„æ­£é¢æ¦‚ç‡ä¸º $p_1$ï¼Œ**ç¡¬å¸2**çš„æ­£é¢æ¦‚ç‡ä¸º $p_2$ï¼Œä¸”æ¯æ¬¡æŠ›ç¡¬å¸å‰ä¼šä»¥æ¦‚ç‡ $\alpha$ é€‰æ‹©ç¡¬å¸1ï¼Œä»¥ $1-\alpha$ é€‰æ‹©ç¡¬å¸2ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ° n æ¬¡æŠ›æ·çš„ç»“æœ $X={x_1,x_2,...,x_n}$ï¼ˆä¾‹å¦‚10æ¬¡ä¸­æœ‰7æ¬¡æ­£é¢ï¼‰ï¼Œä½†**ä¸çŸ¥é“æ¯æ¬¡æŠ›çš„æ˜¯å“ªä¸ªç¡¬å¸**ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ¨æ–­å‚æ•° $Î¸=(p1,p2,Î±)$

  æ­¤æ—¶æˆ‘ä»¬éœ€è¦è·å¾— $P(X|\theta)$â€‹ å°±ä¸æ˜¯é‚£ä¹ˆå®¹æ˜“çš„äº‹å„¿äº†ï¼å¯ç”¨å…¬å¼è¡¨è¾¾ä¸º
  $$
  P(X|\theta) = \sum_{z_1=1}^2 \sum_{z_2=1}^2 \cdots \sum_{z_n=1}^2 \left[ \prod_{i=1}^n P(x_i|z_i, \theta) P(z_i|\theta) \right].
  $$
  å¼å­ä¸­çš„å„ä¸ªæ¦‚ç‡è®¡ç®—å¦‚ä¸‹ï¼š
  $$
  P(z_i=1|\theta) = \alpha, \quad P(z_i=2|\theta) = 1-\alpha, \\
   P(x_i|z_i=1,\theta) = p_1^{x_i}(1-p_1)^{1-x_i}, \\
   P(x_i|z_i=2,\theta) = p_2^{x_i}(1-p_2)^{1-x_i}.
  $$
  å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰å¯èƒ½çš„**éšå˜é‡ç»„åˆç§¯åˆ†æ‰**ï¼Œæ‰èƒ½è·å¾—æœ€ç»ˆçš„ $P(X|\theta)$ï¼Œè¿™ä¸ªè®¡ç®—å¤æ‚åº¦æ˜¯éšç€å®éªŒæ¬¡æ•° n è€ŒæŒ‡æ•°ä¸Šå‡çš„ï¼ŒæŒ‰ç…§æˆ‘ä»¬çš„æ¡ä»¶åˆ™éœ€è¦è®¡ç®— $2^{10}$ é¡¹ ã€‚é‚£ä¹ˆå¦‚æœæˆ‘ä»¬è¿˜è¦å¯¹è¿™ä¸ªå¼å­è¿›è¡Œå…¨æ¦‚ç‡å…¬å¼çš„ç§¯åˆ†ï¼Œè®¡ç®—å¤æ‚åº¦å°±æ›´å¤§äº†
  $$
  P(X) = \int_{p_1} \int_{p_2} \int_{\alpha} P(X|\theta) P(\theta) \, d\alpha \, dp_1 \, dp_2
  $$
  å³ä½¿é‡‡ç”¨æ•°å€¼ç§¯åˆ†ï¼Œå¯¹äºé«˜ç»´ç©ºé—´çš„ç§¯åˆ†æˆæœ¬ä¹Ÿæ˜¯éå¸¸é«˜çš„ï¼Œä¾‹å¦‚ç”¨ç½‘æ ¼æ³•åˆ™éœ€è¦ $O(k^3)$

  æœ€åè‡ªå·±å†é«˜åº¦æ€»ç»“ä¸€ä¸‹å¯¼è‡´è¿™ä¸¤ä¸ªå›°éš¾çš„åŸå› ï¼š

  1. å½“æ¨¡å‹åŒ…å«éšå˜é‡æ—¶ï¼Œä¼¼ç„¶çš„è®¡ç®—æ¶‰åŠé«˜ç»´æ±‚å’Œæˆ–ç§¯åˆ†ï¼Œå¯¼è‡´è®¡ç®—é‡æŒ‡æ•°çˆ†ç‚¸

  2. å½“å‚æ•°ç©ºé—´ç»´åº¦å¢åŠ æˆ–æ¨¡å‹å¤æ‚æ—¶ï¼Œ$P(X)$ çš„è§£æè§£ä¸å¯å¾—ã€‚é«˜ç»´ä¼šæ˜¾è‘—å¢åŠ è®¡ç®—å¤æ‚åº¦ï¼Œè€Œå³ä½¿æ˜¯ä½ç»´æœ‰çš„å¼å­çš„è§£æè§£ä»ç„¶ä¸å¯è§£ï¼Œä¾‹å¦‚ä½ æ— æ³•å¯¹é«˜æ–¯åˆ†å¸ƒæ±‚è§£å®šç§¯åˆ†
     $$
     P(X) = \int_{0}^{b} \int_{-a}^{a} \left[ \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(z_i-\mu)^2}{2\sigma^2}} \right] d\sigma^2 d\mu.
     $$

  ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œå°±éœ€è¦è¿‘ä¼¼æ–¹æ³•äº†ğŸ˜å…¶ä¸­å°±åŒ…å«å˜åˆ†æ¨æ–­å’Œ MCMC æ–¹æ³•ï¼Œè€Œå˜åˆ†æ¨æ–­å°±æ˜¯ VAE çš„ç†è®ºåŸºç¡€ã€‚è€Œä»¥ä¸Šå»å¯»æ‰¾å‚æ•° $\theta$ çš„æ–¹æ³•å°±è¢«ç§°ä½œè´å¶æ–¯æ¨ç†ï¼ˆBayesian Inferenceï¼‰

  > FROM DeepSeek:
  >
  > **Bayesian Inference** æ˜¯ä¸€ç§ç»Ÿè®¡æ¨æ–­æ–¹æ³•ï¼ŒåŸºäºè´å¶æ–¯å®šç†ã€‚è´å¶æ–¯å®šç†æè¿°äº†åœ¨ç»™å®šæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•æ›´æ–°æˆ‘ä»¬å¯¹æŸä¸ªå‡è®¾æˆ–å‚æ•°çš„ä¿¡å¿µã€‚å…·ä½“æ¥è¯´ï¼Œè´å¶æ–¯æ¨æ–­é€šè¿‡ç»“åˆå…ˆéªŒçŸ¥è¯†ï¼ˆpriorï¼‰å’Œæ–°çš„è§‚æµ‹æ•°æ®ï¼ˆlikelihoodï¼‰æ¥è®¡ç®—åéªŒåˆ†å¸ƒï¼ˆposteriorï¼‰

- Joint distribution and conditional distribution

  

- Marginalize

  

- Chain of rules in probability

  https://en.wikipedia.org/wiki/Chain_rule_(probability)

- Markov Chain Monte Carlo (MCMC)

  [wiki](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo)

  [wiki](https://en.wikipedia.org/wiki/Monte_Carlo_method)

- Reparameterization trick

  [wiki](https://en.wikipedia.org/wiki/Reparameterization_trick) [Lilian's blog](https://lilianweng.github.io/posts/2018-08-12-vae/#reparameterization-trick)

  åˆ©ç”¨é‡å‚æ•°åŒ–æŠ€å·§ï¼Œå°†å˜é‡çš„éšæœºæ€§è½¬ç§»åˆ°æ–°æ„é€ çš„å‚æ•°ä¸­ï¼Œä½¿å¾—æ¢¯åº¦èƒ½å¤Ÿé¡ºåˆ©è¿”å›ã€‚åœ¨ä¸‹å›¾ä¸­åŸå§‹éšæœºå˜é‡ä¸º $z$ï¼Œåœ¨è¿›è¡Œåå‘ä¼ æ’­ç®—æ³•æ—¶ï¼Œç”±äºéšæœºæ€§æ¢¯åº¦æ— æ³•å›ä¼ ã€‚ä¸ºäº†è®©æ¢¯åº¦å›ä¼ ï¼Œå¼•å…¥æ–°çš„éšæœºå˜é‡ $\epsilon$ï¼Œç”±è¯¥å˜é‡æ‰¿æ‹…éšæœºå™ªå£°çš„ä½œç”¨

  <img src="Denoising Diffusion Probabilistic Models/reparameterization-trick.png" alt="img" style="zoom:80%;" />

- How does variational inference connected with ELBO?

  [Evidence lower bound - Wikipedia](https://en.wikipedia.org/wiki/Evidence_lower_bound#Variational_Bayesian_inference)

  These words are extremely important to anwer the question: what does these parameter is trying to model? and how to compute these values actually

  > This defines a family of joint distributions pÎ¸ over (X,Z). It is very easy to sample (x,z)âˆ¼pÎ¸: simply sample zâˆ¼p, then compute fÎ¸(z), and finally sample xâˆ¼pÎ¸(â‹…|z) using fÎ¸(z).

  In general, it's impossible to perform the integral pÎ¸(x)=âˆ«pÎ¸(x|z)p(z)dz, forcing us to perform another approximation.
  
- Importance Sampling

  è¯¥æ–¹æ³•ç”¨äº target distribution $p*$ ä¸å¤ªå¥½é‡‡æ ·æ—¶ä½¿ç”¨ proposal distribution $q$ æ¥è·å¾—é‡‡æ ·ç»“æœã€‚æ­¤æ—¶è®¡ç®—æœŸæœ›æ—¶éœ€è¦ä¹˜ä»¥ ratio æ¥è·å¾—æ­£ç¡®çš„æœŸæœ›
  $$
  \mathbb{E}_{x \sim p^*}[f(x)] \approx \frac{1}{N} \sum_{i=1}^{N} \frac{p^*(x_i)}{q(x_i)} f(x_i), \quad \text{where } x_i \sim q(x).
  $$

- Inference & Generate

  

## Question

- ä¸ºä»€ä¹ˆåœ¨ inference é‡‡æ ·çš„æ—¶å€™è¿˜è¦åŠ å…¥éšæœºå™ªå£°ï¼Ÿ

  é‡‡æ ·ï¼Œjust like sampling when generating tokens

- Explaining the square root in the $\sqrt{\alpha_t}$ when doing linear gaussian modeling

  This is to maintain the variance structure of origianl distribution

- How to optimize the first term of VAE $E_{z\sim q_{\phi}(z|x)}[\log{p_{\theta}(x|z)}]$â€‹

  we use the network to produce the mean of of gaussian, what about variance?
  
- ä»€ä¹ˆæ˜¯ç”Ÿæˆæ¨¡å‹ï¼Œä»€ä¹ˆæ˜¯åˆ¤åˆ«æ¨¡å‹ï¼Œä»–ä»¬çš„æ¦‚å¿µæ˜¯ä»€ä¹ˆï¼Ÿ

- ä»€ä¹ˆæ˜¯ biased & unbiased?

- ä¸ºä»€ä¹ˆæˆ‘ä»¬æ˜¯å»ä¼˜åŒ– ELBOï¼Œè€Œä¸æ˜¯ç›´æ¥ä¼˜åŒ–ä¼°è®¡å€¼ $\ln{p_\theta(x) = \frac{p_\theta(x|z) p(z)}{p_\theta(z|x)}}$

  