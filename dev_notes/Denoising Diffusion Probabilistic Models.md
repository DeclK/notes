# Denoising Diffusion Probabilistic Models

[DDPM arxiv](https://arxiv.org/pdf/2006.11239)

[Understanding Diffusion Models: A Unified Perspective arxiv](https://arxiv.org/pdf/2208.11970)

[bilibili-æ‰©æ•£æ¨¡å‹ - Diffusion Modelã€æå®æ¯…2023ã€‘](https://www.bilibili.com/video/BV14c411J7f2) [PPT1](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/DiffusionModel%20(v2).pdf) [PPT2](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/StableDiffusion%20(v2).pdf) [PPT3](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2023-course-data/DDPM%20(v7).pdf) [course page](https://speech.ee.ntu.edu.tw/~hylee/ml/2023-spring.php)

æœ¬æ–‡æ¡£æ ¹æ®æå®æ¯…è§†é¢‘æ•´ç†å¾—åˆ°ï¼Œå¸Œæœ›å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

1. How to train and inference a diffusion model
2. What are the basic mathmatics behind it
3. How SOTA models improve the original diffusion model
4. What are the insights of diffusion can give us

## Notaions in Probability

- $x\sim p(Â·)$ ä»£è¡¨ç€ $x$ æ˜¯ä¸€ä¸ªéšæœºå˜é‡æœä»äºæ¦‚ç‡åˆ†å¸ƒ $p$ï¼Œåœ¨è®ºæ–‡ä¸­ï¼Œä¹Ÿå¸¸å¸¸ç”¨è¯¥ notation æ¥è¡¨ç¤ºåœ¨ $p$ åˆ†å¸ƒä¸­é‡‡æ · $x$
- $E_{x\sim p(x)}$ ä»£è¡¨ç€æ±‚è§£å˜é‡ $x$ çš„æœŸæœ›ï¼Œå¹¶ä¸”è¯¥å˜é‡æœä»æ¦‚ç‡åˆ†å¸ƒ $p(x)$ï¼Œæœ‰æ—¶å€™ä¹Ÿä¼šç”¨ $E_{p(x)}$â€‹ æ¥ç®€å†™è¯¥è¿‡ç¨‹
- $N(x;\mu, \sigma^2)$ æ¥è¡¨ç¤ºä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œå…¶ä¸­éšæœºå˜é‡ä¸º $x$ï¼Œå…¶å‡å€¼å’Œæ–¹å·®åˆ†åˆ«ä¸º $\mu, \sigma^2$ï¼Œæœ‰æ—¶å€™ä¹Ÿä¼šçœç•¥æ‰ $x$ï¼Œç›´æ¥å†™ä½œ $N(\mu, \sigma^2)$ã€‚å¯¹äºä¸€ä¸ªå¤šç»´é«˜æ–¯åˆ†å¸ƒä½¿ç”¨å¦‚ä¸‹ notation: $N(x;\mu,\Sigma)$ï¼Œå…¶ä¸­ $x,\mu$ éƒ½æ˜¯å¤šç»´å‘é‡ï¼Œè€Œ $\Sigma$ ä¸ºåæ–¹å·®çŸ©é˜µ (covariance matrix)

## An intuitive perspective of DDPM

<img src="Denoising Diffusion Probabilistic Models/image-20241206102950079.png" alt="image-20241206102950079" style="zoom:67%;" />

çœ‹å›¾è¯´è¯ï¼šDDPM å…¶å®å°±æ˜¯ä¸€ä¸ªä»éšæœºå™ªå£°å¼€å§‹ï¼Œä¸æ–­å»å™ªç›´åˆ°ç”Ÿæˆç›®æ ‡å¯¹è±¡çš„è¿‡ç¨‹ã€‚é‚£åˆ°åº•ä¼šç”Ÿæˆä»€ä¹ˆæ ·çš„å›¾åƒå‘¢ï¼Ÿæ ¹æ®ä¸Šé¢çš„ç¤ºæ„å›¾ï¼Œæˆ‘ä»¬å…¶å®å¹¶æ²¡æœ‰ç»™å»å™ªæ¨¡å‹ä»»ä½•çš„ä¿¡æ¯ï¼Œåªç»™äº†ä¸€ä¸ªåˆå§‹å™ªå£°å°±å¼€å§‹è®©å…¶è¿›è¡Œå»å™ªï¼Œæ‰€ä»¥ç†è®ºä¸Šä¼šç”Ÿæˆä»»æ„çš„å›¾åƒã€‚è€Œç°åœ¨ diffusion model åœ¨ text-to-image é¢†åŸŸåº”ç”¨éå¸¸å¤šï¼Œæ‰€ä»¥å¦‚æœæƒ³è¦ç”ŸæˆæŒ‡å®šçš„å›¾åƒï¼Œæˆ‘ä»¬éœ€è¦ç»™ diffusion model åŠ å…¥é¢å¤–çš„ä¿¡æ¯ï¼Œä¾‹å¦‚æ–‡å­—ï¼š

<img src="Denoising Diffusion Probabilistic Models/image-20241206161019438.png" alt="image-20241206161019438" style="zoom: 67%;" />

## How to inference DDPM

- Algorithm

  <img src="Denoising Diffusion Probabilistic Models/image-20241205155625447.png" alt="image-20241205155625447" style="zoom:80%;" />

  Notation of the algorithm

  - $\alpha_t, \bar{\alpha_t}$â€‹

    äºŒè€…éƒ½æ˜¯è¶…å‚æ•°ï¼Œç”±æˆ‘ä»¬è‡ªå·±å†³å®šã€‚å®é™…ä¸Šè¿™äº§ç”Ÿäºå¦ä¸€ä¸ªåºåˆ— $\beta_1,\beta_2,...\beta_t$ï¼Œè¿™ä¸ªåºåˆ—ä»£è¡¨äº†æˆ‘ä»¬æ¯ä¸€ä¸ª step ä¸­å™ªå£°æ‰€å çš„æ¯”ä¾‹ï¼Œæ˜¯ä¸€ä¸ªé€’å¢çš„åºåˆ—ï¼Œå¹¶ä¸”å±äº 0~1 ä¹‹é—´ã€‚ç®€å•æ¥è¯´ï¼Œéšç€ forward step çš„åŠ æ·±ï¼Œæˆ‘ä»¬å‘åŸå›¾ä¸­åŠ çš„å™ªå£°ä¼šè¶Šæ¥è¶Šå¤§ï¼Œå³å™ªå£°æ‰€å çš„æ¯”ä¾‹ä¼šè¶Šæ¥è¶Šå¤§
    $$
    \alpha_t = 1-\beta_t\\
    \bar{\alpha_t}=\alpha_1\alpha_2...\alpha_t
    $$

  - $\epsilon_\theta$â€‹

    å…¶å®å°±æ˜¯ neural networkï¼Œå…¶å‚æ•°ç”¨ $\theta$ è¡¨ç¤ºï¼Œè¾“å…¥ä¸º $(x_t,t)$ï¼Œè¯¥ç½‘ç»œçš„ä½œç”¨å°±æ˜¯æ ¹æ®è¾“å…¥å›¾åƒå’Œ time step tï¼Œé¢„æµ‹å‡ºåŠ å…¥åˆ°è¯¥å›¾åƒçš„å™ªå£° $\epsilon$ï¼Œç„¶åç”¨è¾“å…¥å›¾åƒå‡å»è¯¥å™ªå£°å°±èƒ½å¤Ÿè·å¾—å»å™ªå›¾åƒ

  - $\sigma_t$

    ä»ç„¶ä¹Ÿæ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œä»£è¡¨æ¯ä¸€ä¸ª timestep t é¢å¤–åŠ å…¥çš„ variance

- Algorithm ç¤ºæ„å›¾ (at time step 999)

  <img src="Denoising Diffusion Probabilistic Models/image-20241206102258158.png" alt="image-20241206102258158" style="zoom:80%;" />

## How to train DDPM

- Algorithm

  <img src="Denoising Diffusion Probabilistic Models/image-20241205155602678.png" alt="image-20241205155602678" style="zoom: 80%;" />

  DDPM çš„è®­ç»ƒç®—æ³•ä¹Ÿéå¸¸çš„ç®€æ´ï¼Œç®€å•æ¥çœ‹å°±æ˜¯ç”¨ç½‘ç»œ $\epsilon_\theta$ å»é¢„æµ‹åŠ å…¥åˆ°æ ·æœ¬ä¸­çš„å™ªå£°ï¼Œå¸Œæœ›è¿™ä¸ªé¢„æµ‹å™ªå£°å’ŒçœŸå®åŠ å…¥çš„å™ªå£°è¶³å¤Ÿçš„æ¥è¿‘ã€‚ä½†å…¶å®è¿™ä¸ªè®­ç»ƒè¿‡ç¨‹æš—è—ç„ğŸ”ï¼Œé‡Œé¢è¿™äº›ç³»æ•°åˆ°åº•æ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿä¸ºä»€ä¹ˆåœ¨æ¨ç†çš„æ—¶å€™ä¼¼ä¹æ˜¯ä¸€æ­¥ä¸€æ­¥åœ°å»å™ªï¼Œè€Œè¿™ä¸ªè¿‡ç¨‹åœ¨è®­ç»ƒé‡Œæ²¡æœ‰å‘¢ï¼Ÿè¿™äº›éƒ½éœ€è¦åœ¨ä¹‹åçš„æ•°å­¦æ¨å¯¼ä¸­å›ç­”ï¼

## SOTA diffusion framework

- Overall Framework

  ç°ä»Š SOTA çš„ text-to-image æ¨¡å‹éƒ½å¯ä»¥åˆ†ä¸º3å¤§æ­¥éª¤ï¼š

  1. ä½¿ç”¨ text encoder å°†æ–‡å­—è¿›è¡Œ encodeï¼Œç”Ÿæˆ text feature
  2. ä½¿ç”¨ generation model å¯¹å™ªå£°è¿›è¡Œå»å™ªã€‚æ­¤æ—¶éœ€è¦å°† text feature ä½œä¸ºè¾“å…¥ï¼Œç»è¿‡æ¨¡å‹åè¾“å‡ºä¸€ä¸ªä¸­é—´äº§ç‰©ï¼Œè¯¥äº§ç‰©å¯ä»¥æ˜¯ feature mapï¼Œæˆ–è€…å‹ç¼©çš„å›¾ç‰‡
  3. å¯¹ä¸­é—´äº§ç‰©è¿›è¡Œ decodeï¼Œç”Ÿæˆæœ€ç»ˆçš„å›¾åƒ

  <img src="Denoising Diffusion Probabilistic Models/image-20241206163249880.png" alt="image-20241206163249880" style="zoom:67%;" />

  è¿™ä¸‰ä¸ªç»„ä»¶ï¼štext encoder, generation model, decoder é€šå¸¸éƒ½æ˜¯åˆ†å¼€è®­ç»ƒçš„ã€‚ä½†æ˜¯ generation model æ˜¯ä¾èµ–äº decoder çš„ï¼Œå› ä¸º decoder å¿…é¡»è¦è®¤è¯† generation model æ‰€äº§ç”Ÿçš„ç‰¹å¾å›¾ã€‚è¿™å®é™…ä¸Šåœ¨è®­ç»ƒ decoder æ—¶ï¼Œæˆ‘ä»¬è¿˜è®­ç»ƒäº†ä¸€ä¸ª auto encoder æ¥äº§ç”Ÿè¿™ä¸ª latent representations (feature map)ï¼Œç„¶åé€šè¿‡å‘ latent representation åŠ å…¥ noise è®­ç»ƒ generation model

- Text encoder size å¯¹äºå›¾åƒç”Ÿæˆè´¨é‡éå¸¸é‡è¦ï¼Œè€Œ vision encoder size å½±å“è¾ƒå°

  <img src="Denoising Diffusion Probabilistic Models/image-20241206161343570.png" alt="image-20241206161343570" style="zoom: 50%;" />

  FID Frechet Inception Distance å°±æ˜¯ç”¨æ¥è¯„ä»·æ‰€ç”Ÿæˆçš„å›¾åƒé›†ä¸ç›®æ ‡å›¾åƒé›†ä¹‹é—´çš„è·ç¦»ï¼ŒFID åº”è¯¥è¶Šå°è¶Šå¥½ã€‚å¯ä»¥çœ‹åˆ°éšç€ T5 æ¨¡å‹çš„å¢åŠ ï¼ŒFID æ›²çº¿æ˜¯å‘ç€å³ä¸‹è§’ç§»åŠ¨ï¼Œè¯´æ˜ç”Ÿæˆå›¾åƒæœ‰æ˜¾è‘—æ”¹å–„

  <img src="Denoising Diffusion Probabilistic Models/image-20241206161802419.png" alt="image-20241206161802419" style="zoom: 50%;" />

- è¯¾ç¨‹è¿˜ä»‹ç»äº† Stable Diffusion & DALL-Eï¼Œæˆ‘å°±ä¸æ•´ç†äº†

## Maths

- VAE (Variational Auto Encoder) & Diffusion Model

- What does posterior mean?

  This is from Bayesian stuff, also we need to know what kind of hypothesis does Bayesian gives (normally gaussian, but can change into what?)

- What does marginalize mean?

- How to understand latent variable?

  From the Understanding Diffusion Models: A Unified Perspective gives an intuitive philosophy

- Chain of rules in probability

  https://en.wikipedia.org/wiki/Chain_rule_(probability)

- Markov Chain Monte Carlo (MCMC)

- Reparameterization trick

  https://en.wikipedia.org/wiki/Reparameterization_trick

- [Lil's log on diffusion](https://lilianweng.github.io/posts/2021-07-11-diffusion-models)

- How does variational inference connected with ELBO?

  [Evidence lower bound - Wikipedia](https://en.wikipedia.org/wiki/Evidence_lower_bound#Variational_Bayesian_inference)

  These words are extremely important to anwer the question: what does these parameter is trying to model? and how to compute these values actually

  > This defines a family of joint distributions pÎ¸ over (X,Z). It is very easy to sample (x,z)âˆ¼pÎ¸: simply sample zâˆ¼p, then compute fÎ¸(z), and finally sample xâˆ¼pÎ¸(â‹…|z) using fÎ¸(z).

  In general, it's impossible to perform the integral pÎ¸(x)=âˆ«pÎ¸(x|z)p(z)dz, forcing us to perform another approximation.

## Question

- ä¸ºä»€ä¹ˆåœ¨ inference é‡‡æ ·çš„æ—¶å€™è¿˜è¦åŠ å…¥éšæœºå™ªå£°ï¼Ÿ

  é‡‡æ ·ï¼Œjust like sampling when generating tokens

- In the [material](https://arxiv.org/pdf/2208.11970), there are some p is $p_\theta(Â·)$, but some wihout $\theta$, juse $p(Â·)$, how to differenciate them?

  It seems that the $p(Â·)$ without the $\theta$â€‹ means it is a prior distribution, which means we defined it manually at the very beginning, or let's say it is out hypothesis

- Explaining the square root in the $\sqrt{\alpha_t}$ when doing linear gaussian modeling

  This is to maintain the variance structure of origianl distribution

- How to optimize the first term of VAE $E_{z\sim q_{\phi}(z|x)}[\log{p_{\theta}(x|z)}]$â€‹

  we use the network to produce the mean of of gaussian, what about variance?
  
- Why it is hard to compute the $p_\theta(x)$

  [my-chat](https://chatgpt.com/share/675726ae-8364-800a-b33d-0ed508bc3eaf)
