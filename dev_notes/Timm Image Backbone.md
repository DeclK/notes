# Timm Image Backbone

## Common usage

### create model

ä½¿ç”¨ timm åˆ›å»ºæ¨¡å‹éå¸¸ç®€å•

```python
import timm
model = timm.create_model('resnet50', pretrained=True)
```

ä¸ºäº†æ›´é¡ºåˆ©ç†è§£ timm çš„æ¶æ„ï¼Œæˆ‘éœ€è¦äº†è§£ timm æ˜¯å¦‚ä½•ç®¡ç†æ¨¡å‹ï¼Œå¹¶é€šè¿‡å­—ç¬¦ä¸²æ˜ å°„è·å¾—æ¨¡å‹çš„

åœ¨è®¸å¤šæ·±åº¦æ¨¡å‹æ¡†æ¶ä¸­éƒ½æœ‰è¿™æ ·çš„æœºåˆ¶ï¼šå°†å­—ç¬¦ä¸²æ˜ å°„æˆä¸ºæ¨¡å‹ï¼Œä»–ä»¬éƒ½ä½¿ç”¨äº†â€œæ³¨å†Œâ€æœºåˆ¶ï¼Œåœ¨ timm ä¸­ä¹Ÿæ˜¯å¦‚æ­¤ã€‚timm å°†æ‰€æœ‰æ³¨å†Œçš„æ¨¡å‹éƒ½å†™åœ¨äº†æ¨¡å‹æ–‡ä»¶çš„åº•éƒ¨ï¼Œä¾‹å¦‚ä¸‹é¢çš„æ˜¯ `timm.models.resnet.py` ä¸­æ³¨å†Œçš„ resnet50 ç»“æ„

```python
@register_model
def resnet50(pretrained: bool = False, **kwargs) -> ResNet:
    """Constructs a ResNet-50 model.
    """
    model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3])
    return _create_resnet('resnet50', pretrained, **dict(model_args, **kwargs))
```

é€šè¿‡ `@register_model`ï¼Œtimm å°†æ‰€æœ‰æ¨¡å‹ã€æ¨¡å‹é…ç½®éƒ½ç»Ÿä¸€å­˜æ”¾åœ¨äº† `timm.models._registry.py` ä¸­çš„å­—å…¸

```python
_model_entrypoints: Dict[str, Callable[..., Any]] = {}  # mapping of model names to architecture entrypoint fns
_model_default_cfgs: Dict[str, PretrainedCfg] = {}  # central repo for model arch -> default cfg objects
_model_pretrained_cfgs: Dict[str, PretrainedCfg] = {}  # central repo for model arch.tag -> pretrained cfgs
```

### input preprocess

å¯¹äºåŸå§‹å›¾ç‰‡éœ€è¦è¿›è¡Œé¢„å¤„ç†ï¼Œæ‰èƒ½è¾“å…¥åˆ°æ¨¡å‹ä¸­

```python
# get model specific transforms (normalization, resize)
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)
```

### inference

æœ‰äº†æ¨¡å‹ï¼Œæœ‰äº†è¾“å…¥å›¾åƒï¼Œå¯ç›´æ¥è¿›è¡Œæ¨ç†

```python
from PIL import Image
import numpy as np
import timm

img = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))
data_config = timm.data.resolve_model_data_config(model)
transforms = timm.data.create_transform(**data_config, is_training=False)

model.eval()
output = model(transforms(img).unsqueeze(0))  # output is (batch_size, num_features) shaped tensor
```

### Extract features

å°† `create_model` ä¸­çš„å‚æ•° `features_only=True` å³å¯è·å¾—ä¸­é—´ feature map

```python
import torch
import timm
m = timm.create_model('resnest26d', features_only=True, pretrained=True, out_indices=(2,3))
o = m(torch.randn(2, 3, 224, 224))
for x in o:
    print(x.shape)
```

æ€»ç»“ä¸‹ `timm.create_model` çš„å®Œæ•´é€»è¾‘ï¼š

1. è¾“å…¥æœ‰ä¸‰ä¸ªé‡è¦å…ƒç´ ï¼š

   1. `model_name`ï¼Œå…¶ç”±ä¸¤éƒ¨åˆ†ç»„æˆ `model.pretrained_tag`ï¼Œå‰åŠéƒ¨åˆ†ä»£è¡¨æ¨¡å‹åï¼ŒååŠéƒ¨åˆ†ä»£è¡¨é¢„è®­ç»ƒ tag
   2. `pretrained`ï¼Œæ˜¯å¦æ‹‰å–é¢„è®­ç»ƒæƒé‡
   3. `features_only`ï¼Œæ˜¯å¦ä»…ç”¨äºæŠ½å–ç‰¹å¾

2. é€šè¿‡ `model_name` åˆ›å»ºæ¨¡å‹

   æ¨¡å‹çš„åˆ›å»ºæ–¹æ³•å…¨éƒ¨éƒ½å†™åœ¨äº†æ¨¡å‹çš„æ–‡ä»¶é‡Œï¼Œå¦‚ä¸Šé¢çš„ `def resnet50(...)`ï¼Œå°±æ˜¯å…¸å‹çš„æ¨¡å‹åˆ›å»ºæ–¹æ³•ã€‚é€šå¸¸æ¥è¯´ timm è¿˜å–œæ¬¢ç”¨ä¸€å±‚ `_create_model` çš„æŠ½è±¡ï¼Œç”¨äºçœŸå®çš„åˆ›å»ºæ¨¡å‹ï¼Œ`@register_model` æ‰€è£…é¥°å‡½æ•°é€šå¸¸ç”¨äºåˆ›å»º configï¼Œå¯çœ‹åšé…ç½®æ–‡ä»¶ã€‚è€Œ `_create_model` éœ€è¦å¹²ä¸¤ä»¶äº‹ï¼šåŠ è½½é¢„è®­ç»ƒå‡½æ•°ä»¥åŠä½¿ç”¨é…ç½®åˆ›å»ºæ–‡ä»¶

   ```python
   def _create_convnext(variant, pretrained=False, **kwargs):
       # variant is model name, you can find config with variant
       model = build_model_with_cfg(
           ConvNeXt, variant, pretrained,
           feature_cfg=dict(out_indices=(0, 1, 2, 3), flatten_sequential=True),
           **kwargs)
       return model
   ```

   è¿™é‡Œå†™ä¸º `kwargs` çš„è¾“å…¥å¯¹äºé˜…è¯»ä»£ç æ¥è¯´å¾ˆä¸å‹å¥½ï¼Œéœ€è¦è‡ªå·±æŸ¥çœ‹åˆ°åº•ä¼šä¼ å…¥ä»€ä¹ˆä¸œè¥¿ã€‚æˆ‘è®¤ä¸º `kwargs` ä¸»è¦åŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼š

   1. `pratrained_tag`
   2. `features_only`
   3. `out_indices`ï¼Œä¼šè¦†ç›–æ‰é»˜è®¤çš„ `out_indices`

3. å¦‚æœ `features_only` åˆ™å¯èƒ½ä¼šä½¿ç”¨ `FeatureListNet` å»å°è£…ä¸€ä¸‹åŸæ¨¡å‹

   `FeatureListNet` å°±æ˜¯æ ¹æ® `out_indices` è¿”å›è¾“å‡ºï¼Œå¹¶ä¸”ä¼šå‰”é™¤æ‰ `head` ç›¸å…³çš„å±‚ï¼Œåªä¿ç•™å‰å‘è®¡ç®—å¿…é¡»çš„ç½‘ç»œå±‚

## ResNet

timm ä¸­å®ç°çš„ ResNet ç±»å…¶å®å®ç°äº†æ•´ä¸ª ResNet å®¶æ—ï¼ŒåŒ…å«äº† ResNet / ResNeXt / SE-ResNeXt / SE-Net

æ€è·¯ï¼šå…ˆçœ‹ç»å…¸çš„ ResNet34 & ResNet50 & ResNet101ï¼Œç„¶ååœ¨æ­¤åŸºç¡€ä¸Šåšå˜åŒ–ï¼Œææ¸…æ¥šæ¯ä¸€ä¸ªé…ç½®æ‰€å¯¹åº”çš„ï¼ˆå¢åŠ çš„ï¼‰ trick/module

ResNet34 & ResNet50 çš„æ¨¡å‹é…ç½®éå¸¸ç®€å•ï¼ŒäºŒè€…å”¯ä¸€åŒºåˆ«åœ¨äº block çš„é€‰æ‹©ï¼Œåœ¨ä¸‹é¢çš„ table 1 èƒ½çœ‹åˆ°ä¸€äº›ç»´åº¦ç»†èŠ‚

```python
model_args = dict(block=BasicBlock, layers=[3, 4, 6, 3])
model_args = dict(block=Bottleneck, layers=[3, 4, 6, 3])
```

### ImageNet backbone é€šç”¨æ¡†æ¶

ResNet åˆ†ä¸ºå››ä¸ªéƒ¨åˆ†ï¼Œè¿™ä¹Ÿæ˜¯ç›®å‰ vision backbone çš„ä¸»æµæ¡†æ¶

1. Stem

   æ¨¡å‹çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œé€šå¸¸é€‰å–å¤§ kernel æ¥è¿›è¡Œä¸‹é‡‡æ ·ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å¤šä¸ªå° kernel çš„å †å æ¥æ›¿æ¢å¤§ kernel (deeper)

   timm ä¸­å…³è”å‚æ•° `stem_type='deep', stem_width=32`ï¼Œå¹¶ä¸”é€šå¸¸ä»¥åç¼€ `d` æ¥æ ‡è¯†ï¼Œä¾‹å¦‚ `resnet50d`

2. Stem pooling

   é€šå¸¸é‡‡ç”¨ä¸€ä¸ª max pooling è¿›è¡Œä¸‹é‡‡æ ·

3. Feature blocks

   æ¨¡å‹çš„ä¸»ä½“éƒ¨åˆ†ï¼Œé€šå¸¸åˆ†ä¸ºå››ä¸ª stages

   timm ä¸­ä½¿ç”¨å‚æ•° `block=Bottleneck` ä»¥æŒ‡å®š block ç±»å‹

4. Head

   åˆ†ç±»å¤´ï¼Œé€šå¸¸ä¸ºä¸€ä¸ª avrage pool + fc (linear)

### ç†è§£ ResNet building block

ç†è§£ feature blocks æœ‰ä¸‰ä¸ªæ ¸å¿ƒæ¦‚å¿µï¼š

1. Bottleneck

   å¯¹äº resnet50 åŠæ›´å¤§çš„ resnet æ¥è¯´ï¼Œéƒ½ä¼šä½¿ç”¨ Bottleneck çš„ç½‘ç»œå—ä½œä¸ºåŸºç¡€å—ã€‚æ‰€è°“çš„ Bottleneckï¼Œå°±æ˜¯ä¸¤å¤´å¤§ï¼Œä¸­é—´å°ã€‚ä¸‹é¢è¿™ä¸ª table 1 éå¸¸ç»å…¸ï¼Œæ˜¯ resnet å®¶æ—çš„ç»“æ„è¡¨

   <img src="Timm Image Backbone/image-20231226171433236.png" alt="image-20231226171433236" style="zoom: 67%;" />

   è¦çœ‹åˆ°â€œç“¶é¢ˆâ€ï¼Œéœ€è¦å±•å¼€çœ‹ building block çš„ç¬¬ä¸€å±‚ã€‚ä¾‹å¦‚ stage 1 ä¸­æ¯ä¸€ä¸ª building block ç¬¬ä¸€å±‚æ˜¯ä¸€ä¸ª (1x1,64) çš„å·ç§¯ï¼Œè¿™é‡Œçš„ 64 ä»£è¡¨çš„æ˜¯è¾“å‡ºé€šé“æ•°ï¼Œè€Œè¾“å…¥é€šé“æ•°ä¸ºä¸Šä¸€å±‚çš„è¾“å‡ºï¼Œå³ï¼š256ï¼Œæ‰€ä»¥å½¢æˆäº† `256->64->256` çš„â€œç“¶é¢ˆâ€ï¼Œå…¶ä¸­ `expansion=256/64=4`

   é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ª**éšè—çš„â€œç“¶é¢ˆâ€**åœ¨äºä¸åŒ stage ä¹‹é—´çš„è¿æ¥ï¼Œä¾‹å¦‚ stage1 -> stage2 ä¹‹é—´æœ‰ä¸€ä¸ª `256->128->512` çš„ç“¶é¢ˆ

   å…¶å® Bottleneck è¿˜æ˜¯ Inverted Bottleneck éƒ½æ˜¯ä¸€æ ·çš„ï¼Œå› ä¸ºä½ æŠŠå¤šä¸ªç“¶é¢ˆè¿æ¥èµ·æ¥ï¼Œå°±ä¼šå‡ºç°åç“¶é¢ˆğŸ¤”ï¼Œä¾‹å¦‚ `256->64->256->64->256` ä¸­é—´å°±æœ‰åç“¶é¢ˆ `64->256->64`

2. Residual conncections

   ä¸ºäº†å®Œæˆæ®‹å·®é“¾æ¥ï¼Œæˆ‘ä»¬å”¯ä¸€å¯èƒ½éœ€è¦åšçš„æ˜¯å¯¹è¾“å…¥è¿›è¡Œé™é‡‡æ ·ï¼Œå¯ä»¥ä½¿ç”¨ avarage pool + conv1x1 + norm çš„æ–¹å¼å®Œæˆä¸‹é‡‡æ ·å’Œé€šé“æ•°çš„è°ƒæ•´ï¼Œè¿™æ ·çš„ä¸‹é‡‡æ ·æ–¹å¼ç”¨ `d` æ ‡è¯†ï¼Œå¦‚ `resnet50d`ã€‚åŸå§‹çš„ resnet ä¸‹é‡‡æ ·æ–¹å¼é€‰æ‹© conv2d + norm å®Œæˆ

3. Forward order

   å‰å‘è·¯å¾„çš„ä»£ç éå¸¸æ¸…æ™°ï¼Œçœ‹å®Œå°±çŸ¥é“ç½‘ç»œç»“æ„

   ```python
       def forward(self, x: torch.Tensor) -> torch.Tensor:
           shortcut = x
   
           x = self.conv1(x)
           x = self.bn1(x)
           x = self.act1(x)
   
           x = self.conv2(x)
           x = self.bn2(x)
           x = self.drop_block(x)	# not original
           x = self.act2(x)
           x = self.aa(x)			# not original
   
           x = self.conv3(x)
           x = self.bn3(x)
   
           if self.se is not None:
               x = self.se(x)
   
           if self.drop_path is not None:
               x = self.drop_path(x)
   
           if self.downsample is not None:
               shortcut = self.downsample(shortcut)
           x += shortcut
           x = self.act3(x)
   ```

   è§£é‡Šï¼š

   - conv1 conv2 con3 åˆ†åˆ«å¯¹åº”ä¸Šè¡¨ä¸­çš„å·ç§¯ã€‚å¯¹äº resnet34ï¼Œè¡¨ä¸­åªæœ‰ä¸¤å±‚å·ç§¯ï¼Œåˆ†åˆ«å¯¹åº”ä»£ç ä¸­çš„ conv2 conv3ï¼Œåœ¨ä»£ç ä¸­ä¸º BasicBlock class
   - æ¿€æ´»å‡½æ•°å‡ä¸º ReLU
   - se ä¸ºé€šé“æ³¨æ„åŠ›ï¼Œsqueeze and excitation
   - aa ä¸º anti-aliasing / blur pool
   - drop_block & drop_path æ²¡æœ‰å‡ºç°åœ¨æ•´ä¸ªæ®‹å·®ç½‘ç»œå®¶æ—ä¸­
   - ResNeXT å°±æ˜¯è°ƒæ•´ä¸‹ conv2 ä¸­çš„ groups å‚æ•°ä»¥åŠå¯¹åº”çš„ neck channel å³å¯ã€‚è€Œ DW å·ç§¯å°±æ˜¯å°† groups è°ƒæ•´åˆ°å’Œ channel æ•°é‡ä¸€è‡´ï¼Œè¿™é‡Œä¹Ÿä¸éš¾çœ‹åˆ° NAS çš„å½±å­ï¼šé€šè¿‡è°ƒæ•´ channel å’Œ groups è·å¾—æœ€ä¼˜çš„æ¨¡å‹ç»“æ„

## MobileNet

timm MobileNet åªå®ç°äº† V3 ç‰ˆæœ¬ï¼Œå¹¶ä¸”æ˜¯ä½¿ç”¨ EfficientNet çš„åŸºç¡€æ¨¡å—ï¼Œåº”è¯¥æ˜¯å‚è€ƒäº† NAS çš„æ¡†æ¶ï¼Œé€šè¿‡å­—ç¬¦ä¸²æ¥åˆ›å»ºæ¨¡å‹

ä¸ºäº†æ›´æ¸…æ¥šåœ°ç†è§£ MobileNet çš„ç»“æ„ï¼Œæˆ‘é€‰æ‹©ç›´æ¥çœ‹ torch ä¸­çš„å®ç°

```python
from torchvision.models.mobilenetv3 import mobilenet_v3_large

model = mobilenet_v3_large(pretrained=False)
```

ç†è§£ MobileNet ç»“æ„åªéœ€è¦ä¸‰ä¸ªéƒ¨åˆ†

1. First layer

   è¿™ä¸€éƒ¨åˆ†ç±»ä¼¼äº resnet çš„ stem éƒ¨åˆ†ï¼Œç”¨äºåˆå§‹å›¾åƒçš„ä¸‹é‡‡æ ·ã€‚è¿™ä¸€å±‚éå¸¸ç®€å•ï¼Œä»…ç”±ä¸€å±‚ ConvBNAct æ„æˆ

2. Inverted residual blocks

   å€’æ®‹å·®ç½‘ç»œä¸­çš„â€œå€’â€æ˜¯æŒ‡çš„ inverted bottleneckï¼Œè¿™æ˜¯ä¸€ä¸ªä¸¤å¤´å°ï¼Œä¸­é—´å¤§çš„ç»“æ„ã€‚æˆ‘å°†å…¶å‰å‘ç½‘ç»œå†™ä¸ºå¦‚ä¸‹

   ```python
   def forward(self, x):
       shortcut = x
       if self.input_channels != self.expaned_channels:
           # stride = 1, project expand channels
           x = self.ConvBnAct1(x)
           
       x = self.DWBnAct(x)	# downsample if stride = 2
       
       if self.use_se:
           x = self.se(x)
   
       # stride = 1, project out channels
       x = self.ConvBn2(x)
       
       # only used when stride = 1 and in_c = out_c
       if self.use_res:
           x += shortcut
   ```

3. Head

   MobileNet æœ€åçš„å¤´è¦é‡ä¸€äº›ï¼Œé¦–å…ˆå…ˆå°†è¾“å‡ºåšä¸€ä¸ªå‡ç»´ï¼Œç„¶åå†è¿›è¡Œ average poolï¼Œæœ€åä½¿ç”¨ä¸€ä¸ª mlp æ¥é¢„æµ‹

   ```python
   def classifier(self, x):
       # 6 times expansion
       x = self.ConvBnAct(x)
       
       # average pool
       x = self.avgpool(x)
       
       # mlp: linear + hardswish + dropout + linear
       x = self.mlp(x)
   ```

ä¸ºäº†ç²¾ç¡®æè¿°ç½‘ç»œç»“æ„ï¼Œæœ€é‡è¦çš„å°±æ˜¯é…ç½®ä¸­é—´çš„ inverted residual blockï¼ŒMobileNetV3 Large é…ç½®å¦‚ä¸‹ï¼š

```python
inverted_residual_setting = [
    # input_channels, kernel, expanded_channels, out_channels, use_se, activation, stride, dilation
    # reduce_divider = 1, dilation = 1
    bneck_conf(16, 3, 16, 16, False, "RE", 1, 1),
    bneck_conf(16, 3, 64, 24, False, "RE", 2, 1),  # C1
    bneck_conf(24, 3, 72, 24, False, "RE", 1, 1),
    bneck_conf(24, 5, 72, 40, True, "RE", 2, 1),  # C2
    bneck_conf(40, 5, 120, 40, True, "RE", 1, 1),
    bneck_conf(40, 5, 120, 40, True, "RE", 1, 1),
    bneck_conf(40, 3, 240, 80, False, "HS", 2, 1),  # C3
    bneck_conf(80, 3, 200, 80, False, "HS", 1, 1),
    bneck_conf(80, 3, 184, 80, False, "HS", 1, 1),
    bneck_conf(80, 3, 184, 80, False, "HS", 1, 1),
    bneck_conf(80, 3, 480, 112, True, "HS", 1, 1),
    bneck_conf(112, 3, 672, 112, True, "HS", 1, 1),
    bneck_conf(112, 5, 672, 160 // reduce_divider, True, "HS", 2, dilation),  # C4
    bneck_conf(160 // reduce_divider, 5, 960 // reduce_divider, 160 // reduce_divider, True, "HS", 1, dilation),
    bneck_conf(160 // reduce_divider, 5, 960 // reduce_divider, 160 // reduce_divider, True, "HS", 1, dilation),
]
# mlp hidden channel
last_channel = adjust_channels(1280 // reduce_divider)  # C5
```

## RegNet

### RegNet ä¸­çš„æ¦‚å¿µ

1. Design space

   ç”±ä¸€ç¾¤ç›¸åŒæ¶æ„ä¸åŒå‚æ•°çš„æ¨¡å‹ç»„æˆçš„ç©ºé—´ï¼Œå³ä¸º design spaceã€‚é€šè¿‡ä¸€äº›ç»å…¸çš„ç»Ÿè®¡æ•°æ®ï¼Œæˆ‘ä»¬èƒ½å¯¹è¿™ä¸ªç©ºé—´çš„æ¨¡å‹è¿›è¡Œåˆ†æï¼Œä¾‹å¦‚ï¼šerror distribution

2. Error distribution

   ä¸ºäº†è¡¡é‡æŸä¸€ä¸ªç©ºé—´çš„å¥½åï¼Œè®ºæ–‡ä½¿ç”¨äº† EDF (empirical distribution function) æ¥æè¿°ç©ºé—´è´¨é‡

   å¯¹äº n ä¸ªæ¨¡å‹æ¥è¯´ï¼Œå…¶ error EDF è¡¨ç¤ºä¸º
   $$
   F(e)=\frac{1}{n}\sum^{n}_{i=1}1[e_i<e]
   $$
   å…¶ä¸­ $e_i$ ä¸ºç¬¬ i ä¸ªæ¨¡å‹çš„è¯¯å·®ï¼Œ$F(e)$ æè¿°äº†è¯¯å·®ä½äº e çš„æ¨¡å‹æ•°é‡ï¼Œå…¶ä¸€èˆ¬å›¾åƒå¦‚ä¸‹ï¼Œæ ‡ç­¾ä¸­çš„æ•°æ®ä¸º min error & mean error

   <img src="Timm Image Backbone/image-20240102210455760.png" alt="image-20240102210455760" style="zoom:50%;" />

   è®ºæ–‡ä¸­å°±æ˜¯å¯¹ä¸€ä¸ªæœç´¢ç©ºé—´é‡‡æ ·500ä¸ªæ¨¡å‹ï¼Œç„¶åè¿›è¡ŒçŸ­æœŸçš„è®­ç»ƒï¼ˆ10 epochs on ImageNetï¼‰ï¼Œæ„å»ºå‡ºè¿™ä¸ªæœç´¢ç©ºé—´çš„ EDFã€‚EDF è¶Šé å·¦è¶Šå¥½

3. AnyNet & AnyNetX design space

   AnyNet ç©ºé—´å³ä¸ºç»å…¸çš„ ResNet æ¶æ„ï¼Œç”±ä¸‰éƒ¨åˆ†ç»„æˆï¼šstem, body, headã€‚å…¶ä¸­ body æ˜¯æœ€æ ¸å¿ƒçš„éƒ¨åˆ†ï¼Œbody ç”±å¤šä¸ª stages ç»„æˆï¼Œæ¯ä¸€ä¸ª stages ç”±å¤šä¸ªç›¸åŒçš„ block ç»„æˆï¼Œå¹¶ä¸”æ¯ä¸ª stage éƒ½ä¼šè¿›è¡Œä¸‹é‡‡æ ·

   <img src="Timm Image Backbone/image-20240102211236806.png" alt="image-20240102211236806" style="zoom: 80%;" />

   AnyNetX åˆ™è§„å®šäº† block çš„å½¢å¼ï¼šå³ä¸º ResNet ä¸­çš„ Bottleneckï¼ˆå’Œæœ¬æ–‡ä¸­çš„ bottleneck æœ‰å¾®å°åŒºåˆ«ï¼‰ï¼ŒåŒæ—¶ä¹Ÿç§° bottleneck ä¸º X block

   AnyNetX ä¸­æ¯ä¸€ä¸ª stage çš„è‡ªç”±åº¦å¦‚ä¸‹ï¼š

   1. number of blocksï¼Œ $d_i\le16$
   2. block widthï¼Œ $w_i\le1024$ å¹¶ä¸”ä¸º8çš„å€æ•°
   3. bottleneck ratioï¼Œ$b_i \in \{1,2,4\}$
   4. group widthï¼Œ$g_i\in\{1,2,4,...,32\}$

   å¦‚æœæœ‰ 4 ä¸ª stage çš„è¯ï¼Œæ•´ä¸ªæ¨¡å‹çš„æœç´¢ç©ºé—´ä¸º $(16Â·128Â·3Â·6)^4â‰ˆ10^{18}$

### ç†è§£ RegNet

RegNet çš„ç›®æ ‡å°±åªæœ‰ä¸¤ä¸ªï¼š

1. ç®€åŒ– design space çš„æœç´¢ç©ºé—´
2. ç®€åŒ–è¿‡åçš„ç©ºé—´ï¼Œèƒ½å¤Ÿæœ‰æ›´å¥½çš„æ¨¡å‹è¡¨ç°

ä» AnyNetX (ä¹Ÿç§°ä¸º AnyNetX_A) å¼€å§‹ï¼Œä¸€æ­¥ä¸€æ­¥å®Œæˆè¿™ä¸¤ä¸ªç›®æ ‡ï¼š

1. AnyNetX_Bï¼Œé‡‡ç”¨äº† shared bottleneck ratioï¼Œå³æ‰€æœ‰ stage é‡‡æ ·ç›¸åŒçš„ bottleneck ratioã€‚è®ºæ–‡å‘ç°è¯¥çº¦æŸå¹¶ä¸æ”¹å˜ EDF å›¾åƒ
2. AnyNetX_Cï¼Œé‡‡ç”¨äº† shared group widthï¼ŒåŒæ ·ä¸æ”¹å˜ EDF
3. AnyNetX_Dï¼Œè§„å®šæ¯ä¸ª stage çš„å®½åº¦é€çº§é€’å¢ $w_{i+1}\ge w_i$ï¼Œè¿™å°†å¤§å¤§æ”¹è¿› EDF å›¾åƒåˆ†å¸ƒã€‚è¿™ç¬¦åˆç°æœ‰æµè¡Œæ¨¡å‹çš„é…ç½®
4. AnyNetX_Eï¼Œè§„å®šæ¯ä¸ª stage çš„æ·±åº¦é€çº§é€’å¢ $d_{i+1}\ge d_i$ï¼Œè¿™å°†æ”¹å–„ EDF å›¾åƒåˆ†å¸ƒã€‚**å®é™…ä¸Šè¿™å¹¶ä¸ç¬¦åˆç°æœ‰æµè¡Œæ¨¡å‹çš„é…ç½®**ï¼Œå› ä¸ºæœ€åä¸€ä¸ª stage çš„æ·±åº¦é€šå¸¸ä¼šå‡å°

AnyNetX_E çš„æœç´¢ç©ºé—´ç›¸æ¯”åŸå§‹çš„ AnyNet ä¸‹é™äº†7ä¸ªæ•°é‡çº§ã€‚è®ºæ–‡åˆ†æ AnyNetX_E ä¸­çš„å¥½æ¨¡å‹é•¿ä»€ä¹ˆæ ·å­ï¼Œå‘ç°äº†æ·±åº¦å’Œå®½åº¦çš„é‡åŒ–çº¿æ€§å…³ç³»

![image-20240102215419460](Timm Image Backbone/image-20240102215419460.png)

è®ºæ–‡ä¸€å¼€å§‹ä½¿ç”¨å¦‚ä¸‹çš„çº¿æ€§å…¬å¼æ¥é™åˆ¶å®½åº¦å’Œæ·±åº¦
$$
u_j=w_0+w_aÂ·j
$$
å…¶ä¸­ $u_j$ æ˜¯ç¬¬ j ä¸ª block çš„å®½åº¦ï¼ˆj æ˜¯æ‹‰é€šæ‰€æœ‰ stage æ¥æ•°çš„ï¼‰ï¼Œ$w_0,w_a$ æ˜¯è¶…å‚æ•°

ä¸ºäº†å°†è¿™ä¸ªçº¿æ€§å…¬å¼é‡åŒ–ï¼Œè®ºæ–‡ä½¿ç”¨ä¸€ä¸ª $w_m$ æ¥ä½œä¸ºä¸€ä¸ªå®½åº¦å› å­ï¼ˆwidth multiplierï¼‰
$$
u_j=w_0Â·w_m^{s_j}\\
$$
çœŸæ­£çš„é‡åŒ–å°±æ˜¯å¯¹ $s_j$ è¿›è¡Œä¸Šä¸‹å–æ•´ï¼Œè·å¾—çœŸæ­£çš„æ¯ä¸€ä¸ªæ·±åº¦çš„å®½åº¦
$$
w_j=w_0Â·w_m^{\left \lfloor s_j \right \rceil }
$$
$w_m$ æˆ‘ç†è§£ä¸ºä¸€ä¸ªâ€œé˜¶æ¢¯â€å› å­ï¼Œä¾‹å¦‚å½“ $w_m=2$ æ—¶ï¼Œæ¯ä¸€ä¸ªé˜¶æ®µçš„å®½åº¦å°±ä¼šç¿»å€ï¼Œè€Œ $\left \lfloor s_j \right \rceil$ å°±ä»£è¡¨äº†å“ªä¸€ä¸ª stageï¼Œå½“ $w_m$ å–å¾—æ¯”è¾ƒå¤§æ—¶ï¼Œé˜¶æ¢¯å°±å°‘ï¼Œå¹³å°å°±é•¿ï¼›åä¹‹é˜¶æ¢¯å°±å¤šï¼Œå¹³å°å°±çŸ­

é€šè¿‡è¿™ä¸ªçº¿æ€§é‡åŒ–æ¡ä»¶æ¥é™åˆ¶ç½‘ç»œåï¼Œæœç´¢ç©ºé—´ä¹Ÿä¼šå¤§é‡ç¼©å°ï¼ˆ~$10^8$ï¼‰ï¼Œè‡³æ­¤æˆ‘ä»¬ç§°è¿™ä¸ªæœç´¢ç©ºé—´ä¸º **RegNetX**

åŒæ—¶ RegNetX çš„å‚æ•°å˜ä¸º 6 ä¸ªï¼š$w_0,w_a,w_m,d,b,g$ï¼Œå®ƒä»¬çš„èŒƒå›´ä¸ºï¼š$d<64,w_0,w_a<256,1.5\le w_m \le3$ï¼Œ$b,g$ çš„èŒƒå›´å’Œä¹‹å‰ä¸€è‡´

æ¥ä¸‹æ¥è®ºæ–‡å°†ç»§ç»­æ¢ç´¢ RegNetX ç©ºé—´é‡Œæ›´å¥½çš„æ¨¡å‹ï¼Œå› ä¸ºæˆ‘ä»¬ä»ç„¶æœ‰ $10^8$ çš„æœç´¢æ•°é‡ã€‚è®ºæ–‡é‡‡æ · 100 ä¸ªæ¨¡å‹ï¼Œè®­ç»ƒæ›´ä¹…ä¸€äº›ï¼Œè·å¾—å¦‚ä¸‹ç»“è®ºï¼š

1. æ·±åº¦ä¸ç®¡å¯¹äºè®¡ç®—é‡å°çš„æ¨¡å‹å’Œè®¡ç®—é‡å¤§çš„æ¨¡å‹ï¼Œéƒ½å¤§çº¦åœ¨20ä¸ª blocks (~60 layers)
2. å¥½çš„æ¨¡å‹éƒ½ä½¿ç”¨äº† bottleneck ratio = 1
3. å¥½çš„æ¨¡å‹éƒ½ä½¿ç”¨äº† width multiplier $w_m$ â‰ˆ 2.5
4. $g, w_a, w_0$ éƒ½éšç€å¤æ‚åº¦å¢åŠ 

<img src="Timm Image Backbone/image-20240103093000572.png" alt="image-20240103093000572" style="zoom:67%;" />

æœ‰äº†å¦‚ä¸Šçš„è§‚å¯Ÿè¿‡åï¼Œç»§ç»­é™åˆ¶ RegNetX çš„æœç´¢ç©ºé—´ï¼š

1. å– bottleneck ratio = 1
2. ç½‘ç»œæ·±åº¦ $12 \le d \le 28$ï¼Œåšäº†è¯¥é™åï¼Œæ­é…äºçº¿æ€§é‡åŒ–çº¦æŸï¼Œæˆ‘ä»¬ä¹‹å‰çš„ $d_{i+1}\ge d_i$ å°±ä¸å­˜åœ¨äº†ï¼Œå› ä¸ºæœ€åä¸€ä¸ª stage çš„æ·±åº¦ä¼šè¢«é™åˆ¶ä½ã€‚**è¿™æ ·å°±ç¬¦åˆäº†ä¸»æµæ¨¡å‹é…ç½®**ï¼šæœ€åä¸€ä¸ª stage blocks æ•°é‡å°‘
3. å®½åº¦å› å­ $w_m \ge 2$ 

è‡³æ­¤ design space çš„è®¾è®¡å°±ç»“æŸï¼Œä¹‹åè®ºæ–‡å°±åœ¨è¿™ä¸ªç©ºé—´é‡Œé‡‡æ ·ï¼Œä»¥è·å¾—ä¸åŒè®¡ç®—é‡çš„æ¨¡å‹ã€‚**è¿™æ ·çš„è®¾è®¡ä¸å½“å‰ mobile networks ä¸­æ‰€ä½¿ç”¨çš„ inverted bottleneck æ˜¯å†²çªçš„**ï¼Œå› ä¸ºå…¶å– bottleneck ratio < 1 å¹¶ä¸”å§‹ç»ˆå– g = 1ï¼Œä½œè€…æµ‹è¯•äº†è¯¥ç©ºé—´çš„ EDFï¼Œæ˜¯å·®äº b = 1ï¼Œg â‰¥ 1 çš„ã€‚å¹¶ä¸”ä½œè€…è¿˜æµ‹è¯•äº†åˆ†è¾¨ç‡ï¼Œè®¤ä¸º r = 224x224 æ˜¯æœ€ä½³çš„ï¼Œ**æå‡åˆ†è¾¨ç‡å¹¶æ²¡æœ‰æå‡ EDF**

ä¸ªäººè®¤ä¸ºé€ æˆä¸Šè¿°åŸå› æœ‰ä¸¤ç‚¹ï¼š

1. RegNet ç¼©å°æœç´¢ç©ºé—´çš„æ ‡å‡†ä¸€ç›´éƒ½æ˜¯ EDFï¼Œè€Œ EDF åªæœ‰å¯¹è¯¯å·®çš„è¯„ä¼°ï¼Œæ²¡æœ‰å¯¹æ¨¡å‹å‚æ•°ã€é€Ÿåº¦è¿›è¡Œè¯„ä¼°ã€‚åœ¨ä¹‹åçš„å®éªŒä¹Ÿçœ‹åˆ° RegNet åœ¨è¾ƒå°å‚æ•°ä¸‹å¯èƒ½è¡¨ç°ç•¥é€Šäº EfficientNetã€‚ä½†æ˜¯å¦‚æœæ‰€æ¯”è¾ƒçš„æ–¹å¼ä¸ºç›¸åŒæ¿€æ´»å€¼ï¼ˆactivationsï¼‰çš„æ¡ä»¶ä¸‹ï¼ŒRegNet æ˜¯èƒœå‡ºçš„ï¼Œè¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆ RegNet çš„é€Ÿåº¦è¿œå¿«äº EfficientNet
2. åˆ†è¾¨ç‡çš„æå‡å¸¦æ¥å‡†ç¡®ç‡çš„æå‡æ˜¯ä¸äº‰çš„äº‹å®ï¼Œæˆ‘çŒœæµ‹æ˜¯è®­ç»ƒæ–¹å¼çš„é—®é¢˜ï¼Œå¯¼è‡´äº†æ¨¡å‹æ— æ³•åœ¨å¤šåˆ†è¾¨ç‡ä¸‹æ³›åŒ–

è®ºæ–‡è¿˜åœ¨ X block ä¸­åŠ å…¥äº† SE layerï¼Œç”±è¯¥ block ç»„æˆçš„æ¨¡å‹ç§°ä¸º RegNetYï¼Œæ•ˆæœæ›´å¥½äº†

### RegNet bottleneck

timm ä¸­ regnet bottleneck å’Œ ResNet ä¸­å‡ ä¹ä¸€æ ·ï¼Œæœ‰å¦‚ä¸‹åŒºåˆ«

> This is almost exactly the same as a ResNet Bottlneck. The main difference is the SE block is moved from after conv3 to after conv2. Otherwise, it's just redefining the arguments for groups/bottleneck channels.
>

å‰å‘ä»£ç å¦‚ä¸‹

```python
    def forward(self, x):
        shortcut = x
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.se(x)
        x = self.conv3(x)	# no activation
        if self.downsample is not None:
            # NOTE stuck with downsample as the attr name due to weight compatibility
            # now represents the shortcut, no shortcut if None, and non-downsample shortcut == nn.Identity()
            x = self.drop_path(x) + self.downsample(shortcut)
        x = self.act3(x)
        return x
```

## Swin Transformer

æœ€ç»ˆè¿˜æ˜¯è¦å‘è¿™ç¯‡ ICCV 2021 best paper å‘èµ·è¿›æ”»ã€‚æˆ‘ä¸ä»…æƒ³è¦çŸ¥é“ Swin çš„å®ç°ç»†èŠ‚ï¼Œæˆ‘æ›´æƒ³äº†è§£å…¶è®­ç»ƒæ–¹æ³•ï¼Œå› ä¸º timm ä¸­åæœŸå°†ä¹‹å‰å¾ˆå¤šç½‘ç»œæŒ‰ç…§ swin çš„æ–¹å¼é‡æ–°è®­äº†ä¸€æ¬¡ï¼Œéƒ½æœ‰æå‡ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘è¿˜æƒ³çŸ¥é“ swin æœ€å…·ä»·å€¼çš„æ€æƒ³

### Swin ä¸­çš„æ¦‚å¿µ

1. **PatchEmbed**

   å’Œ ResNet ä¸­çš„ Stem æ˜¯ä¸€ä¸ªæ¦‚å¿µï¼Œå°†åˆå§‹å›¾åƒè¿›è¡Œä¸‹é‡‡æ ·ï¼Œæ‰€ä½¿ç”¨çš„æ˜¯ `Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)`ï¼Œå¹¶ä¸”åŠ å…¥äº† post norm ç»´æŒæ•°å€¼ç¨³å®šï¼Œè¿˜å°†è¾“å‡ºå¼ é‡å˜ä¸º `NHWC` å¸ƒå±€

2. **Relative Positional Bias**

   æ€ªä¸å¾—çœ‹ SparseBEV çš„æ—¶å€™è§‰å¾— SASA çœ¼ç†Ÿï¼Œå’Œ relative posisional bias çš„å½¢å¼æ˜¯ä¸€æ¨¡ä¸€æ ·çš„ã€‚å…¶ç›®çš„å°±æ˜¯åœ¨ attention ä¸­åŠ å…¥ä½ç½®çš„æ„ŸçŸ¥ï¼Œä¹Ÿè®¸è·ç¦»è¿‘çš„æƒé‡æ›´æ”¹æ›´é«˜ä¸€äº›ï¼Œè·ç¦»è¿œçš„æƒé‡ä½ä¸€äº›
   $$
   Attn(Q,K,V)=Softmax(\frac{QK^T}{\sqrt d} + B)
   $$
   ä¸ºäº†ä¿æŒçµæ´»æ€§ï¼ŒB (bias) æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°ï¼Œå¹¶ä¸”å¯¹æ¯ä¸€ä¸ª head éƒ½ä¸ä¸€æ ·

   åˆ›å»ºè¿™ä¸ª bias çš„è¿‡ç¨‹æ˜¯å€¼å¾—å­¦ä¹ çš„ï¼Œæˆ‘ä»¬çš„ç›®çš„æ˜¯ï¼šåˆ›å»ºä¸€ä¸ª bias matrix $(HW,HW,1)$ï¼Œä»£è¡¨äº† windows ä¸­ä¸¤ä¸¤ä¹‹é—´çš„ biasã€‚æˆ‘è‡ªå·±çš„å®ç°å¦‚ä¸‹ï¼Œæ˜¯ä¸€ä¸ªäºŒç»´çš„å®ç°è€Œæ²¡æœ‰å±•å¼€åˆ°ä¸€ç»´ï¼Œè¿ç”¨äº† **`meshgrid` & ç´¢å¼•**

   ```python
   # impl my relative bias
   import torch
   
   def get_relative_position_index(win_h, win_w):
       # get coordinates of center pixel
       coord = torch.stack((torch.meshgrid(torch.arange(win_h), torch.arange(win_w))), dim=-1) # (H, W, 2)
       coord = coord.reshape(-1, 2) # (H*W, 2)
       offset = coord.unsqueeze(1) - coord.unsqueeze(0) # (HW, HW, 2)
       offset[:, :, 0] += win_h - 1    # shift to non-negative
       offset[:, :, 1] += win_w - 1
       return offset
   
   win_h, win_w = 3, 3
   num_heads = 2
   rel_bias = torch.randn(2*win_h-1, 2*win_w-1, num_heads)
   index = get_relative_position_index(3, 3)
   bias = rel_bias[index[:, :, 0], index[:, :, 1]] # (HW, HW, num_heads)
   ```

3. **WindowAttention**

   çª—å£æ³¨æ„åŠ›éå¸¸å®¹æ˜“ç†è§£ï¼šå°†è¾“å…¥çš„ç‰¹å¾å›¾åˆ†è§£ä¸ºçª—å£ `(num_windows * B, N, C)`ï¼Œåœ¨æ¯ä¸€ä¸ªçª—å£å†…è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚å› ä¸ºéœ€è¦ä½¿ç”¨ relative positional biasï¼Œæ‰€ä»¥è¦å°† `num_heads` çš„ç»´åº¦å•ç‹¬åˆ†å‡ºæ¥ï¼Œä¹‹å‰çš„æ³¨æ„åŠ›å¯ä»¥ç›´æ¥åˆå¹¶åˆ° batch çš„ç»´åº¦

   æ²¡æœ‰ä½¿ç”¨ `attn_mask` æ¥æ·»åŠ  biasï¼Œä½†å®é™…ä¸Šæ˜¯å¯ä»¥ä¸€èµ·åšçš„ï¼Œå‰å‘ä»£ç å¦‚ä¸‹

   ```python
       def forward(self, x, mask: Optional[torch.Tensor] = None):
           B_, N, C = x.shape
           qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)	 # (3, B, num_heads, N, C)
           q, k, v = qkv.unbind(0)
   
           q = q * self.scale
           attn = q @ k.transpose(-2, -1) # (B, num_heads, N, N)
           
           # relative positional bias
           attn = attn + self._get_rel_pos_bias()
           
           if mask is not None:	# shift attn mask
               num_win = mask.shape[0]
               attn = attn.view(-1, num_win, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
               attn = attn.view(-1, self.num_heads, N, N)
           # attend
           attn = self.softmax(attn)
           attn = self.attn_drop(attn)
           x = attn @ v
   
           x = x.transpose(1, 2).reshape(B_, N, -1)
           x = self.proj(x)
           x = self.proj_drop(x)
           return x
   ```

   è¿™é‡Œå¯¹æ¯”äº†ä¸€äº› torch çš„å¸¸ç”¨æ“ä½œï¼š`split, chunk, unbind`ï¼Œä»¥åŠ `einops`ã€‚å‰é¢ä¸‰ä¸ªæ“ä½œæ˜¯å°†æŸä¸€ä¸ªç»´åº¦è¿›è¡Œåˆ†å¼€ï¼Œsplit æ˜¯ä¼ å…¥ä¸€ä¸ª `split_size`ï¼Œchunk ä¼ å…¥åˆ†ç¦»æ•°é‡ï¼Œç›¸å½“äºä¼ å…¥å‡åŒ€çš„ `split_size`ï¼Œå¹¶ä¸”æœ€åä¸€ä¸ª size ä¸ºè‡ªåŠ¨è°ƒæ•´çš„ï¼Œè€Œ unbind ç›¸å½“äº `chunk(dim_size)`ã€‚einops åœ¨å¯¹äºå½¢çŠ¶çš„æ“ä½œæ˜¯æœ€æ–¹ä¾¿çš„

4. **ShifedWindowAttention**

   æ‰€è°“çš„æ»‘åŠ¨çª—å£ï¼Œå°±æ˜¯å°† window åˆ’åˆ†æ–œä¸Šæ»‘åŠ¨ä¸€æ®µè·ç¦» `shift_size`ï¼Œè¿™ä¸ªæ“ä½œæ˜¯ä½¿ç”¨çš„ `torch.roll` å®Œæˆçš„

   ```python
   import torch
   
   x = torch.arange(16).view(4, 4)
   # roll
   y = torch.roll(x, (-1, -1), dims=(0, 1))
   
   # results
   tensor([[ 0,  1,  2,  3],
           [ 4,  5,  6,  7],
           [ 8,  9, 10, 11],
           [12, 13, 14, 15]])
   ----
   tensor([[ 5,  6,  7,  4],
           [ 9, 10, 11,  8],
           [13, 14, 15, 12],
           [ 1,  2,  3,  0]])
   ```

   çª—å£æ´»åŠ¨è¿‡åä¸èƒ½æŒ‰ç…§å¹³å¸¸çš„ window attention å®Œæˆï¼Œå› ä¸ºæœ‰æ‹¼æ¥çš„éƒ¨åˆ†ï¼Œæ‰€ä»¥éœ€è¦åˆ¶ä½œ attention mask æ¥å¤„ç†ä¾¿å®œåˆ°å³ä¸‹è§’çš„å›¾åƒã€‚å…¶å®ç°ä½¿ç”¨äº† python built-in  `slice` æ¥å·§å¦™å®Œæˆï¼Œå¯è§†åŒ–ç»“æœå‚è€ƒ [issue](https://github.com/microsoft/Swin-Transformer/issues/38#issuecomment-823810343)ã€‚ç®€å•æ€»ç»“ï¼š

   1. ç”Ÿæˆä¸€ä¸ªå¤§çš„ mask æ¨¡æ¿ (H, W)ï¼Œå…¶ä¸­ HW ä»£è¡¨**æ•´ä¸ªå›¾ç‰‡**çš„é«˜å®½

   2. å¯¹ mask è¿›è¡Œåˆ†åŒºï¼Œæ€»å…±åˆ†æˆ 8 ä¸ªå­åŒºåŸŸï¼Œå½“ window æ•°é‡ä¸º 4 ä¸ªçš„æ—¶å€™ç¤ºæ„å›¾å¦‚ä¸‹ï¼ˆå®é™…ä¸Š windows 0 çš„åŒºåŸŸåœ¨ H W å¤§çš„æ—¶å€™ï¼Œæ¯”ä¾‹ä¼šæ¯”è¾ƒå¤§ï¼‰

      <img src="Timm Image Backbone/image-20240107160029961.png" alt="image-20240107160029961" style="zoom:67%;" />

      ä»£ç å¦‚ä¸‹ï¼Œåˆ©ç”¨äº† sice æ¥å¯¹åŒºåŸŸè¿›è¡Œèµ‹å€¼æ“ä½œ

      ```python
                  img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1
                  cnt = 0
                  for h in (
                          slice(0, -self.window_size[0]),
                          slice(-self.window_size[0], -self.shift_size[0]),
                          slice(-self.shift_size[0], None)):
                      for w in (
                              slice(0, -self.window_size[1]),
                              slice(-self.window_size[1], -self.shift_size[1]),
                              slice(-self.shift_size[1], None)):
                          img_mask[:, h, w, :] = cnt
                          cnt += 1
      ```

   3. ä½¿ç”¨ `window_partition` å°† mask åˆ‡åˆ†æˆ window size

      ```python
      # (1, H, W, 1) -> (nW, window_size, window_size, 1)
      mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
      ```

   4. åœ¨æ¯ä¸€ä¸ª window size å†…éƒ¨ï¼Œä¸åŒå­åŒºåŸŸä¹‹é—´æ˜¯ä¸åšæ³¨æ„åŠ›çš„ï¼Œæ‰€ä»¥åŒºåŸŸå·ä¸åŒçš„åŠ å…¥æƒ©ç½š `attn_mask = -100`

      ```python
      mask_windows = mask_windows.view(-1, self.window_area)	# (nW, HW)
      attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)	# (nW, HW, HW)
      # mask
      attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
      ```

5. **PatchMerging**

   æ˜¯ Swin ä¸­ä¸‹é‡‡æ ·çš„æ–¹å¼ï¼Œå°† 2x2 çš„åŒºåŸŸç‰¹å¾å †å èµ·æ¥ï¼Œç„¶åç”¨ä¸€ä¸ª Linear è½¬æ¢ç»´åº¦ã€‚è¯¥æ–¹æ³•æ›¿æ¢äº†åŸå§‹çš„å·ç§¯ä¸‹é‡‡æ ·

   ```python
       def forward(self, x):
           B, H, W, C = x.shape
           x = x.reshape(B, H // 2, 2, W // 2, 2, C).permute(0, 1, 3, 4, 2, 5).flatten(3)
           x = self.norm(x)
           x = self.reduction(x)	# linear project 4C->2C
           return x
   ```

### ç†è§£ Swin Transformer

äº†è§£äº† Swin çš„åŸºæœ¬æ¦‚å¿µï¼Œæ¥ä¸‹æ¥å°±æ˜¯ç»„è£…ä»–ä»¬ã€‚Swin çš„æ¶æ„ç›´æ¥ä¸Šå›¾å°±è¡Œäº†

![image-20240107192040916](Timm Image Backbone/image-20240107192040916.png)

åŒæ—¶å†æ•´ç†ä¸€ä¸‹ timm ä¸­ç»Ÿä¸€çš„åˆ†ç±»å¤´

```python
def forward_head(self, x):
    # x is direct output of Stage-4
    x = self.norm(x)	# this norm is actually written in forward_features part
    
    # head
    x = self.global_pool(x)	# nn.AdaptiveAvgPool2d(1)
    out = self.linear(x)
    return out
```

è¿™é‡Œæˆ‘åŠ å…¥äº†ä¸€ä¸ª LayerNormï¼Œå®é™…ä¸Šè¿™ä¸€å±‚æ˜¯åœ¨ `forward_features` ä¸­çš„ï¼Œä½†æ˜¯åœ¨ä¸Šé¢çš„ç¤ºæ„å›¾ä¸­ä¸èƒ½å±•ç¤ºå‡ºæ¥ï¼ŒåŠ åœ¨è¿™é‡Œè¡¨ç¤ºå¼ºè°ƒ

## Concept

1. DropPath çš„ç†è§£ï¼Œ[DropPath in TIMM seems like a Dropout?](https://stackoverflow.com/questions/69175642/droppath-in-timm-seems-like-a-dropout)

   å®é™…ä¸Š DropPath æ˜¯ä»¥ä¸€å®šæ¦‚ç‡ä¸¢å¼ƒæ•´ä¸ª sampleï¼Œä¾‹å¦‚æˆ‘ä»¬æœ‰ä¸€ä¸ª batch `(N,C,H,W)`ï¼Œæˆ‘ä»¬å°†éšæœºèˆå¼ƒä¸€ä¸ªæ ·æœ¬ï¼Œå°†å…¶å€¼è®¾ç½®ä¸º0ã€‚åœ¨å¸¸è§„çš„å‰å‘è·¯å¾„ä¸­ï¼Œç›´æ¥ä¸¢å¼ƒæ ·æœ¬æ ¹æœ¬æ— æ³•äº§ç”Ÿæœ‰æ•ˆçš„è¾“å‡ºï¼Œä½†æ˜¯ç”¨åœ¨æ®‹å·®é“¾æ¥ä¸­å°±ä¸ä¸€æ ·äº†
   $$
   H_l=ReLU(b_lf_l(H_{l-1})+H_{l-1})
   $$
   å…¶ä¸­ $H_l$ ä»£è¡¨éšè—å±‚è¾“å‡ºï¼Œ$b_l$ ä¸ºä¸€ä¸ªä¼¯åŠªåˆ©åˆ†å¸ƒï¼ˆäºŒé¡¹åˆ†å¸ƒï¼‰ï¼Œå¦‚æœæˆ‘ä»¬éšæœºä¸¢å¼ƒäº†æŸä¸ªæ ·æœ¬ï¼Œå³ $b_l=0$ï¼Œç»“æœå°†å…¨éƒ¨ä½¿ç”¨ä¸Šä¸€å±‚çš„éšè—å±‚è¾“å‡ºã€‚è¿™æ ·çš„ DropPath å¯ä»¥åˆ›é€ â€œéšæœºæ·±åº¦â€ç°è±¡ï¼Œèƒ½å¤Ÿè®©è®­ç»ƒæ›´ç¨³å®šã€æ”¶æ•›æ›´å¿«ï¼Œè®©ç½‘ç»œçš„æ·±åº¦å˜å¾—æ›´æ·±

2. dataclassï¼Œå‚è€ƒ [ç†è§£ python dataclass](https://zhuanlan.zhihu.com/p/59657729)

   dataclass æ˜¯ä¸€ä¸ªé€‚åˆäºå­˜å‚¨æ•°æ®å¯¹è±¡çš„ç±»ï¼Œå¯ä»¥é€šè¿‡ `@dataclass` è£…é¥°å™¨å®ç°ï¼Œæˆ‘ä»¬åªéœ€è¦å®šä¹‰å¥½æç¤ºç±»å‹å³å¯

   ```python
   from dataclasses import dataclass
   @dataclass
   class Point:
       x: int
       y: int
       z: int = 0	# default
   ```

   è¿™æ ·å°±å¯ä»¥ä¸é€šè¿‡å®šä¹‰ `__init__` æ–¹æ³•æˆ–è€…ç±»æ–¹æ³•æ¥æ–¹ä¾¿åˆå§‹åŒ–ï¼Œå¹¶ä¸”åœ¨ print æ—¶èƒ½å¤Ÿæœ‰è¾ƒé«˜çš„å¯è¯»æ€§

   ```python
   p = Point(1, 2)
   p = Point(x='2', y=3)	# no error
   ```

   åœ¨ timm ä¸­ä¹Ÿä½¿ç”¨äº† `replace` æ–¹æ³•æ›¿æ¢å…¶ä¸­çš„å­—æ®µ

   ```python
   from dataclasses import replace
   replace(p, **kwargs)
   ```

   é™¤æ­¤ä¹‹å¤– dataclass è¿˜å¯ä»¥åŠ å…¥å‚æ•°

   ```python
   @dataclass(order=True, frozen=True, repr=False)
   ```

   ä»¥ä¸Šä¸‰ä¸ªä¾‹å­ä»£è¡¨æ˜¯å¦èƒ½å¤Ÿæ¯”è¾ƒï¼Œæ˜¯å¦èƒ½å¤Ÿåœ¨åˆ›é€ åè¢«æ”¹å˜ï¼Œä»¥åŠæ˜¯å¦ç”Ÿæˆå­—ç¬¦ä¸²è¡¨ç¤º

   è¿˜å¯ä»¥é€šè¿‡å®šä¹‰ `__post_init__` æ–¹å¼æ¥åˆ›å»ºå¤æ‚çš„åˆå§‹åŒ–

   ```python
   import math
   @dataclass
   class FloatNumber:
       val: float = 0.0
       def __post_init__(self):
           self.decimal, self.integer = math.modf(self.val)
   # >>> a = Number(2.2)
   # >>> a.val
   # >>> 2.2
   # >>> a.integer
   # >>> 2.0
   ```

   å¦‚æœå®šä¹‰äº† `__init__` æ–¹æ³•ï¼Œä½¿ç”¨ dataclass å°±æ²¡æœ‰æ„ä¹‰
   
   è¿˜å¯ä»¥ä½¿ç”¨ `field(default_factory=func)` æ¥è·å¾—æ›´å¤æ‚çš„åˆå§‹åŒ–c

## é—®é¢˜

1. timm ä¸­çš„ tag è¡¨ç¤ºäº†ä»€ä¹ˆæ„æ€

   é€šå¸¸è¿™äº› tag è¡¨ç¤ºäº†æ¨¡å‹æ˜¯å¦‚ä½•è¿›è¡Œè®­ç»ƒçš„ï¼Œä½¿ç”¨äº†å“ªäº›æ•°æ®é›†ï¼Œä½†ä¸æ¸…æ¥šå…·ä½“çš„è®­ç»ƒé…ç½®

2. timm ä¸­å¦‚ä½•ä¸æ›´æ”¹æ¨¡å‹è·å¾—äº†ä¸­é—´ç‰¹å¾ï¼Ÿ

3. PretrainedCfg & DefaultCfg æœ‰ä»€ä¹ˆä½œç”¨ï¼ŸDataclass æ˜¯ä¸€ä¸ªè£…é¥°å™¨ï¼Œæœ‰ä»€ä¹ˆä½œç”¨

4. jit æ˜¯ä¸€ä¸ªéœ€è¦æ³¨æ„çš„ç‚¹å—ï¼Ÿ

5. timm æ˜¯å¦‚ä½•ä¹¦å†™é•¿ä»£ç çš„ï¼Ÿ

   - ä»£ç æ®µè½çš„é•¿åº¦

     å°±åƒå†™æ–‡ç« ä¸€æ ·ï¼Œæ¯ä¸€ä¸ªäººçš„æ–­å¥æ˜¯ä¸ä¸€æ ·çš„ï¼Œçœ‹è‡ªèº«å–œæ¬¢å†™é•¿å¥è¿˜æ˜¯å†™çŸ­å¥

     æœ‰çš„ä»£ç æ®µè½å°±ç®—åªæœ‰ä¸¤è¡Œä¹Ÿæœ‰ç©ºè¡Œé—´éš”ã€‚æœ€ç»ˆå†³å®šæ¡ä»¶ï¼šè¿™æ®µä»£ç ä¸å‘¨å›´ä»£ç çš„ç›¸å…³ç¨‹åº¦

   - å¯¹äºç®€å•ä»£ç ï¼Œæˆ‘ç»å¸¸å–œæ¬¢å†™å¦‚ä¸‹ä»£ç 

     ```python
     a = func(); a = a.func
     ```

     å¯ä»¥é€šè¿‡åˆç†çš„å‘½åæ–¹å¼æ¥é¿å…ï¼Œä¹Ÿå°½é‡å°‘å†™ä¸€äº› `_var` çš„å˜é‡åç§°

   - æ³¨é‡Šæƒ…å†µ

     ç®€è¦çš„å¿…è¦è¯´æ˜ï¼Œå¦‚æœæ˜¯åœ¨ä¸€ä¸ªä»£ç æ®µè½ä¸­å°±å¤¹åœ¨ä¸­é—´ï¼Œå…¶ä»–æ—¶å€™å¯ä»¥ç”¨ç©ºè¡Œé—´éš”

   - å¯¹äºæ¯ä¸€ä¸ªæ¨¡å‹ç±»åˆ«éƒ½è¿›è¡Œäº†å‚æ•°è§£é‡Šï¼Œå¯¹é‡è¦æ¨¡å‹è§£é‡Šæ¨¡å‹æ¥æºä»¥åŠç‰¹æ®Šå«ä¹‰

   - å¯¹äºé•¿çš„ä»£ç è¡Œå¹¶æ²¡æœ‰åšå¤ªå¤šçš„åˆ†è¡Œå¤„ç†ï¼Œå°¤å…¶æ˜¯ `if ... else`ï¼Œtimm å¾ˆå–œæ¬¢å†™åœ¨ä¸€è¡Œ

6. **timm è®­ç»ƒ resnet çš„æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ**å¯¹æ¯” EVA, ConvNeXT, Swin, ResNet v1 v2, MAE

   ä»¥ `.sw` tag çš„è®­ç»ƒæ–¹æ³• [discussion](https://github.com/huggingface/pytorch-image-models/discussions/1829) [_timm_hparams.md](https://gist.github.com/rwightman/943c0fe59293b44024bbd2d5d23e6303) [ResNet strikes back](https://arxiv.org/abs/2110.00476) [pytorch pretrained](https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/) [how to train your vits](https://arxiv.org/abs/2106.10270)

7. activation å’Œ norm çš„ä½¿ç”¨ä½ç½®ï¼šnorm ä¼šé¢‘ç¹ä½¿ç”¨ä»¥ç»´æŒæ•°å€¼ç¨³å®šï¼Œ**åœ¨ ViT ä¸­é€šå¸¸ä½¿ç”¨åœ¨æœ€å‰é¢ï¼Œå³ pre-norm**ï¼Œä½† norm åœ¨ mlp ä¸­åŸºæœ¬ä¸ä¼šä½¿ç”¨ã€‚activation åœ¨æ¯ä¸€ä¸ª conv or linear å±‚è¿‡åéƒ½ä¼šæœ‰ï¼Œé™¤éæ˜¯æœ€åçš„è¾“å‡ºå±‚

   mlp ä¸­åŸºæœ¬ä¸Šä¸å¸¦ norm å±‚ï¼Œä½†æ˜¯æ¯ä¸€å±‚ linear è¿‡ååŸºæœ¬ä¸Šéœ€è¦ä½¿ç”¨ dropoutï¼Œä½†æ˜¯å¤§å¤šæ•°æ—¶å€™è®¾ç½®ä¸º 0.0ğŸ§ä½†æ˜¯ drop path ç”¨å¾—æ›´å¤šï¼Œåœ¨ swin å’Œ convnext ä¸­éƒ½æœ‰ä½¿ç”¨

   ```python
       def forward(self, x):
           x = self.fc1(x)
           x = self.act(x)
           x = self.drop1(x)
           x = self.fc2(x)
           x = self.drop2(x)
           return x
   ```

   rule: 

   1. no activation if use residual!!! ä¸ºä»€ä¹ˆæœ€åä¸€å±‚æ²¡æœ‰ act? å› ä¸ºè¦ä½¿ç”¨æ®‹å·®è¿æ¥ï¼Œè¿™æ›´æœ‰åˆ©äºæ®‹å·®å­¦ä¹ ã€‚å¦‚æœåŠ å…¥äº†æ¿€æ´»å±‚ï¼Œæœ‰çš„å€¼å°±ç›´æ¥è¢«ç½®é›¶äº†ï¼Œç ´åäº†æ®‹å·®å­¦ä¹ 
   2. it is ok not to use relu for each block outputï¼šmobilenet, transformers, regnetz (timm), efficientnet

8. å·ç§¯ backbone å’Œ transformer backbone æœ‰ä»€ä¹ˆå¿…ç„¶çš„å·®åˆ«å—ï¼Ÿä¸ºä»€ä¹ˆ Swin çš„æ¨ªç©ºå‡ºä¸–æå‡äº† SOTA è¿™ä¹ˆå¤šï¼Ÿ

   æˆ‘è®¤ä¸º ConvNeXT å¯èƒ½ç»™å‡ºäº†ç­”æ¡ˆï¼šSwin çš„æˆåŠŸä»ç„¶æ˜¯ **Transformer æ¶æ„**çš„æˆåŠŸï¼šæ›´å°‘çš„ activation layerï¼Œæ›´å°‘çš„ norm å¹¶ä¸”ä½¿ç”¨ LayerNormã€‚å¦å¤–ä¸€ä¸ªå…³é”®ç‚¹ï¼š**ä¸‹é‡‡æ ·çš„æ–¹å¼ä¹Ÿéå¸¸é‡è¦ï¼**åœ¨ ConvNeXT ä¸­æŒ‡å‡ºï¼Œåªä½¿ç”¨ä¸€ä¸ªç®€å•çš„ Conv2d è¿›è¡Œ stride 2 ä¸‹é‡‡æ ·ä¼šç›´æ¥å¯¼è‡´è®­ç»ƒå‘æ•£ï¼ä½†æ˜¯åœ¨åŠ å…¥äº† pre-norm ä¹‹åï¼Œè®­ç»ƒå°±ä¼šå˜å¾—ç¨³å®šï¼Œå¹¶ä¸”æå‡äº†å‡†ç¡®ç‡ã€‚ConvNeXT å¾—å‡ºç»“è®ºï¼š**åœ¨åˆ†è¾¨ç‡æ”¹å˜å‰ï¼Œä½¿ç”¨ä¸€å±‚ norm layer æ˜¯å¿…è¦çš„ï¼Œè¿™ä¼šæå¤§å¢å¼ºè®­ç»ƒç¨³å®šæ€§**ã€‚å¹¶ä¸”åŒºåˆ«ä¸ ResNetï¼Œä¸‹é‡‡æ ·æ˜¯ä¸ä¼šå‚ä¸åˆ°æ®‹å·®è¿æ¥çš„ç»“æ„å½“ä¸­çš„ï¼Œè¿™æ ·çš„ä¸‹é‡‡æ ·æ–¹å¼èƒ½å¤Ÿæ˜¾è‘—æå‡è¡¨ç°

